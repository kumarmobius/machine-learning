name: Stacking and Voting V10
inputs:
  - {name: X_data, type: Dataset}
  - {name: y_data, type: Dataset}
  - {name: model_type, type: String, optional: true}
  - {name: ensemble_type, type: String, description: "none | stacking | voting", optional: true, default: "stacking"}
  - {name: n_ensemble_models, type: Integer, description: "If >1 and model_names not provided, randomly pick this many models from default pool", optional: true, default: "0"}
  - {name: model_names, type: String, description: "Optional comma-separated list of model names to use verbatim (overrides n_ensemble_models)", optional: true, default: ""}
  - {name: meta_model_name, type: String, description: "Optional meta-model name for stacking (e.g. 'lightgbm' or 'logistic'). If absent, prefer LightGBM/XGBoost when available, else fallback.", optional: true, default: ""}
  - {name: stacking_cv, type: Integer, default: "4"}
  - {name: random_state, type: Integer, optional: true, default: "50"}
  
  # Tree-based hyperparameters
  - {name: n_estimators, type: Integer, optional: true, default: "100"}
  - {name: n_estimators_boost, type: Integer, optional: true, default: "100"}
  - {name: max_depth_tree, type: Integer, optional: true, default: "6"}
  - {name: max_depth_boost, type: Integer, optional: true, default: "5"}
  - {name: min_samples_leaf, type: Integer, optional: true, default: "2"}
  - {name: min_samples_split, type: Integer, optional: true, default: "5"}
  - {name: max_features, type: String, optional: true, default: "sqrt"}
  - {name: learning_rate, type: Float, optional: true, default: "0.1"}
  
  # KNN hyperparameters
  - {name: knn_k, type: Integer, optional: true, default: "9"}
  - {name: knn_weights, type: String, optional: true, default: "distance"}
  
  # SVM hyperparameters
  - {name: svm_C, type: Float, optional: true, default: "10.0"}
  - {name: svm_gamma, type: String, optional: true, default: "scale"}
  - {name: svm_kernel, type: String, optional: true, default: "rbf"}
  - {name: svm_max_iter, type: Integer, optional: true, default: "5000"}
  - {name: nu_default, type: Float, optional: true, default: "0.6"}
  
  # Linear model hyperparameters
  - {name: ridge_alpha, type: Float, optional: true, default: "1.0"}
  - {name: lasso_alpha, type: Float, optional: true, default: "0.1"}
  - {name: elasticnet_alpha, type: Float, optional: true, default: "0.1"}
  - {name: elasticnet_l1_ratio, type: Float, optional: true, default: "0.5"}
  
  # Neural network hyperparameters
  - {name: mlp_hidden_layers, type: String, optional: true, default: "128,64"}
  - {name: mlp_activation, type: String, optional: true, default: "relu"}
  - {name: mlp_alpha, type: Float, optional: true, default: "0.0001"}
  - {name: mlp_max_iter, type: Integer, optional: true, default: "300"}
  
  # SGD hyperparameters
  - {name: sgd_max_iter, type: Integer, optional: true, default: "5000"}
  - {name: sgd_alpha, type: Float, optional: true, default: "0.0001"}
  
  # Boosting library hyperparameters
  - {name: lgbm_n_estimators, type: Integer, optional: true, default: "100"}
  - {name: lgbm_learning_rate, type: Float, optional: true, default: "0.1"}
  - {name: lgbm_max_depth, type: Integer, optional: true, default: "10"}
  - {name: lgbm_num_leaves, type: Integer, optional: true, default: "30"}
  
  - {name: xgb_n_estimators, type: Integer, optional: true, default: "100"}
  - {name: xgb_learning_rate, type: Float, optional: true, default: "0.05"}
  - {name: xgb_max_depth, type: Integer, optional: true, default: "10"}
  
  - {name: cat_n_estimators, type: Integer, optional: true, default: "100"}
  - {name: cat_learning_rate, type: Float, optional: true, default: "0.05"}
  - {name: cat_depth, type: Integer, optional: true, default: "10"}

outputs:
  - {name: model_pickle, type: Model }
  - {name: metrics_json, type: Data }
  - {name: model_config, type: Data }

implementation:
  container:
    image:  gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, json, os, sys, traceback, random, warnings
        import pandas as pd, numpy as np, joblib
        import cloudpickle
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import (
            ExtraTreesClassifier, ExtraTreesRegressor,
            RandomForestClassifier, RandomForestRegressor,
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor,
            BaggingClassifier, BaggingRegressor,
            VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor,
            HistGradientBoostingClassifier, HistGradientBoostingRegressor
        )
        from sklearn.linear_model import (
            LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor,
            Ridge, RidgeClassifier, Lasso, ElasticNet, BayesianRidge,
            PassiveAggressiveClassifier, PassiveAggressiveRegressor
        )
        from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
        from sklearn.neural_network import MLPClassifier, MLPRegressor
        from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR, NuSVR
        from scipy import stats
        
        # Suppress convergence warnings
        warnings.filterwarnings('ignore', category=UserWarning)
        warnings.filterwarnings('ignore', module='sklearn')
      
        try:
            from lightgbm import LGBMClassifier, LGBMRegressor
        except Exception:
            LGBMClassifier = None
            LGBMRegressor = None
        try:
            from xgboost import XGBClassifier, XGBRegressor
        except Exception:
            XGBClassifier = None
            XGBRegressor = None
        try:
            from catboost import CatBoostClassifier, CatBoostRegressor
        except Exception:
            CatBoostClassifier = None
            CatBoostRegressor = None
      
        def load_parquet_df(path):
            return pd.read_parquet(path) 
      
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
      
        def get_model_params(model):
            try:
                if hasattr(model, 'get_params'):
                    params = model.get_params(deep=False)
                    # Convert non-serializable objects to strings
                    serializable_params = {}
                    for k, v in params.items():
                        if isinstance(v, (int, float, str, bool, type(None))):
                            serializable_params[k] = v
                        elif isinstance(v, (list, tuple)):
                            serializable_params[k] = str(v)
                        else:
                            serializable_params[k] = str(type(v).__name__)
                    return serializable_params
                return {}
            except Exception as e:
                print(f"[WARN] Could not extract params: {e}")
                return {}
      
        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', required=True)
        ap.add_argument('--y_data', required=True)
        ap.add_argument('--model_type', default="classification")
        ap.add_argument('--ensemble_type', default="none")
        ap.add_argument('--n_ensemble_models', type=int, default=0)
        ap.add_argument('--model_names', default="")
        ap.add_argument('--meta_model_name', default="")
        ap.add_argument('--stacking_cv', type=int, default=5)
        ap.add_argument('--random_state', type=int, default=42)
        
        # Tree-based hyperparameters
        ap.add_argument('--n_estimators', type=int, default=500)
        ap.add_argument('--n_estimators_boost', type=int, default=300)
        ap.add_argument('--max_depth_tree', type=int, default=12)
        ap.add_argument('--max_depth_boost', type=int, default=8)
        ap.add_argument('--min_samples_leaf', type=int, default=2)
        ap.add_argument('--min_samples_split', type=int, default=5)
        ap.add_argument('--max_features', default="sqrt")
        ap.add_argument('--learning_rate', type=float, default=0.1)
        
        # KNN hyperparameters
        ap.add_argument('--knn_k', type=int, default=9)
        ap.add_argument('--knn_weights', default="distance")
        
        # SVM hyperparameters
        ap.add_argument('--svm_C', type=float, default=10.0)
        ap.add_argument('--svm_gamma', default="scale")
        ap.add_argument('--svm_kernel', default="rbf")
        ap.add_argument('--svm_max_iter', type=int, default=50000)
        ap.add_argument('--nu_default', type=float, default=0.6)
        
        # Linear model hyperparameters
        ap.add_argument('--ridge_alpha', type=float, default=1.0)
        ap.add_argument('--lasso_alpha', type=float, default=0.1)
        ap.add_argument('--elasticnet_alpha', type=float, default=0.1)
        ap.add_argument('--elasticnet_l1_ratio', type=float, default=0.5)
        
        # Neural network hyperparameters
        ap.add_argument('--mlp_hidden_layers', default="512,256,128")
        ap.add_argument('--mlp_activation', default="relu")
        ap.add_argument('--mlp_alpha', type=float, default=0.0001)
        ap.add_argument('--mlp_max_iter', type=int, default=1000)
        
        # SGD hyperparameters
        ap.add_argument('--sgd_max_iter', type=int, default=5000)
        ap.add_argument('--sgd_alpha', type=float, default=0.0001)
        
        # Boosting library hyperparameters
        ap.add_argument('--lgbm_n_estimators', type=int, default=300)
        ap.add_argument('--lgbm_learning_rate', type=float, default=0.05)
        ap.add_argument('--lgbm_max_depth', type=int, default=10)
        ap.add_argument('--lgbm_num_leaves', type=int, default=63)
        
        ap.add_argument('--xgb_n_estimators', type=int, default=300)
        ap.add_argument('--xgb_learning_rate', type=float, default=0.05)
        ap.add_argument('--xgb_max_depth', type=int, default=10)
        
        ap.add_argument('--cat_n_estimators', type=int, default=300)
        ap.add_argument('--cat_learning_rate', type=float, default=0.05)
        ap.add_argument('--cat_depth', type=int, default=10)
        
        ap.add_argument('--model_pickle', required=True)
        ap.add_argument('--metrics_json', required=True)
        ap.add_argument('--model_config', required=True)
        args = ap.parse_args()
      
        try:
            random.seed(int(args.random_state))
            np.random.seed(int(args.random_state))
      
            # Parse MLP hidden layers
            mlp_hidden_layers = tuple(int(x.strip()) for x in args.mlp_hidden_layers.split(',') if x.strip())
      
            # Build hyperparameters dictionary from args
            STATIC = {
                # Tree-based
                "n_estimators": args.n_estimators,
                "n_estimators_boost": args.n_estimators_boost,
                "min_samples_leaf": args.min_samples_leaf,
                "min_samples_split": args.min_samples_split,
                "max_depth_tree": args.max_depth_tree,
                "max_depth_boost": args.max_depth_boost,
                "max_features": args.max_features,
                "learning_rate": args.learning_rate,
                
                # KNN
                "knn_k": args.knn_k,
                "knn_weights": args.knn_weights,
                "knn_algorithm": "auto",
                
                # SVM
                "svm_C": args.svm_C,
                "svm_gamma": args.svm_gamma,
                "svm_kernel": args.svm_kernel,
                "svm_max_iter": args.svm_max_iter,
                "nu_default": args.nu_default,
                
                # Linear models
                "ridge_alpha": args.ridge_alpha,
                "lasso_alpha": args.lasso_alpha,
                "elasticnet_alpha": args.elasticnet_alpha,
                "elasticnet_l1_ratio": args.elasticnet_l1_ratio,
                
                # Neural networks
                "mlp_hidden_layer_sizes": mlp_hidden_layers,
                "mlp_activation": args.mlp_activation,
                "mlp_alpha": args.mlp_alpha,
                "mlp_learning_rate": "adaptive",
                "mlp_max_iter": args.mlp_max_iter,
                
                # SGD
                "sgd_max_iter": args.sgd_max_iter,
                "sgd_tol": 1e-4,
                "sgd_learning_rate": "optimal",
                "sgd_alpha": args.sgd_alpha,
                
                # Boosting libraries
                "meta_lgbm_n_estimators": args.lgbm_n_estimators,
                "meta_lgbm_learning_rate": args.lgbm_learning_rate,
                "meta_lgbm_max_depth": args.lgbm_max_depth,
                "meta_lgbm_num_leaves": args.lgbm_num_leaves,
                
                "meta_xgb_n_estimators": args.xgb_n_estimators,
                "meta_xgb_learning_rate": args.xgb_learning_rate,
                "meta_xgb_max_depth": args.xgb_max_depth,
                
                "meta_cat_n_estimators": args.cat_n_estimators,
                "meta_cat_learning_rate": args.cat_learning_rate,
                "meta_cat_depth": args.cat_depth,
                
                # Ensemble settings
                "STACKING_CV": int(args.stacking_cv or 5),
                "TRIAL_FIT_SVM": True
            }
      
            # Extended algorithm pools
            DEFAULT_POOL_CLASSIF = [
                "extratrees", "randomforest", "decisiontree", "adaboost", 
                "gradientboosting", "histgradientboosting", "bagging",
                "svc", "nusvc", "linearsvc", "knn", "logistic", "ridge_classifier",
                "sgd", "passive_aggressive", "gaussiannb", "bernoullinb", 
                "mlp", "lda", "qda", "xgboost", "lightgbm", "catboost"
            ]
            DEFAULT_POOL_REGR = [
                "extratrees", "randomforest", "decisiontree", "adaboost",
                "gradientboosting", "histgradientboosting", "bagging",
                "knn", "linearreg", "ridge", "lasso", "elasticnet", 
                "bayesian_ridge", "svr", "nusvr", "linearsvr", "sgd", 
                "passive_aggressive", "mlp", "xgboost", "lightgbm", "catboost"
            ]
      
            X = load_parquet_df(args.X_data)
            y_df = load_parquet_df(args.y_data)
            # Normalize y into DataFrame always
            if isinstance(y_df, pd.DataFrame):
                y_df = y_df.copy()
            else:
                # if single series/1d, make single-column DataFrame
                y_df = pd.DataFrame({"target": pd.Series(y_df).copy()})
      
            print(f"[INFO] X shape: {X.shape}, y shape: {y_df.shape}")
      
            task = args.model_type.strip().lower()
            ensemble_type = args.ensemble_type.strip().lower()
            n_pick = int(args.n_ensemble_models or 0)
            model_names_input = (args.model_names or "").strip()
            meta_name = (args.meta_model_name or "").strip().lower()
            stacking_cv = int(args.stacking_cv or STATIC["STACKING_CV"])
      
            # prepare selected_model_names function
            def choose_pool(task, n_pick, model_names_input):
                if model_names_input:
                    selected_model_names = [m.strip().lower() for m in model_names_input.split(",") if m.strip()]
                    print(f"[INFO] Using provided model_names: {selected_model_names}")
                    return selected_model_names
                pool = DEFAULT_POOL_CLASSIF if task == "classification" else DEFAULT_POOL_REGR
                if n_pick > 1:
                    if n_pick > len(pool):
                        print(f"[WARN] n_ensemble_models={n_pick} > pool size {len(pool)}. Using full pool.")
                        return pool.copy()
                    else:
                        sel = random.sample(pool, n_pick)
                        print(f"[INFO] Randomly selected {len(sel)} models: {sel}")
                        return sel
                elif n_pick == 1:
                    sel = [random.choice(pool)]
                    print(f"[INFO] n_ensemble_models==1 -> randomly picked single model: {sel}")
                    return sel
                else:
                    if ensemble_type in ("stacking","voting"):
                        print(f"[INFO] No model_names/n specified; using full default pool for ensemble.")
                        return pool.copy()
                    else:
                        print("[INFO] No ensemble requested and no model_names provided -> will train single default model 'extratrees'")
                        return []
      
            selected_model_names = choose_pool(task, n_pick, model_names_input)
      
            # Enhanced factory functions with better hyperparameters
            def make_base_estimator(name, task):
                n = name.strip().lower()
                rs = args.random_state
                
                if task == "classification":
                    if n == "knn":
                        return KNeighborsClassifier(
                            n_neighbors=STATIC["knn_k"], 
                            weights=STATIC["knn_weights"],
                            algorithm=STATIC["knn_algorithm"],
                            n_jobs=-1
                        )
                    if n == "decisiontree":
                        return DecisionTreeClassifier(
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_split=STATIC["min_samples_split"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            class_weight='balanced',
                            random_state=rs
                        )
                    if n == "extratrees":
                        return ExtraTreesClassifier(
                            n_estimators=STATIC["n_estimators"],
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            min_samples_split=STATIC["min_samples_split"],
                            max_features=STATIC["max_features"],
                            class_weight='balanced',
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "randomforest":
                        return RandomForestClassifier(
                            n_estimators=STATIC["n_estimators"],
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_split=STATIC["min_samples_split"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            max_features=STATIC["max_features"],
                            class_weight='balanced',
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "adaboost":
                        return AdaBoostClassifier(
                            n_estimators=STATIC["n_estimators_boost"],
                            learning_rate=STATIC["learning_rate"],
                            random_state=rs
                        )
                    if n == "gradientboosting":
                        return GradientBoostingClassifier(
                            n_estimators=STATIC["n_estimators_boost"],
                            max_depth=STATIC["max_depth_boost"],
                            learning_rate=STATIC["learning_rate"],
                            subsample=0.8,
                            random_state=rs
                        )
                    if n == "histgradientboosting":
                        return HistGradientBoostingClassifier(
                            max_iter=STATIC["n_estimators_boost"],
                            max_depth=STATIC["max_depth_boost"],
                            learning_rate=STATIC["learning_rate"],
                            random_state=rs
                        )
                    if n == "bagging":
                        return BaggingClassifier(
                            n_estimators=150,
                            max_samples=0.8,
                            max_features=0.8,
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "svc":
                        return SVC(
                            C=STATIC["svm_C"],
                            gamma=STATIC["svm_gamma"],
                            kernel=STATIC["svm_kernel"],
                            probability=True,
                            class_weight='balanced',
                            max_iter=STATIC["svm_max_iter"],
                            random_state=rs
                        )
                    if n == "nusvc":
                        return NuSVC(
                            nu=STATIC["nu_default"],
                            gamma=STATIC["svm_gamma"],
                            probability=True,
                            class_weight='balanced',
                            max_iter=STATIC["svm_max_iter"],
                            random_state=rs
                        )
                    if n == "linearsvc":
                        return LinearSVC(
                            C=STATIC["svm_C"],
                            max_iter=STATIC["svm_max_iter"],
                            dual='auto',  # Fixed: use 'auto' instead of False
                            tol=1e-4,
                            random_state=rs
                        )
                    if n == "logistic":
                        return LogisticRegression(
                            max_iter=3000,
                            solver='lbfgs',
                            C=1.0,
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "ridge_classifier":
                        return RidgeClassifier(
                            alpha=STATIC["ridge_alpha"],
                            random_state=rs
                        )
                    if n == "sgd":
                        return SGDClassifier(
                            max_iter=STATIC["sgd_max_iter"],
                            tol=STATIC["sgd_tol"],
                            learning_rate=STATIC["sgd_learning_rate"],
                            alpha=STATIC["sgd_alpha"],
                            random_state=rs
                        )
                    if n == "passive_aggressive":
                        return PassiveAggressiveClassifier(
                            max_iter=STATIC["sgd_max_iter"],
                            tol=STATIC["sgd_tol"],
                            random_state=rs
                        )
                    if n == "gaussiannb":
                        return GaussianNB()
                    if n == "bernoullinb":
                        return BernoulliNB()
                    if n == "mlp":
                        return MLPClassifier(
                            hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"],
                            activation=STATIC["mlp_activation"],
                            alpha=STATIC["mlp_alpha"],
                            learning_rate=STATIC["mlp_learning_rate"],
                            max_iter=STATIC["mlp_max_iter"],
                            random_state=rs
                        )
                    if n == "lda":
                        return LinearDiscriminantAnalysis()
                    if n == "qda":
                        return QuadraticDiscriminantAnalysis()
                    if n == "xgboost" and XGBClassifier is not None:
                        return XGBClassifier(
                            n_estimators=STATIC["meta_xgb_n_estimators"],
                            max_depth=STATIC["meta_xgb_max_depth"],
                            learning_rate=STATIC["meta_xgb_learning_rate"],
                            subsample=0.8,
                            colsample_bytree=0.8,
                            use_label_encoder=False,
                            eval_metric='logloss',
                            random_state=rs
                        )
                    if n == "lightgbm" and LGBMClassifier is not None:
                        return LGBMClassifier(
                            n_estimators=STATIC["meta_lgbm_n_estimators"],
                            max_depth=STATIC["meta_lgbm_max_depth"],
                            learning_rate=STATIC["meta_lgbm_learning_rate"],
                            num_leaves=STATIC["meta_lgbm_num_leaves"],
                            subsample=0.8,
                            colsample_bytree=0.8,
                            random_state=rs
                        )
                    if n == "catboost" and CatBoostClassifier is not None:
                        return CatBoostClassifier(
                            iterations=STATIC["meta_cat_n_estimators"],
                            depth=STATIC["meta_cat_depth"],
                            learning_rate=STATIC["meta_cat_learning_rate"],
                            verbose=0,
                            random_state=rs
                        )
                    raise ValueError(f"Unsupported classifier name: {name}")
                
                else:  # Regression
                    if n == "knn":
                        return KNeighborsRegressor(
                            n_neighbors=STATIC["knn_k"],
                            weights=STATIC["knn_weights"],
                            algorithm=STATIC["knn_algorithm"],
                            n_jobs=-1
                        )
                    if n == "decisiontree":
                        return DecisionTreeRegressor(
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_split=STATIC["min_samples_split"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            random_state=rs
                        )
                    if n == "extratrees":
                        return ExtraTreesRegressor(
                            n_estimators=STATIC["n_estimators"],
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            min_samples_split=STATIC["min_samples_split"],
                            max_features=STATIC["max_features"],
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "randomforest":
                        return RandomForestRegressor(
                            n_estimators=STATIC["n_estimators"],
                            max_depth=STATIC["max_depth_tree"],
                            min_samples_split=STATIC["min_samples_split"],
                            min_samples_leaf=STATIC["min_samples_leaf"],
                            max_features=STATIC["max_features"],
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "adaboost":
                        return AdaBoostRegressor(
                            n_estimators=STATIC["n_estimators_boost"],
                            learning_rate=STATIC["learning_rate"],
                            random_state=rs
                        )
                    if n == "gradientboosting":
                        return GradientBoostingRegressor(
                            n_estimators=STATIC["n_estimators_boost"],
                            max_depth=STATIC["max_depth_boost"],
                            learning_rate=STATIC["learning_rate"],
                            subsample=0.8,
                            random_state=rs
                        )
                    if n == "histgradientboosting":
                        return HistGradientBoostingRegressor(
                            max_iter=STATIC["n_estimators_boost"],
                            max_depth=STATIC["max_depth_boost"],
                            learning_rate=STATIC["learning_rate"],
                            random_state=rs
                        )
                    if n == "bagging":
                        return BaggingRegressor(
                            n_estimators=150,
                            max_samples=0.8,
                            max_features=0.8,
                            n_jobs=-1,
                            random_state=rs
                        )
                    if n == "svr":
                        return SVR(
                            C=STATIC["svm_C"],
                            gamma=STATIC["svm_gamma"],
                            kernel=STATIC["svm_kernel"],
                            max_iter=STATIC["svm_max_iter"]
                        )
                    if n == "nusvr":
                        return NuSVR(
                            nu=STATIC["nu_default"],
                            C=STATIC["svm_C"],
                            gamma=STATIC["svm_gamma"],
                            max_iter=STATIC["svm_max_iter"]
                        )
                    if n == "linearsvr":
                        # Fixed: use squared_epsilon_insensitive loss or dual='auto'
                        return LinearSVR(
                            C=STATIC["svm_C"],
                            max_iter=STATIC["svm_max_iter"],
                            dual='auto',  # Fixed: use 'auto' instead of False
                            loss='squared_epsilon_insensitive',  # Fixed: compatible loss function
                            tol=1e-4,
                            random_state=rs
                        )
                    if n in ("linearreg", "linear"):
                        return LinearRegression(n_jobs=-1)
                    if n == "ridge":
                        return Ridge(
                            alpha=STATIC["ridge_alpha"],
                            random_state=rs
                        )
                    if n == "lasso":
                        return Lasso(
                            alpha=STATIC["lasso_alpha"],
                            max_iter=5000,
                            random_state=rs
                        )
                    if n == "elasticnet":
                        return ElasticNet(
                            alpha=STATIC["elasticnet_alpha"],
                            l1_ratio=STATIC["elasticnet_l1_ratio"],
                            max_iter=5000,
                            random_state=rs
                        )
                    if n == "bayesian_ridge":
                        return BayesianRidge()
                    if n == "sgd":
                        return SGDRegressor(
                            max_iter=STATIC["sgd_max_iter"],
                            tol=STATIC["sgd_tol"],
                            learning_rate=STATIC["sgd_learning_rate"],
                            alpha=STATIC["sgd_alpha"],
                            random_state=rs
                        )
                    if n == "passive_aggressive":
                        return PassiveAggressiveRegressor(
                            max_iter=STATIC["sgd_max_iter"],
                            tol=STATIC["sgd_tol"],
                            random_state=rs
                        )
                    if n == "mlp":
                        return MLPRegressor(
                            hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"],
                            activation=STATIC["mlp_activation"],
                            alpha=STATIC["mlp_alpha"],
                            learning_rate=STATIC["mlp_learning_rate"],
                            max_iter=STATIC["mlp_max_iter"],
                            random_state=rs
                        )
                    if n == "xgboost" and XGBRegressor is not None:
                        return XGBRegressor(
                            n_estimators=STATIC["meta_xgb_n_estimators"],
                            max_depth=STATIC["meta_xgb_max_depth"],
                            learning_rate=STATIC["meta_xgb_learning_rate"],
                            subsample=0.8,
                            colsample_bytree=0.8,
                            random_state=rs
                        )
                    if n == "lightgbm" and LGBMRegressor is not None:
                        return LGBMRegressor(
                            n_estimators=STATIC["meta_lgbm_n_estimators"],
                            max_depth=STATIC["meta_lgbm_max_depth"],
                            learning_rate=STATIC["meta_lgbm_learning_rate"],
                            num_leaves=STATIC["meta_lgbm_num_leaves"],
                            subsample=0.8,
                            colsample_bytree=0.8,
                            random_state=rs
                        )
                    if n == "catboost" and CatBoostRegressor is not None:
                        return CatBoostRegressor(
                            iterations=STATIC["meta_cat_n_estimators"],
                            depth=STATIC["meta_cat_depth"],
                            learning_rate=STATIC["meta_cat_learning_rate"],
                            verbose=0,
                            random_state=rs
                        )
                    raise ValueError(f"Unsupported regressor name: {name}")
      
            def make_meta_estimator(name, task):
                n = (name or "").strip().lower()
                rs = args.random_state
                
                if n in ("", "lightgbm", "lgbm", "lgb"):
                    if task == "classification" and LGBMClassifier is not None:
                        return LGBMClassifier(
                            n_estimators=STATIC["meta_lgbm_n_estimators"],
                            max_depth=STATIC["meta_lgbm_max_depth"],
                            learning_rate=STATIC["meta_lgbm_learning_rate"],
                            num_leaves=STATIC["meta_lgbm_num_leaves"],
                            random_state=rs
                        )
                    if task == "regression" and LGBMRegressor is not None:
                        return LGBMRegressor(
                            n_estimators=STATIC["meta_lgbm_n_estimators"],
                            max_depth=STATIC["meta_lgbm_max_depth"],
                            learning_rate=STATIC["meta_lgbm_learning_rate"],
                            num_leaves=STATIC["meta_lgbm_num_leaves"],
                            random_state=rs
                        )
                if n in ("xgboost", "xgb"):
                    if task == "classification" and XGBClassifier is not None:
                        return XGBClassifier(
                            n_estimators=STATIC["meta_xgb_n_estimators"],
                            max_depth=STATIC["meta_xgb_max_depth"],
                            learning_rate=STATIC["meta_xgb_learning_rate"],
                            use_label_encoder=False,
                            eval_metric='logloss',
                            random_state=rs
                        )
                    if task == "regression" and XGBRegressor is not None:
                        return XGBRegressor(
                            n_estimators=STATIC["meta_xgb_n_estimators"],
                            max_depth=STATIC["meta_xgb_max_depth"],
                            learning_rate=STATIC["meta_xgb_learning_rate"],
                            random_state=rs
                        )
                if n in ("catboost", "cat"):
                    if task == "classification" and CatBoostClassifier is not None:
                        return CatBoostClassifier(
                            iterations=STATIC["meta_cat_n_estimators"],
                            depth=STATIC["meta_cat_depth"],
                            learning_rate=STATIC["meta_cat_learning_rate"],
                            verbose=0,
                            random_state=rs
                        )
                    if task == "regression" and CatBoostRegressor is not None:
                        return CatBoostRegressor(
                            iterations=STATIC["meta_cat_n_estimators"],
                            depth=STATIC["meta_cat_depth"],
                            learning_rate=STATIC["meta_cat_learning_rate"],
                            verbose=0,
                            random_state=rs
                        )
                if task == "classification":
                    return LogisticRegression(max_iter=3000, random_state=rs)
                else:
                    return Ridge(alpha=1.0, random_state=rs)
      
            # Core training function that reuses your ensemble logic for a single 1d target
            def train_single_target(X, y_series, target_name):
                print(f"[INFO] Training target: {target_name} (n_samples={len(y_series)})")
                trained_object = None
                trained_estimators = []
                trained_base_info = []
                model_to_train = None
                model_params = {}  # Store actual model parameters
      
                if ensemble_type not in ("stacking","voting"):
                    if selected_model_names:
                        model_to_train = selected_model_names[0]
                    else:
                        model_to_train = "extratrees"
                    print(f"[INFO] Training single model: {model_to_train} for target {target_name}")
                    model_inst = make_base_estimator(model_to_train, task)
                    if task == "classification":
                        try:
                            sw = compute_sample_weight(class_weight='balanced', y=pd.Series(y_series).astype(int))
                        except Exception:
                            sw = None
                        if sw is not None:
                            try:
                                model_inst.fit(X, y_series, sample_weight=sw)
                            except TypeError:
                                model_inst.fit(X, y_series)
                        else:
                            model_inst.fit(X, y_series)
                    else:
                        model_inst.fit(X, y_series)
                    trained_object = model_inst
                    trained_base_info = [model_to_train]
                    model_params[model_to_train] = get_model_params(model_inst)
                else:
                    if not selected_model_names:
                        raise ValueError("Ensemble requested but no base models selected.")
                    estimators = []
                    for i, nm in enumerate(selected_model_names):
                        try:
                            est = make_base_estimator(nm, task)
                            estimators.append((f"{nm}_{i}", est))
                        except Exception as e:
                            print(f"[WARN] Skipping model {nm}: {e}")
                    if not estimators:
                        raise ValueError("No valid base estimators available for ensemble after filtering.")
                    min_class_count = None
                    if task == "classification":
                        try:
                            class_counts = pd.Series(y_series).value_counts()
                            min_class_count = int(class_counts.min())
                            print(f"[INFO] class_counts min = {min_class_count}, distribution:{class_counts.to_string()}")
                        except Exception as e:
                            print(f"[WARN] Could not compute class counts: {e}")
                            min_class_count = None
                    filtered_estimators = []
                    for name, est in estimators:
                        nm = name.split("_")[0]
                        if nm == "nusvc" and (min_class_count is not None and min_class_count < 2):
                            print(f"[WARN] Skipping {name} because min_class_count={min_class_count} makes NuSVC infeasible")
                            continue
                        if nm in ("svc","linearsvc") and (min_class_count is not None and min_class_count < 1):
                            print(f"[WARN] Skipping {name} because data too small for SVM")
                            continue
                        filtered_estimators.append((name, est))
      
                    if not filtered_estimators:
                        raise ValueError("All base estimators were filtered out due to data constraints")
                    safe_estimators = []
                    for name, est in filtered_estimators:
                        nm = name.split("_")[0]
                        if nm in ("svc","nusvc","linearsvc","svr","nusvr","linearsvr") and STATIC["TRIAL_FIT_SVM"]:
                            try:
                                print(f"[INFO] Trial-fitting fragile estimator {name} to check feasibility...")
                                with warnings.catch_warnings():
                                    warnings.simplefilter("ignore")
                                    est.fit(X, y_series)
                                safe_estimators.append((name, est))
                                model_params[name] = get_model_params(est)
                            except Exception as e:
                                print(f"[WARN] Trial-fit failed for {name}: {e}. Skipping this estimator.")
                        else:
                            safe_estimators.append((name, est))
      
                    if not safe_estimators:
                        raise ValueError("No base estimators passed trial-fit. Reduce model list or check data.")
                    if len(safe_estimators) == 1:
                        name0, est0 = safe_estimators[0]
                        print(f"[INFO] Only one valid estimator ({name0}) -> training single model")
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            est0.fit(X, y_series)
                        trained_estimators = [(name0, est0)]
                        trained_object = est0
                        model_params[name0] = get_model_params(est0)
                    else:
                        if ensemble_type == "voting":
                            try:
                                if task == "classification":
                                    try:
                                        vote = VotingClassifier(estimators=safe_estimators, voting="soft", n_jobs=-1)
                                        with warnings.catch_warnings():
                                            warnings.simplefilter("ignore")
                                            vote.fit(X, y_series)
                                    except Exception:
                                        vote = VotingClassifier(estimators=safe_estimators, voting="hard", n_jobs=-1)
                                        with warnings.catch_warnings():
                                            warnings.simplefilter("ignore")
                                            vote.fit(X, y_series)
                                    trained_object = vote
                                    trained_base_info = [nm for nm,_ in safe_estimators]
                                    for nm, est in safe_estimators:
                                        model_params[nm] = get_model_params(est)
                                else:
                                    vote = VotingRegressor(estimators=safe_estimators, n_jobs=-1)
                                    with warnings.catch_warnings():
                                        warnings.simplefilter("ignore")
                                        vote.fit(X, y_series)
                                    trained_object = vote
                                    trained_base_info = [nm for nm,_ in safe_estimators]
                                    for nm, est in safe_estimators:
                                        model_params[nm] = get_model_params(est)
                            except Exception as e:
                                print(f"[WARN] Voting ensemble failed: {e}. Training base estimators individually as fallback.")
                                trained_estimators = []
                                for nm, est in safe_estimators:
                                    try:
                                        with warnings.catch_warnings():
                                            warnings.simplefilter("ignore")
                                            est.fit(X, y_series)
                                        trained_estimators.append((nm, est))
                                        model_params[nm] = get_model_params(est)
                                    except Exception as e2:
                                        print(f"[WARN] Base estimator {nm} failed in fallback fit: {e2}")
                                trained_object = None
                        else:
                            final_est = make_meta_estimator(meta_name, task)
                            try:
                                if task == "classification":
                                    stack = StackingClassifier(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                else:
                                    stack = StackingRegressor(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                with warnings.catch_warnings():
                                    warnings.simplefilter("ignore")
                                    stack.fit(X, y_series)
                                trained_object = stack
                                trained_base_info = [nm for nm,_ in safe_estimators]
                                # Extract params from base and meta estimators
                                for nm, est in safe_estimators:
                                    model_params[nm] = get_model_params(est)
                                model_params['meta_estimator'] = get_model_params(final_est)
                            except Exception as e:
                                print(f"[WARN] Initial stacking.fit failed: {e}. Attempting to identify and remove failing base estimators and retry once.")
                                surviving = []
                                for nm, est in safe_estimators:
                                    try:
                                        print(f"[INFO] Trial-fitting estimator {nm} separately...")
                                        with warnings.catch_warnings():
                                            warnings.simplefilter("ignore")
                                            est.fit(X, y_series)
                                        surviving.append((nm, est))
                                        model_params[nm] = get_model_params(est)
                                    except Exception as e2:
                                        print(f"[WARN] Estimator {nm} failed trial-fit: {e2}. Removing it from ensemble.")
                                if len(surviving) >= 2:
                                    try:
                                        if task == "classification":
                                            stack2 = StackingClassifier(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                        else:
                                            stack2 = StackingRegressor(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                        with warnings.catch_warnings():
                                            warnings.simplefilter("ignore")
                                            stack2.fit(X, y_series)
                                        trained_object = stack2
                                        trained_base_info = [nm for nm,_ in surviving]
                                        model_params['meta_estimator'] = get_model_params(final_est)
                                    except Exception as e3:
                                        print(f"[WARN] Retry stacking.fit also failed: {e3}. Falling back to training base estimators individually.")
                                        trained_estimators = []
                                        for nm, est in surviving:
                                            try:
                                                with warnings.catch_warnings():
                                                    warnings.simplefilter("ignore")
                                                    est.fit(X, y_series)
                                                trained_estimators.append((nm, est))
                                            except Exception as e4:
                                                print(f"[WARN] Base estimator {nm} failed in fallback fit: {e4}")
                                        trained_object = None
                                else:
                                    print("[WARN] Not enough surviving base estimators for stacking after filtering. Training surviving estimators individually or single model fallback.")
                                    trained_estimators = []
                                    for nm, est in surviving:
                                        try:
                                            with warnings.catch_warnings():
                                                warnings.simplefilter("ignore")
                                                est.fit(X, y_series)
                                            trained_estimators.append((nm, est))
                                        except Exception as e5:
                                            print(f"[WARN] Base estimator {nm} failed in fallback fit: {e5}")
                                    if len(trained_estimators) == 1:
                                        trained_object = trained_estimators[0][1]
                                    else:
                                        trained_object = None
      
                # compute predictions & metrics for this single target
                metrics = {"task": task, "samples": int(len(y_series)), "features": int(X.shape[1]), "ensemble_type": ensemble_type, "models_used": selected_model_names or [model_to_train], "stacking_cv": stacking_cv, "target": str(target_name)}
                try:
                    predictor = trained_object if trained_object is not None else (None if not trained_estimators else None)
                    if predictor is not None:
                        y_pred = predictor.predict(X)
                    else:
                        if trained_estimators:
                            if task == "regression":
                                preds = []
                                for nm, est in trained_estimators:
                                    preds.append(est.predict(X))
                                y_pred = np.mean(np.vstack(preds), axis=0)
                            else:
                                preds = []
                                for nm, est in trained_estimators:
                                    preds.append(np.array(est.predict(X)))
                                preds = np.vstack(preds)
                                y_pred = stats.mode(preds, axis=0).mode[0].ravel()
                        else:
                            raise ValueError("No predictor available to compute metrics.")
                    if task == "classification":
                        metrics.update({
                            "accuracy_train": float(accuracy_score(y_series, y_pred)),
                            "precision_train": float(precision_score(y_series, y_pred, average='weighted', zero_division=0)),
                            "recall_train": float(recall_score(y_series, y_pred, average='weighted', zero_division=0)),
                            "f1_train": float(f1_score(y_series, y_pred, average='weighted', zero_division=0))
                        })
                    else:
                        metrics.update({
                            "r2_train": float(r2_score(y_series, y_pred)),
                            "rmse_train": float(np.sqrt(mean_squared_error(y_series, y_pred))),
                            "mae_train": float(mean_absolute_error(y_series, y_pred))
                        })
                except Exception as e:
                    print(f"[WARN] Could not compute metrics for target {target_name}: {e}")
                # decide object to save for this target
                save_obj = None
                if trained_object is not None:
                    save_obj = trained_object
                elif trained_estimators:
                    save_obj = {nm: est for nm, est in trained_estimators}
                else:
                    raise ValueError("No trained model or estimators to save for target " + str(target_name))
      
                return save_obj, metrics, trained_base_info, model_params
      
            # Main multi/single target driver
            multi_output = (y_df.shape[1] > 1)
            models_saved = {}
            metrics_out = {}
            base_info_map = {}
            all_model_params = {}  # Store all model parameters
      
            if not multi_output:
                # single-target: use existing logic by calling train_single_target once
                col = y_df.columns[0]
                save_obj, met, info, params = train_single_target(X, y_df.iloc[:,0], col)
                models_saved[col] = save_obj
                metrics_out[col] = met
                base_info_map[col] = info
                all_model_params[col] = params
            else:
                print(f"[INFO] Detected multi-target (n_targets={y_df.shape[1]}). Training one model per target independently.")
                for col in y_df.columns:
                    save_obj, met, info, params = train_single_target(X, y_df[col], col)
                    models_saved[col] = save_obj
                    metrics_out[col] = met
                    base_info_map[col] = info
                    all_model_params[col] = params
      
            # Save the models (one dict for all targets)
            ensure_dir_for(args.model_pickle)
            with open(args.model_pickle, "wb") as fh:
                cloudpickle.dump(models_saved, fh)
      
            # Build aggregated metrics
            aggregate = {"n_targets": len(metrics_out), "samples": int(len(X))}
            # compute mean of numeric metrics across targets
            numeric_keys = {}
            for t, m in metrics_out.items():
                for k, v in m.items():
                    if isinstance(v, (int, float, np.integer, np.floating)):
                        numeric_keys.setdefault(k, []).append(float(v))
            for k, vals in numeric_keys.items():
                try:
                    aggregate[f"{k}_mean"] = float(np.mean(vals))
                except Exception:
                    pass
      
            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as fh:
                json.dump({"per_target": metrics_out, "aggregate": aggregate}, fh, indent=2, ensure_ascii=False)
      
            model_config = {
                "ensemble_type": ensemble_type,
                "models_selected": selected_model_names,
                "meta_model_name": meta_name or ("lightgbm" if (LGBMClassifier or LGBMRegressor) else ("xgboost" if (XGBClassifier or XGBRegressor) else "fallback")),
                "hyperparameters_configured": STATIC,
                "stacking_cv": stacking_cv,
                "random_state": int(args.random_state),
                "multi_output": multi_output,
                "targets": list(y_df.columns),
                "model_parameters": all_model_params  # Add actual model parameters
            }
            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as fh:
                json.dump(model_config, fh, indent=2, ensure_ascii=False)
      
            print("="*80)
            print("SUCCESS: Training finished")
            print(json.dumps({"per_target_metrics_sample": {k: metrics_out[k] for k in list(metrics_out.keys())[:3]}, "aggregate": aggregate}, indent=2))
            print("="*80)
      
        except Exception as e:
            print("ERROR DURING TRAINING:", e, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --ensemble_type
      - {inputValue: ensemble_type}
      - --n_ensemble_models
      - {inputValue: n_ensemble_models}
      - --model_names
      - {inputValue: model_names}
      - --meta_model_name
      - {inputValue: meta_model_name}
      - --stacking_cv
      - {inputValue: stacking_cv}
      - --random_state
      - {inputValue: random_state}
      - --n_estimators
      - {inputValue: n_estimators}
      - --n_estimators_boost
      - {inputValue: n_estimators_boost}
      - --max_depth_tree
      - {inputValue: max_depth_tree}
      - --max_depth_boost
      - {inputValue: max_depth_boost}
      - --min_samples_leaf
      - {inputValue: min_samples_leaf}
      - --min_samples_split
      - {inputValue: min_samples_split}
      - --max_features
      - {inputValue: max_features}
      - --learning_rate
      - {inputValue: learning_rate}
      - --knn_k
      - {inputValue: knn_k}
      - --knn_weights
      - {inputValue: knn_weights}
      - --svm_C
      - {inputValue: svm_C}
      - --svm_gamma
      - {inputValue: svm_gamma}
      - --svm_kernel
      - {inputValue: svm_kernel}
      - --svm_max_iter
      - {inputValue: svm_max_iter}
      - --nu_default
      - {inputValue: nu_default}
      - --ridge_alpha
      - {inputValue: ridge_alpha}
      - --lasso_alpha
      - {inputValue: lasso_alpha}
      - --elasticnet_alpha
      - {inputValue: elasticnet_alpha}
      - --elasticnet_l1_ratio
      - {inputValue: elasticnet_l1_ratio}
      - --mlp_hidden_layers
      - {inputValue: mlp_hidden_layers}
      - --mlp_activation
      - {inputValue: mlp_activation}
      - --mlp_alpha
      - {inputValue: mlp_alpha}
      - --mlp_max_iter
      - {inputValue: mlp_max_iter}
      - --sgd_max_iter
      - {inputValue: sgd_max_iter}
      - --sgd_alpha
      - {inputValue: sgd_alpha}
      - --lgbm_n_estimators
      - {inputValue: lgbm_n_estimators}
      - --lgbm_learning_rate
      - {inputValue: lgbm_learning_rate}
      - --lgbm_max_depth
      - {inputValue: lgbm_max_depth}
      - --lgbm_num_leaves
      - {inputValue: lgbm_num_leaves}
      - --xgb_n_estimators
      - {inputValue: xgb_n_estimators}
      - --xgb_learning_rate
      - {inputValue: xgb_learning_rate}
      - --xgb_max_depth
      - {inputValue: xgb_max_depth}
      - --cat_n_estimators
      - {inputValue: cat_n_estimators}
      - --cat_learning_rate
      - {inputValue: cat_learning_rate}
      - --cat_depth
      - {inputValue: cat_depth}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
