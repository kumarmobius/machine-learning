name: Stacking and Voting V4
inputs:
  - {name: X_data, type: Dataset}
  - {name: y_data, type: Dataset}
  - {name: model_type, type: String, optional: true}
  - {name: ensemble_type, type: String, description: "none | stacking | voting", optional: true, default: "stacking"}
  - {name: n_ensemble_models, type: Integer, description: "If >1 and model_names not provided, randomly pick this many models from default pool", optional: true, default: "0"}
  - {name: model_names, type: String, description: "Optional comma-separated list of model names to use verbatim (overrides n_ensemble_models)", optional: true, default: ""}
  - {name: meta_model_name, type: String, description: "Optional meta-model name for stacking", optional: true, default: ""}
  - {name: stacking_cv, type: Integer, default: "5"}
  - {name: random_state, type: Integer, optional: true, default: "50"}
outputs:
  - {name: model_pickle, type: Data }
  - {name: metrics_json, type: Data }
  - {name: model_config, type: Data }

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, json, os, sys, traceback, random, warnings
        
        # Suppress ALL warnings at the very beginning
        warnings.filterwarnings('ignore')
        os.environ['PYTHONWARNINGS'] = 'ignore'
        
        import pandas as pd, numpy as np, joblib
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import (
            ExtraTreesClassifier, ExtraTreesRegressor,
            RandomForestClassifier, RandomForestRegressor,
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor,
            BaggingClassifier, BaggingRegressor,
            VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor,
            HistGradientBoostingClassifier, HistGradientBoostingRegressor
        )
        from sklearn.linear_model import (
            LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor,
            Ridge, RidgeClassifier, Lasso, ElasticNet, 
            BayesianRidge, HuberRegressor, PassiveAggressiveClassifier, PassiveAggressiveRegressor
        )
        from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
        from sklearn.neural_network import MLPClassifier, MLPRegressor
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR, NuSVR
        from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
        from scipy import stats
      
        try:
            from lightgbm import LGBMClassifier, LGBMRegressor
            LIGHTGBM_AVAILABLE = True
        except Exception:
            LGBMClassifier = None
            LGBMRegressor = None
            LIGHTGBM_AVAILABLE = False
        try:
            from xgboost import XGBClassifier, XGBRegressor
            XGBOOST_AVAILABLE = True
        except Exception:
            XGBClassifier = None
            XGBRegressor = None
            XGBOOST_AVAILABLE = False
        try:
            from catboost import CatBoostClassifier, CatBoostRegressor
            CATBOOST_AVAILABLE = True
        except Exception:
            CatBoostClassifier = None
            CatBoostRegressor = None
            CATBOOST_AVAILABLE = False
        
        # Print library availability once
        print(f"[INFO] Library availability - XGBoost: {XGBOOST_AVAILABLE}, LightGBM: {LIGHTGBM_AVAILABLE}, CatBoost: {CATBOOST_AVAILABLE}")
      
        def load_parquet_df(path):
            return pd.read_parquet(path, engine="auto")
      
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
      
        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', required=True)
        ap.add_argument('--y_data', required=True)
        ap.add_argument('--model_type', default="classification")
        ap.add_argument('--ensemble_type', default="none")
        ap.add_argument('--n_ensemble_models', type=int, default=0)
        ap.add_argument('--model_names', default="")
        ap.add_argument('--meta_model_name', default="")
        ap.add_argument('--stacking_cv', type=int, default=5)
        ap.add_argument('--random_state', type=int, default=42)
        ap.add_argument('--model_pickle', required=True)
        ap.add_argument('--metrics_json', required=True)
        ap.add_argument('--model_config', required=True)
        args = ap.parse_args()
      
        try:
            random.seed(int(args.random_state))
            np.random.seed(int(args.random_state))
      
            STATIC = {
                "n_estimators": 300,
                "min_samples_leaf": 1,
                "max_depth_tree": 8,
                "knn_k": 7,
                "svm_C": 1.0,
                "svm_gamma": "scale",
                "nu_default": 0.5,
                "meta_lgbm_n_estimators": 200,
                "meta_xgb_n_estimators": 200,
                "meta_cat_n_estimators": 200,
                "mlp_hidden_layer_sizes": (256,128),
                "sgd_max_iter": 2000,
                "ridge_alpha": 1.0,
                "lasso_alpha": 0.1,
                "elasticnet_alpha": 0.1,
                "elasticnet_l1_ratio": 0.5,
                "huber_epsilon": 1.35,
                "linearsvr_max_iter": 50000,
                "linearsvr_tol": 1e-4,
                "STACKING_CV": int(args.stacking_cv or 5),
                "TRIAL_FIT_SVM": True
            }
      
            # EXPANDED ALGORITHM POOLS
            DEFAULT_POOL_CLASSIF = [
                # Tree-based
                "extratrees", "randomforest", "decisiontree", "gradientboosting", "histgradientboosting",
                # Boosting
                "adaboost",
                # Ensemble
                "bagging",
                # SVM
                "svc", "nusvc", "linearsvc",
                # Neighbors
                "knn",
                # Linear
                "logistic", "ridge_classifier", "sgd", "passive_aggressive",
                # Naive Bayes
                "gaussiannb", "bernoullinb",
                # Discriminant Analysis
                "lda", "qda",
                # Neural Network
                "mlp",
                # Gradient Boosting Libraries
                "xgboost", "lightgbm", "catboost"
            ]
            
            DEFAULT_POOL_REGR = [
                # Tree-based
                "extratrees", "randomforest", "decisiontree", "gradientboosting", "histgradientboosting",
                # Boosting
                "adaboost",
                # Ensemble
                "bagging",
                # Neighbors
                "knn",
                # Linear
                "linearreg", "ridge", "lasso", "elasticnet", "bayesianridge", "huber", "sgd", "passive_aggressive",
                # SVM
                "svr", "nusvr", "linearsvr",
                # Neural Network
                "mlp",
                # Gradient Boosting Libraries
                "xgboost", "lightgbm", "catboost"
            ]
      
            X = load_parquet_df(args.X_data)
            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                y_df = y_df.copy()
            else:
                y_df = pd.DataFrame({"target": pd.Series(y_df).copy()})
      
            print(f"[INFO] X shape: {X.shape}, y shape: {y_df.shape}")
      
            task = args.model_type.strip().lower()
            ensemble_type = args.ensemble_type.strip().lower()
            n_pick = int(args.n_ensemble_models or 0)
            model_names_input = (args.model_names or "").strip()
            meta_name = (args.meta_model_name or "").strip().lower()
            stacking_cv = int(args.stacking_cv or STATIC["STACKING_CV"])
      
            def choose_pool(task, n_pick, model_names_input):
                if model_names_input:
                    selected_model_names = [m.strip().lower() for m in model_names_input.split(",") if m.strip()]
                    print(f"[INFO] Using provided model_names: {selected_model_names}")
                    return selected_model_names
                pool = DEFAULT_POOL_CLASSIF if task == "classification" else DEFAULT_POOL_REGR
                if n_pick > 1:
                    if n_pick > len(pool):
                        print(f"[WARN] n_ensemble_models={n_pick} > pool size {len(pool)}. Using full pool.")
                        return pool.copy()
                    else:
                        sel = random.sample(pool, n_pick)
                        print(f"[INFO] Randomly selected {len(sel)} models: {sel}")
                        return sel
                elif n_pick == 1:
                    sel = [random.choice(pool)]
                    print(f"[INFO] n_ensemble_models==1 -> randomly picked single model: {sel}")
                    return sel
                else:
                    if ensemble_type in ("stacking","voting"):
                        print(f"[INFO] No model_names/n specified; using full default pool for ensemble.")
                        return pool.copy()
                    else:
                        print("[INFO] No ensemble requested and no model_names provided -> will train single default model 'extratrees'")
                        return []
      
            selected_model_names = choose_pool(task, n_pick, model_names_input)
      
            def make_base_estimator(name, task):
                n = name.strip().lower()
                rs = args.random_state
                
                if task == "classification":
                    # Tree-based
                    if n == "extratrees":
                        return ExtraTreesClassifier(n_estimators=STATIC["n_estimators"], min_samples_leaf=STATIC["min_samples_leaf"], n_jobs=-1, random_state=rs, class_weight='balanced')
                    if n == "randomforest":
                        return RandomForestClassifier(n_estimators=STATIC["n_estimators"], max_depth=STATIC["max_depth_tree"], n_jobs=-1, random_state=rs, class_weight='balanced')
                    if n == "decisiontree":
                        return DecisionTreeClassifier(max_depth=STATIC["max_depth_tree"], class_weight='balanced', random_state=rs)
                    if n == "gradientboosting":
                        return GradientBoostingClassifier(n_estimators=int(STATIC["n_estimators"]/2), max_depth=6, random_state=rs)
                    if n == "histgradientboosting":
                        return HistGradientBoostingClassifier(max_iter=200, max_depth=8, random_state=rs)
                    
                    # Boosting
                    if n == "adaboost":
                        return AdaBoostClassifier(n_estimators=STATIC["n_estimators"], random_state=rs)
                    
                    # Ensemble
                    if n == "bagging":
                        return BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=rs)
                    
                    # SVM
                    if n == "svc":
                        return SVC(C=STATIC["svm_C"], gamma=STATIC["svm_gamma"], probability=True, class_weight='balanced', random_state=rs)
                    if n == "nusvc":
                        return NuSVC(nu=STATIC["nu_default"], probability=True, class_weight='balanced', random_state=rs)
                    if n == "linearsvc":
                        return LinearSVC(C=STATIC["svm_C"], max_iter=STATIC["linearsvr_max_iter"], tol=STATIC["linearsvr_tol"], dual=False)
                    
                    # Neighbors
                    if n == "knn":
                        return KNeighborsClassifier(n_neighbors=STATIC["knn_k"], n_jobs=-1)
                    
                    # Linear
                    if n == "logistic":
                        return LogisticRegression(max_iter=2000, n_jobs=-1, random_state=rs)
                    if n == "ridge_classifier":
                        return RidgeClassifier(alpha=STATIC["ridge_alpha"], random_state=rs)
                    if n == "sgd":
                        return SGDClassifier(max_iter=STATIC["sgd_max_iter"], tol=1e-3, random_state=rs, n_jobs=-1)
                    if n == "passive_aggressive":
                        return PassiveAggressiveClassifier(max_iter=1000, random_state=rs)
                    
                    # Naive Bayes
                    if n == "gaussiannb":
                        return GaussianNB()
                    if n == "bernoullinb":
                        return BernoulliNB()
                    
                    # Discriminant Analysis
                    if n == "lda":
                        return LinearDiscriminantAnalysis()
                    if n == "qda":
                        return QuadraticDiscriminantAnalysis()
                    
                    # Neural Network
                    if n == "mlp":
                        return MLPClassifier(hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"], max_iter=500, random_state=rs)
                    
                    # Gradient Boosting Libraries
                    if n == "xgboost" and XGBOOST_AVAILABLE:
                        return XGBClassifier(n_estimators=STATIC["meta_xgb_n_estimators"], use_label_encoder=False, eval_metric='logloss', random_state=rs)
                    if n == "lightgbm" and LIGHTGBM_AVAILABLE:
                        return LGBMClassifier(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=rs, verbose=-1)
                    if n == "catboost" and CATBOOST_AVAILABLE:
                        return CatBoostClassifier(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=rs)
                    
                    if n in ["xgboost", "lightgbm", "catboost"]:
                        # Silently skip unavailable libraries - already logged at startup
                        raise ValueError(f"Library not available: {name}")
                    
                    raise ValueError(f"Unsupported classifier name: {name}")
                
                else:  # regression
                    # Tree-based
                    if n == "extratrees":
                        return ExtraTreesRegressor(n_estimators=STATIC["n_estimators"], min_samples_leaf=STATIC["min_samples_leaf"], n_jobs=-1, random_state=rs)
                    if n == "randomforest":
                        return RandomForestRegressor(n_estimators=STATIC["n_estimators"], max_depth=STATIC["max_depth_tree"], n_jobs=-1, random_state=rs)
                    if n == "decisiontree":
                        return DecisionTreeRegressor(max_depth=STATIC["max_depth_tree"], random_state=rs)
                    if n == "gradientboosting":
                        return GradientBoostingRegressor(n_estimators=int(STATIC["n_estimators"]/2), max_depth=6, random_state=rs)
                    if n == "histgradientboosting":
                        return HistGradientBoostingRegressor(max_iter=200, max_depth=8, random_state=rs)
                    
                    # Boosting
                    if n == "adaboost":
                        return AdaBoostRegressor(n_estimators=STATIC["n_estimators"], random_state=rs)
                    
                    # Ensemble
                    if n == "bagging":
                        return BaggingRegressor(n_estimators=100, n_jobs=-1, random_state=rs)
                    
                    # Neighbors
                    if n == "knn":
                        return KNeighborsRegressor(n_neighbors=STATIC["knn_k"], n_jobs=-1)
                    
                    # Linear
                    if n in ("linearreg", "linear"):
                        return LinearRegression(n_jobs=-1)
                    if n == "ridge":
                        return Ridge(alpha=STATIC["ridge_alpha"], random_state=rs)
                    if n == "lasso":
                        return Lasso(alpha=STATIC["lasso_alpha"], max_iter=2000, random_state=rs)
                    if n == "elasticnet":
                        return ElasticNet(alpha=STATIC["elasticnet_alpha"], l1_ratio=STATIC["elasticnet_l1_ratio"], max_iter=2000, random_state=rs)
                    if n == "bayesianridge":
                        return BayesianRidge()
                    if n == "huber":
                        return HuberRegressor(epsilon=STATIC["huber_epsilon"], max_iter=200)
                    if n == "sgd":
                        return SGDRegressor(max_iter=STATIC["sgd_max_iter"], tol=1e-3, random_state=rs)
                    if n == "passive_aggressive":
                        return PassiveAggressiveRegressor(max_iter=1000, random_state=rs)
                    
                    # SVM
                    if n == "svr":
                        return SVR(C=STATIC["svm_C"], gamma=STATIC["svm_gamma"])
                    if n == "nusvr":
                        return NuSVR(nu=STATIC["nu_default"], C=STATIC["svm_C"], gamma=STATIC["svm_gamma"])
                    if n == "linearsvr":
                        return LinearSVR(C=STATIC["svm_C"], max_iter=STATIC["linearsvr_max_iter"], tol=STATIC["linearsvr_tol"], dual=False)
                    
                    # Neural Network
                    if n == "mlp":
                        return MLPRegressor(hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"], max_iter=500, random_state=rs)
                    
                    # Gradient Boosting Libraries
                    if n == "xgboost" and XGBOOST_AVAILABLE:
                        return XGBRegressor(n_estimators=STATIC["meta_xgb_n_estimators"], random_state=rs)
                    if n == "lightgbm" and LIGHTGBM_AVAILABLE:
                        return LGBMRegressor(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=rs, verbose=-1)
                    if n == "catboost" and CATBOOST_AVAILABLE:
                        return CatBoostRegressor(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=rs)
                    
                    if n in ["xgboost", "lightgbm", "catboost"]:
                        # Silently skip unavailable libraries - already logged at startup
                        raise ValueError(f"Library not available: {name}")
                    
                    raise ValueError(f"Unsupported regressor name: {name}")
      
            def make_meta_estimator(name, task):
                n = (name or "").strip().lower()
                rs = args.random_state
                
                if n in ("", "lightgbm", "lgbm", "lgb"):
                    if task == "classification" and LIGHTGBM_AVAILABLE:
                        return LGBMClassifier(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=rs, verbose=-1)
                    if task == "regression" and LIGHTGBM_AVAILABLE:
                        return LGBMRegressor(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=rs, verbose=-1)
                
                if n in ("xgboost", "xgb"):
                    if task == "classification" and XGBOOST_AVAILABLE:
                        return XGBClassifier(n_estimators=STATIC["meta_xgb_n_estimators"], use_label_encoder=False, eval_metric='logloss', random_state=rs)
                    if task == "regression" and XGBOOST_AVAILABLE:
                        return XGBRegressor(n_estimators=STATIC["meta_xgb_n_estimators"], random_state=rs)
                
                if n in ("catboost", "cat"):
                    if task == "classification" and CATBOOST_AVAILABLE:
                        return CatBoostClassifier(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=rs)
                    if task == "regression" and CATBOOST_AVAILABLE:
                        return CatBoostRegressor(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=rs)
                
                if task == "classification":
                    return LogisticRegression(max_iter=2000, random_state=rs)
                else:
                    return Ridge(alpha=1.0, random_state=rs)
      
            def train_single_target(X, y_series, target_name):
                print(f"[INFO] Training target: {target_name} (n_samples={len(y_series)})")
                trained_object = None
                trained_estimators = []
                trained_base_info = []
                model_to_train = None
      
                if ensemble_type not in ("stacking", "voting"):
                    if selected_model_names:
                        model_to_train = selected_model_names[0]
                    else:
                        model_to_train = "extratrees"
                    print(f"[INFO] Training single model: {model_to_train} for target {target_name}")
                    model_inst = make_base_estimator(model_to_train, task)
                    if task == "classification":
                        try:
                            sw = compute_sample_weight(class_weight='balanced', y=pd.Series(y_series).astype(int))
                        except Exception:
                            sw = None
                        if sw is not None:
                            try:
                                model_inst.fit(X, y_series, sample_weight=sw)
                            except TypeError:
                                model_inst.fit(X, y_series)
                        else:
                            model_inst.fit(X, y_series)
                    else:
                        model_inst.fit(X, y_series)
                    trained_object = model_inst
                    trained_base_info = [model_to_train]
                else:
                    if not selected_model_names:
                        raise ValueError("Ensemble requested but no base models selected.")
                    estimators = []
                    for i, nm in enumerate(selected_model_names):
                        try:
                            est = make_base_estimator(nm, task)
                            estimators.append((f"{nm}_{i}", est))
                        except Exception as e:
                            print(f"[WARN] Skipping model {nm}: {e}")
                    
                    if not estimators:
                        raise ValueError("No valid base estimators available for ensemble after filtering.")
                    
                    min_class_count = None
                    if task == "classification":
                        try:
                            class_counts = pd.Series(y_series).value_counts()
                            min_class_count = int(class_counts.min())
                            print(f"[INFO] class_counts min = {min_class_count}, distribution:{class_counts.to_string()}")
                        except Exception as e:
                            print(f"[WARN] Could not compute class counts: {e}")
                            min_class_count = None
                    
                    filtered_estimators = []
                    for name, est in estimators:
                        nm = name.split("_")[0]
                        if nm == "nusvc" and (min_class_count is not None and min_class_count < 2):
                            print(f"[WARN] Skipping {name} because min_class_count={min_class_count} makes NuSVC infeasible")
                            continue
                        if nm in ("svc", "linearsvc") and (min_class_count is not None and min_class_count < 1):
                            print(f"[WARN] Skipping {name} because data too small for SVM")
                            continue
                        filtered_estimators.append((name, est))
      
                    if not filtered_estimators:
                        raise ValueError("All base estimators were filtered out due to data constraints")
                    
                    safe_estimators = []
                    for name, est in filtered_estimators:
                        nm = name.split("_")[0]
                        if nm in ("svc", "nusvc", "linearsvc", "svr", "nusvr", "linearsvr") and STATIC["TRIAL_FIT_SVM"]:
                            try:
                                print(f"[INFO] Trial-fitting fragile estimator {name} to check feasibility...")
                                est.fit(X, y_series)
                                safe_estimators.append((name, est))
                            except Exception as e:
                                print(f"[WARN] Trial-fit failed for {name}: {e}. Skipping this estimator.")
                        else:
                            safe_estimators.append((name, est))
      
                    if not safe_estimators:
                        raise ValueError("No base estimators passed trial-fit. Reduce model list or check data.")
                    
                    if len(safe_estimators) == 1:
                        name0, est0 = safe_estimators[0]
                        print(f"[INFO] Only one valid estimator ({name0}) -> training single model")
                        est0.fit(X, y_series)
                        trained_estimators = [(name0, est0)]
                        trained_object = est0
                    else:
                        if ensemble_type == "voting":
                            try:
                                if task == "classification":
                                    try:
                                        vote = VotingClassifier(estimators=safe_estimators, voting="soft", n_jobs=-1)
                                        vote.fit(X, y_series)
                                    except Exception:
                                        vote = VotingClassifier(estimators=safe_estimators, voting="hard", n_jobs=-1)
                                        vote.fit(X, y_series)
                                    trained_object = vote
                                    trained_base_info = [nm for nm, _ in safe_estimators]
                                else:
                                    vote = VotingRegressor(estimators=safe_estimators, n_jobs=-1)
                                    vote.fit(X, y_series)
                                    trained_object = vote
                                    trained_base_info = [nm for nm, _ in safe_estimators]
                            except Exception as e:
                                print(f"[WARN] Voting ensemble failed: {e}. Training base estimators individually as fallback.")
                                trained_estimators = []
                                for nm, est in safe_estimators:
                                    try:
                                        est.fit(X, y_series)
                                        trained_estimators.append((nm, est))
                                    except Exception as e2:
                                        print(f"[WARN] Base estimator {nm} failed in fallback fit: {e2}")
                                trained_object = None
                        else:
                            final_est = make_meta_estimator(meta_name, task)
                            try:
                                if task == "classification":
                                    stack = StackingClassifier(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                else:
                                    stack = StackingRegressor(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                stack.fit(X, y_series)
                                trained_object = stack
                                trained_base_info = [nm for nm, _ in safe_estimators]
                            except Exception as e:
                                print(f"[WARN] Initial stacking.fit failed: {e}. Attempting to identify and remove failing base estimators and retry once.")
                                surviving = []
                                for nm, est in safe_estimators:
                                    try:
                                        print(f"[INFO] Trial-fitting estimator {nm} separately...")
                                        est.fit(X, y_series)
                                        surviving.append((nm, est))
                                    except Exception as e2:
                                        print(f"[WARN] Estimator {nm} failed trial-fit: {e2}. Removing it from ensemble.")
                                
                                if len(surviving) >= 2:
                                    try:
                                        if task == "classification":
                                            stack2 = StackingClassifier(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                        else:
                                            stack2 = StackingRegressor(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                        stack2.fit(X, y_series)
                                        trained_object = stack2
                                        trained_base_info = [nm for nm, _ in surviving]
                                    except Exception as e3:
                                        print(f"[WARN] Retry stacking.fit also failed: {e3}. Falling back to training base estimators individually.")
                                        trained_estimators = []
                                    for nm, est in surviving:
                                        try:
                                            est.fit(X, y_series)
                                            trained_estimators.append((nm, est))
                                        except Exception as e5:
                                            print(f"[WARN] Base estimator {nm} failed in fallback fit: {e5}")
                                    if len(trained_estimators) == 1:
                                        trained_object = trained_estimators[0][1]
                                    else:
                                        trained_object = None
      
                metrics = {
                    "task": task, 
                    "samples": int(len(y_series)), 
                    "features": int(X.shape[1]), 
                    "ensemble_type": ensemble_type, 
                    "models_used": selected_model_names or [model_to_train], 
                    "stacking_cv": stacking_cv, 
                    "target": str(target_name)
                }
                
                try:
                    predictor = trained_object if trained_object is not None else (None if not trained_estimators else None)
                    if predictor is not None:
                        y_pred = predictor.predict(X)
                    else:
                        if trained_estimators:
                            if task == "regression":
                                preds = []
                                for nm, est in trained_estimators:
                                    preds.append(est.predict(X))
                                y_pred = np.mean(np.vstack(preds), axis=0)
                            else:
                                preds = []
                                for nm, est in trained_estimators:
                                    preds.append(np.array(est.predict(X)))
                                preds = np.vstack(preds)
                                y_pred = stats.mode(preds, axis=0).mode[0].ravel()
                        else:
                            raise ValueError("No predictor available to compute metrics.")
                    
                    if task == "classification":
                        metrics.update({
                            "accuracy_train": float(accuracy_score(y_series, y_pred)),
                            "precision_train": float(precision_score(y_series, y_pred, average='weighted', zero_division=0)),
                            "recall_train": float(recall_score(y_series, y_pred, average='weighted', zero_division=0)),
                            "f1_train": float(f1_score(y_series, y_pred, average='weighted', zero_division=0))
                        })
                    else:
                        metrics.update({
                            "r2_train": float(r2_score(y_series, y_pred)),
                            "rmse_train": float(np.sqrt(mean_squared_error(y_series, y_pred))),
                            "mae_train": float(mean_absolute_error(y_series, y_pred))
                        })
                except Exception as e:
                    print(f"[WARN] Could not compute metrics for target {target_name}: {e}")
                
                save_obj = None
                if trained_object is not None:
                    save_obj = trained_object
                elif trained_estimators:
                    save_obj = {nm: est for nm, est in trained_estimators}
                else:
                    raise ValueError("No trained model or estimators to save for target " + str(target_name))
      
                return save_obj, metrics, trained_base_info
      
            multi_output = (y_df.shape[1] > 1)
            models_saved = {}
            metrics_out = {}
            base_info_map = {}
      
            if not multi_output:
                col = y_df.columns[0]
                save_obj, met, info = train_single_target(X, y_df.iloc[:, 0], col)
                models_saved[col] = save_obj
                metrics_out[col] = met
                base_info_map[col] = info
            else:
                print(f"[INFO] Detected multi-target (n_targets={y_df.shape[1]}). Training one model per target independently.")
                for col in y_df.columns:
                    save_obj, met, info = train_single_target(X, y_df[col], col)
                    models_saved[col] = save_obj
                    metrics_out[col] = met
                    base_info_map[col] = info
      
            ensure_dir_for(args.model_pickle)
            joblib.dump(models_saved, args.model_pickle)
      
            aggregate = {"n_targets": len(metrics_out), "samples": int(len(X))}
            numeric_keys = {}
            for t, m in metrics_out.items():
                for k, v in m.items():
                    if isinstance(v, (int, float, np.integer, np.floating)):
                        numeric_keys.setdefault(k, []).append(float(v))
            for k, vals in numeric_keys.items():
                try:
                    aggregate[f"{k}_mean"] = float(np.mean(vals))
                except Exception:
                    pass
      
            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as fh:
                json.dump({"per_target": metrics_out, "aggregate": aggregate}, fh, indent=2, ensure_ascii=False)
      
            model_config = {
                "ensemble_type": ensemble_type,
                "models_selected": selected_model_names,
                "meta_model_name": meta_name or ("lightgbm" if LIGHTGBM_AVAILABLE else ("xgboost" if XGBOOST_AVAILABLE else "fallback")),
                "static_defaults": STATIC,
                "stacking_cv": stacking_cv,
                "random_state": int(args.random_state),
                "multi_output": multi_output,
                "targets": list(y_df.columns),
                "libraries_available": {
                    "lightgbm": LIGHTGBM_AVAILABLE,
                    "xgboost": XGBOOST_AVAILABLE,
                    "catboost": CATBOOST_AVAILABLE
                }
            }
            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as fh:
                json.dump(model_config, fh, indent=2, ensure_ascii=False)
      
            print("=" * 80)
            print("SUCCESS: Training finished")
            print(json.dumps({
                "per_target_metrics_sample": {k: metrics_out[k] for k in list(metrics_out.keys())[:3]}, 
                "aggregate": aggregate
            }, indent=2))
            print("=" * 80)
      
        except Exception as e:
            print("ERROR DURING TRAINING:", e, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --ensemble_type
      - {inputValue: ensemble_type}
      - --n_ensemble_models
      - {inputValue: n_ensemble_models}
      - --model_names
      - {inputValue: model_names}
      - --meta_model_name
      - {inputValue: meta_model_name}
      - --stacking_cv
      - {inputValue: stacking_cv}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config} []
                                    trained_estimators =
