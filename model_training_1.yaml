name: Stacking and Voting V2
inputs:
  - {name: X_data, type: Dataset}
  - {name: y_data, type: Dataset}
  - {name: model_type, type: String, optional: true}
  - {name: ensemble_type, type: String, description: "none | stacking | voting", optional: true, default: "stacking"}
  - {name: n_ensemble_models, type: Integer, description: "If >1 and model_names not provided, randomly pick this many models from default pool", optional: true, default: "0"}
  - {name: model_names, type: String, description: "Optional comma-separated list of model names to use verbatim (overrides n_ensemble_models)", optional: true, default: ""}
  - {name: meta_model_name, type: String, description: "Optional meta-model name for stacking (e.g. 'lightgbm' or 'logistic'). If absent, prefer LightGBM/XGBoost when available, else fallback.", optional: true, default: ""}
  - {name: stacking_cv, type: Integer, default: "5"}
  - {name: random_state, type: Integer, optional: true, default: "50"}
outputs:
  - {name: model_pickle, type: Data }
  - {name: metrics_json, type: Data }
  - {name: model_config, type: Data }

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, json, os, sys, traceback, random
        import pandas as pd, numpy as np, joblib
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import (
            ExtraTreesClassifier, ExtraTreesRegressor,
            RandomForestClassifier, RandomForestRegressor,
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor,
            BaggingClassifier, BaggingRegressor,
            VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor
        )
        from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor
        from sklearn.naive_bayes import GaussianNB
        from sklearn.neural_network import MLPClassifier, MLPRegressor
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR
        from scipy import stats

        try:
            from lightgbm import LGBMClassifier, LGBMRegressor
        except Exception:
            LGBMClassifier = None
            LGBMRegressor = None
        try:
            from xgboost import XGBClassifier, XGBRegressor
        except Exception:
            XGBClassifier = None
            XGBRegressor = None
        try:
            from catboost import CatBoostClassifier, CatBoostRegressor
        except Exception:
            CatBoostClassifier = None
            CatBoostRegressor = None

        def load_parquet_df(path):
            return pd.read_parquet(path, engine="auto")

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', required=True)
        ap.add_argument('--y_data', required=True)
        ap.add_argument('--model_type', default="classification")
        ap.add_argument('--ensemble_type', default="none")
        ap.add_argument('--n_ensemble_models', type=int, default=0)
        ap.add_argument('--model_names', default="")
        ap.add_argument('--meta_model_name', default="")
        ap.add_argument('--stacking_cv', type=int, default=5)
        ap.add_argument('--random_state', type=int, default=42)
        ap.add_argument('--model_pickle', required=True)
        ap.add_argument('--metrics_json', required=True)
        ap.add_argument('--model_config', required=True)
        args = ap.parse_args()

        try:
            random.seed(int(args.random_state))
            np.random.seed(int(args.random_state))

            STATIC = {
                "n_estimators": 300,    
                "min_samples_leaf": 1,
                "max_depth_tree": 8,  
                "knn_k": 7,
                "svm_C": 1.0,
                "svm_gamma": "scale",
                "nu_default": 0.5,
                "meta_lgbm_n_estimators": 200,
                "meta_xgb_n_estimators": 200,
                "meta_cat_n_estimators": 200,
                "mlp_hidden_layer_sizes": (256,128),
                "sgd_max_iter": 1000,
                "STACKING_CV": int(args.stacking_cv or 5),
                "TRIAL_FIT_SVM": True
            }

            DEFAULT_POOL_CLASSIF = [
                "extratrees","randomforest","decisiontree","adaboost","gradientboosting","bagging",
                "svc","nusvc","linearsvc","knn","logistic","sgd","gaussiannb","mlp",
                "xgboost","lightgbm","catboost"
            ]
            DEFAULT_POOL_REGR = [
                "extratrees","randomforest","decisiontree","adaboost","gradientboosting","bagging",
                "knn","linearreg","svr","linearsvr","sgd","mlp","xgboost","lightgbm","catboost"
            ]

            X = load_parquet_df(args.X_data)
            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1:
                    y = y_df.iloc[:,0].copy()
                else:
                    raise ValueError("y_data has multiple columns â€” provide a single-column parquet (preprocessed/encoded).")
            else:
                y = pd.Series(y_df).copy()

            print(f"[INFO] X shape: {X.shape}, y shape: {y.shape}")

            task = args.model_type.strip().lower()
            ensemble_type = args.ensemble_type.strip().lower()
            n_pick = int(args.n_ensemble_models or 0)
            model_names_input = (args.model_names or "").strip()
            meta_name = (args.meta_model_name or "").strip().lower()
            stacking_cv = int(args.stacking_cv or STATIC["STACKING_CV"])
            if model_names_input:
                selected_model_names = [m.strip().lower() for m in model_names_input.split(",") if m.strip()]
                print(f"[INFO] Using provided model_names: {selected_model_names}")
            else:
                pool = DEFAULT_POOL_CLASSIF if task == "classification" else DEFAULT_POOL_REGR
                if n_pick > 1:
                    if n_pick > len(pool):
                        print(f"[WARN] n_ensemble_models={n_pick} > pool size {len(pool)}. Using full pool.")
                        selected_model_names = pool.copy()
                    else:
                        selected_model_names = random.sample(pool, n_pick)
                    print(f"[INFO] Randomly selected {len(selected_model_names)} models: {selected_model_names}")
                elif n_pick == 1:
                    selected_model_names = [random.choice(pool)]
                    print(f"[INFO] n_ensemble_models==1 -> randomly picked single model: {selected_model_names}")
                else:
                    if ensemble_type in ("stacking","voting"):
                        selected_model_names = pool.copy()
                        print(f"[INFO] No model_names/n specified; using full default pool for ensemble: {selected_model_names}")
                    else:
                        selected_model_names = []
                        print("[INFO] No ensemble requested and no model_names provided -> will train single default model 'extratrees'")

            def make_base_estimator(name):
                n = name.strip().lower()
                if task == "classification":
                    if n == "knn":
                        return KNeighborsClassifier(n_neighbors=STATIC["knn_k"], n_jobs=-1)
                    if n == "decisiontree":
                        return DecisionTreeClassifier(max_depth=STATIC["max_depth_tree"], class_weight='balanced', random_state=args.random_state)
                    if n == "extratrees":
                        return ExtraTreesClassifier(n_estimators=STATIC["n_estimators"], min_samples_leaf=STATIC["min_samples_leaf"], n_jobs=-1, random_state=args.random_state, class_weight='balanced')
                    if n == "randomforest":
                        return RandomForestClassifier(n_estimators=STATIC["n_estimators"], max_depth=STATIC["max_depth_tree"], n_jobs=-1, random_state=args.random_state, class_weight='balanced')
                    if n == "adaboost":
                        return AdaBoostClassifier(n_estimators=STATIC["n_estimators"], random_state=args.random_state)
                    if n == "gradientboosting":
                        return GradientBoostingClassifier(n_estimators=int(STATIC["n_estimators"]/2), max_depth=6, random_state=args.random_state)
                    if n == "bagging":
                        return BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=args.random_state)
                    if n == "svc":
                        return SVC(C=STATIC["svm_C"], gamma=STATIC["svm_gamma"], probability=True, class_weight='balanced', random_state=args.random_state)
                    if n == "nusvc":
                        return NuSVC(nu=STATIC["nu_default"], probability=True, class_weight='balanced', random_state=args.random_state)
                    if n == "linearsvc":
                        return LinearSVC(C=STATIC["svm_C"], max_iter=10000)
                    if n == "logistic":
                        return LogisticRegression(max_iter=2000, n_jobs=-1, random_state=args.random_state)
                    if n == "sgd":
                        return SGDClassifier(max_iter=STATIC["sgd_max_iter"], tol=1e-3, random_state=args.random_state)
                    if n == "gaussiannb":
                        return GaussianNB()
                    if n == "mlp":
                        return MLPClassifier(hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"], max_iter=500, random_state=args.random_state)
                    if n == "xgboost" and XGBClassifier is not None:
                        return XGBClassifier(n_estimators=STATIC["meta_xgb_n_estimators"], use_label_encoder=False, eval_metric='logloss', random_state=args.random_state)
                    if n == "lightgbm" and LGBMClassifier is not None:
                        return LGBMClassifier(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=args.random_state)
                    if n == "catboost" and CatBoostClassifier is not None:
                        return CatBoostClassifier(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=args.random_state)
                    raise ValueError(f"Unsupported classifier name: {name}")
                else:
                    if n == "knn":
                        return KNeighborsRegressor(n_neighbors=STATIC["knn_k"], n_jobs=-1)
                    if n == "decisiontree":
                        return DecisionTreeRegressor(max_depth=STATIC["max_depth_tree"], random_state=args.random_state)
                    if n == "extratrees":
                        return ExtraTreesRegressor(n_estimators=STATIC["n_estimators"], min_samples_leaf=STATIC["min_samples_leaf"], n_jobs=-1, random_state=args.random_state)
                    if n == "randomforest":
                        return RandomForestRegressor(n_estimators=STATIC["n_estimators"], max_depth=STATIC["max_depth_tree"], n_jobs=-1, random_state=args.random_state)
                    if n == "adaboost":
                        return AdaBoostRegressor(n_estimators=STATIC["n_estimators"], random_state=args.random_state)
                    if n == "gradientboosting":
                        return GradientBoostingRegressor(n_estimators=int(STATIC["n_estimators"]/2), max_depth=6, random_state=args.random_state)
                    if n == "bagging":
                        return BaggingRegressor(n_estimators=100, n_jobs=-1, random_state=args.random_state)
                    if n == "svr":
                        return SVR(C=STATIC["svm_C"], gamma=STATIC["svm_gamma"])
                    if n == "linearsvr":
                        return LinearSVR(C=STATIC["svm_C"], max_iter=10000)
                    if n in ("linearreg","linear"):
                        return LinearRegression()
                    if n == "sgd":
                        return SGDRegressor(max_iter=STATIC["sgd_max_iter"], tol=1e-3, random_state=args.random_state)
                    if n == "mlp":
                        return MLPRegressor(hidden_layer_sizes=STATIC["mlp_hidden_layer_sizes"], max_iter=500, random_state=args.random_state)
                    if n == "xgboost" and XGBRegressor is not None:
                        return XGBRegressor(n_estimators=STATIC["meta_xgb_n_estimators"], random_state=args.random_state)
                    if n == "lightgbm" and LGBMRegressor is not None:
                        return LGBMRegressor(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=args.random_state)
                    if n == "catboost" and CatBoostRegressor is not None:
                        return CatBoostRegressor(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=args.random_state)
                    raise ValueError(f"Unsupported regressor name: {name}")

            def make_meta_estimator(name):
                n = (name or "").strip().lower()
                if n in ("", "lightgbm", "lgbm", "lgb"):
                    if task == "classification" and LGBMClassifier is not None:
                        return LGBMClassifier(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=args.random_state)
                    if task == "regression" and LGBMRegressor is not None:
                        return LGBMRegressor(n_estimators=STATIC["meta_lgbm_n_estimators"], random_state=args.random_state)
                if n in ("xgboost","xgb"):
                    if task == "classification" and XGBClassifier is not None:
                        return XGBClassifier(n_estimators=STATIC["meta_xgb_n_estimators"], use_label_encoder=False, eval_metric='logloss', random_state=args.random_state)
                    if task == "regression" and XGBRegressor is not None:
                        return XGBRegressor(n_estimators=STATIC["meta_xgb_n_estimators"], random_state=args.random_state)
                if n in ("catboost","cat"):
                    if task == "classification" and CatBoostClassifier is not None:
                        return CatBoostClassifier(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=args.random_state)
                    if task == "regression" and CatBoostRegressor is not None:
                        return CatBoostRegressor(iterations=STATIC["meta_cat_n_estimators"], verbose=0, random_state=args.random_state)
                if task == "classification":
                    return LogisticRegression(max_iter=2000)
                else:
                    return LinearRegression()

            trained_object = None
            trained_estimators = []
            trained_base_info = []
            model_to_train = None

            if ensemble_type not in ("stacking","voting"):
                if selected_model_names:
                    model_to_train = selected_model_names[0]
                else:
                    model_to_train = "extratrees"
                print(f"[INFO] Training single model: {model_to_train}")
                model_inst = make_base_estimator(model_to_train)
                if task == "classification":
                    try:
                        sw = compute_sample_weight(class_weight='balanced', y=pd.Series(y).astype(int))
                    except Exception:
                        sw = None
                    if sw is not None:
                        try:
                            model_inst.fit(X, y, sample_weight=sw)
                        except TypeError:
                            model_inst.fit(X, y)
                    else:
                        model_inst.fit(X, y)
                else:
                    model_inst.fit(X, y)
                trained_object = model_inst
                trained_base_info = [model_to_train]
            else:
                if not selected_model_names:
                    raise ValueError("Ensemble requested but no base models selected.")
                estimators = []
                for i, nm in enumerate(selected_model_names):
                    try:
                        est = make_base_estimator(nm)
                        estimators.append((f"{nm}_{i}", est))
                    except Exception as e:
                        print(f"[WARN] Skipping model {nm}: {e}")
                if not estimators:
                    raise ValueError("No valid base estimators available for ensemble after filtering.")
                min_class_count = None
                if task == "classification":
                    try:
                        class_counts = pd.Series(y).value_counts()
                        min_class_count = int(class_counts.min())
                        print(f"[INFO] class_counts min = {min_class_count}, distribution:\n{class_counts.to_string()}")
                    except Exception as e:
                        print(f"[WARN] Could not compute class counts: {e}")
                        min_class_count = None
                filtered_estimators = []
                for name, est in estimators:
                    nm = name.split("_")[0]
                    if nm == "nusvc" and (min_class_count is not None and min_class_count < 2):
                        print(f"[WARN] Skipping {name} because min_class_count={min_class_count} makes NuSVC infeasible")
                        continue
                    if nm in ("svc","linearsvc") and (min_class_count is not None and min_class_count < 1):
                        print(f"[WARN] Skipping {name} because data too small for SVM")
                        continue
                    filtered_estimators.append((name, est))

                if not filtered_estimators:
                    raise ValueError("All base estimators were filtered out due to data constraints")
                safe_estimators = []
                for name, est in filtered_estimators:
                    nm = name.split("_")[0]
                    if nm in ("svc","nusvc","linearsvc","svr","linearsvr") and STATIC["TRIAL_FIT_SVM"]:
                        try:
                            print(f"[INFO] Trial-fitting fragile estimator {name} to check feasibility...")
                            est.fit(X, y)
                            safe_estimators.append((name, est))
                        except Exception as e:
                            print(f"[WARN] Trial-fit failed for {name}: {e}. Skipping this estimator.")
                    else:
                        safe_estimators.append((name, est))

                if not safe_estimators:
                    raise ValueError("No base estimators passed trial-fit. Reduce model list or check data.")
                if len(safe_estimators) == 1:
                    name0, est0 = safe_estimators[0]
                    print(f"[INFO] Only one valid estimator ({name0}) -> training single model")
                    est0.fit(X, y)
                    trained_estimators = [(name0, est0)]
                    trained_object = est0
                else:
                    if ensemble_type == "voting":
                        try:
                            if task == "classification":
                                try:
                                    vote = VotingClassifier(estimators=safe_estimators, voting="soft", n_jobs=-1)
                                    vote.fit(X, y)
                                except Exception:
                                    vote = VotingClassifier(estimators=safe_estimators, voting="hard", n_jobs=-1)
                                    vote.fit(X, y)
                                trained_object = vote
                                trained_base_info = [nm for nm,_ in safe_estimators]
                            else:
                                vote = VotingRegressor(estimators=safe_estimators, n_jobs=-1)
                                vote.fit(X, y)
                                trained_object = vote
                                trained_base_info = [nm for nm,_ in safe_estimators]
                        except Exception as e:
                            print(f"[WARN] Voting ensemble failed: {e}. Training base estimators individually as fallback.")
                            trained_estimators = []
                            for nm, est in safe_estimators:
                                try:
                                    est.fit(X, y)
                                    trained_estimators.append((nm, est))
                                except Exception as e2:
                                    print(f"[WARN] Base estimator {nm} failed in fallback fit: {e2}")
                            trained_object = None
                    else:
                        final_est = make_meta_estimator(meta_name)
                        try:
                            if task == "classification":
                                stack = StackingClassifier(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                            else:
                                stack = StackingRegressor(estimators=safe_estimators, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                            stack.fit(X, y)
                            trained_object = stack
                            trained_base_info = [nm for nm,_ in safe_estimators]
                        except Exception as e:
                            print(f"[WARN] Initial stacking.fit failed: {e}. Attempting to identify and remove failing base estimators and retry once.")
                            surviving = []
                            for nm, est in safe_estimators:
                                try:
                                    print(f"[INFO] Trial-fitting estimator {nm} separately...")
                                    est.fit(X, y)
                                    surviving.append((nm, est))
                                except Exception as e2:
                                    print(f"[WARN] Estimator {nm} failed trial-fit: {e2}. Removing it from ensemble.")
                            if len(surviving) >= 2:
                                try:
                                    if task == "classification":
                                        stack2 = StackingClassifier(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                    else:
                                        stack2 = StackingRegressor(estimators=surviving, final_estimator=final_est, n_jobs=-1, passthrough=False, cv=stacking_cv)
                                    stack2.fit(X, y)
                                    trained_object = stack2
                                    trained_base_info = [nm for nm,_ in surviving]
                                except Exception as e3:
                                    print(f"[WARN] Retry stacking.fit also failed: {e3}. Falling back to training base estimators individually.")
                                    trained_estimators = []
                                    for nm, est in surviving:
                                        try:
                                            est.fit(X, y)
                                            trained_estimators.append((nm, est))
                                        except Exception as e4:
                                            print(f"[WARN] Base estimator {nm} failed in fallback fit: {e4}")
                                    trained_object = None
                            else:
                                print("[WARN] Not enough surviving base estimators for stacking after filtering. Training surviving estimators individually or single model fallback.")
                                trained_estimators = []
                                for nm, est in surviving:
                                    try:
                                        est.fit(X, y)
                                        trained_estimators.append((nm, est))
                                    except Exception as e5:
                                        print(f"[WARN] Base estimator {nm} failed in fallback fit: {e5}")
                                if len(trained_estimators) == 1:
                                    trained_object = trained_estimators[0][1]
                                else:
                                    trained_object = None

            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1]), "ensemble_type": ensemble_type, "models_used": selected_model_names or [model_to_train], "stacking_cv": stacking_cv}
            try:
                predictor = None
                if trained_object is not None:
                    predictor = trained_object
                elif trained_estimators:
                    predictor = None
                else:
                    predictor = None

                if predictor is not None:
                    y_pred = predictor.predict(X)
                else:
                    if trained_estimators:
                        if task == "regression":
                            preds = []
                            for nm, est in trained_estimators:
                                preds.append(est.predict(X))
                            y_pred = np.mean(np.vstack(preds), axis=0)
                        else:
                            preds = []
                            for nm, est in trained_estimators:
                                preds.append(np.array(est.predict(X)))
                            preds = np.vstack(preds)
                            y_pred = stats.mode(preds, axis=0).mode[0].ravel()
                    else:
                        raise ValueError("No predictor available to compute metrics.")

                if task == "classification":
                    metrics.update({
                        "accuracy_train": float(accuracy_score(y, y_pred)),
                        "precision_train": float(precision_score(y, y_pred, average='weighted', zero_division=0)),
                        "recall_train": float(recall_score(y, y_pred, average='weighted', zero_division=0)),
                        "f1_train": float(f1_score(y, y_pred, average='weighted', zero_division=0))
                    })
                else:
                    metrics.update({
                        "r2_train": float(r2_score(y, y_pred)),
                        "rmse_train": float(np.sqrt(mean_squared_error(y, y_pred))),
                        "mae_train": float(mean_absolute_error(y, y_pred))
                    })
            except Exception as e:
                print(f"[WARN] Could not compute metrics: {e}")
            ensure_dir_for(args.model_pickle)
            to_save = None
            if trained_object is not None:
                to_save = trained_object
            elif trained_estimators:
                to_save = {nm: est for nm, est in trained_estimators}
            else:
                raise ValueError("No trained model or estimators to save.")

            joblib.dump(to_save, args.model_pickle)

            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as fh:
                json.dump(metrics, fh, indent=2, ensure_ascii=False)

            model_config = {
                "ensemble_type": ensemble_type,
                "models_selected": selected_model_names,
                "meta_model_name": meta_name or ("lightgbm" if (LGBMClassifier or LGBMRegressor) else ("xgboost" if (XGBClassifier or XGBRegressor) else "fallback")),
                "static_defaults": STATIC,
                "stacking_cv": stacking_cv,
                "random_state": int(args.random_state)
            }
            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as fh:
                json.dump(model_config, fh, indent=2, ensure_ascii=False)

            print("="*80)
            print("SUCCESS: Training finished")
            print(json.dumps(metrics, indent=2))
            print("="*80)

        except Exception as e:
            print("ERROR DURING TRAINING:", e, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --ensemble_type
      - {inputValue: ensemble_type}
      - --n_ensemble_models
      - {inputValue: n_ensemble_models}
      - --model_names
      - {inputValue: model_names}
      - --meta_model_name
      - {inputValue: meta_model_name}
      - --stacking_cv
      - {inputValue: stacking_cv}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
