name: Evaluate Model v15.5
inputs:
  - {name: test_data, type: Dataset, description: "Path to test data file (raw, not preprocessed)"}
  - {name: target_column, type: String, description: "Target column name"}
  - {name: cleaning_metadata, type: Data, description: "JSON metadata from Data Cleaning brick"}
  - {name: preprocessor, type: Data, description: "Path to saved Preprocessor"}
  - {name: feature_selector, type: Data, description: "Path to saved FeatureSelector", optional: true}
  - {name: pca, type: Data, description: "Path to saved PCA transformer", optional: true}
  - {name: model_pickle, type: Model, description: "Path to saved model"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: preprocess_metadata, type: Data, description: "JSON emitted at train time", optional: true}
outputs:
  - {name: metrics_json, type: String, description: "Evaluation metrics JSON"}
  - {name: predictions, type: String, description: "Parquet with predictions"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -c
      - |
        # Install feature-engine (other packages should already be in the image)
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \
          'feature-engine'
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, io, gzip, zipfile, traceback, re
        import pandas as pd, numpy as np, joblib, cloudpickle, logging
        from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                                      r2_score, mean_squared_error, confusion_matrix)
        from datetime import datetime

        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        def detect_and_deduplicate_dates(df, exclude_cols=None):
            df = df.copy()
            report = {'date_columns': [], 'skipped': [], 'duplicate_dates_dropped': []}
            exclude_cols = set(exclude_cols or [])
            cand = [c for c in df.columns if c not in exclude_cols and 
                    (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
            
            parsed_dates = {}
            for col in cand:
                ser = df[col].astype(object)
                sample = ser.dropna().astype(str).head(500)
                if sample.empty: 
                    continue
                
                try:
                    parsed = pd.to_datetime(sample, errors='coerce', infer_datetime_format=True)
                    frac = parsed.notna().mean()
                except Exception:
                    continue
                
                if frac >= 0.6:
                    try:
                        full = pd.to_datetime(ser, errors='coerce', infer_datetime_format=True)
                        
                        if not pd.api.types.is_datetime64_any_dtype(full):
                            continue
                        if full.notna().sum() == 0:
                            continue
                        
                        parsed_dates[col] = {
                            'datetime': full,
                            'parsable_fraction': frac,
                            'granularity': _get_date_granularity(full, col)
                        }
                    except Exception:
                        continue
            
            if not parsed_dates:
                return df, report
            
            date_groups = []
            processed = set()
            
            for col1 in parsed_dates.keys():
                if col1 in processed:
                    continue
                group = [col1]
                dt1 = parsed_dates[col1]['datetime']
                
                for col2 in parsed_dates.keys():
                    if col2 == col1 or col2 in processed:
                        continue
                    dt2 = parsed_dates[col2]['datetime']
                    if _are_dates_duplicate(dt1, dt2):
                        group.append(col2)
                        processed.add(col2)
                
                processed.add(col1)
                date_groups.append(group)
            
            cols_to_drop = []
            for group in date_groups:
                if len(group) > 1:
                    best_col = max(group, key=lambda c: parsed_dates[c]['granularity'])
                    duplicates = [c for c in group if c != best_col]
                    cols_to_drop.extend(duplicates)
                    report['duplicate_dates_dropped'].append({
                        'kept': best_col,
                        'dropped': duplicates
                    })
                    logger.info(f"Keeping '{best_col}', dropping {duplicates}")
            
            if cols_to_drop:
                df = df.drop(columns=cols_to_drop)
                for col in cols_to_drop:
                    del parsed_dates[col]
            
            for col, data in parsed_dates.items():
                full = data['datetime']
                try:
                    df[col + "_orig"] = df[col]
                    df[col] = full
                    df[col + "_year"] = df[col].dt.year.astype('Int64')
                    df[col + "_month"] = df[col].dt.month.astype('Int64')
                    df[col + "_day"] = df[col].dt.day.astype('Int64')
                    df[col + "_dayofweek"] = df[col].dt.dayofweek.astype('Int64')
                    df[col + "_quarter"] = df[col].dt.quarter.astype('Int64')
                    df[col + "_month_sin"] = np.sin(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_month_cos"] = np.cos(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_day_sin"] = np.sin(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_day_cos"] = np.cos(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5, 6]).astype('Int64')
                    df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                    report['date_columns'].append({'col': col})
                    logger.info(f"Extracted features from '{col}'")
                except Exception as e:
                    logger.warning(f"Failed to extract from '{col}': {e}")
            
            return df, report

        def _get_date_granularity(dt_series, col_name):
            valid_dates = dt_series.dropna()
            if len(valid_dates) > 0:
                has_time = (valid_dates.dt.hour != 0).any() or (valid_dates.dt.minute != 0).any()
                if has_time:
                    return 3
                has_day_variation = valid_dates.dt.day.nunique() > 1
                return 2 if has_day_variation else 1
            col_lower = col_name.lower()
            if 'timestamp' in col_lower:
                return 3
            elif 'date' in col_lower:
                return 2
            return 1

        def _are_dates_duplicate(dt1, dt2):
            valid_mask = dt1.notna() & dt2.notna()
            if valid_mask.sum() == 0:
                return False
            dt1_valid = dt1[valid_mask]
            dt2_valid = dt2[valid_mask]
            match_rate = (
                (dt1_valid.dt.year == dt2_valid.dt.year) &
                (dt1_valid.dt.month == dt2_valid.dt.month) &
                (dt1_valid.dt.day == dt2_valid.dt.day)
            ).mean()
            return match_rate >= 0.95

        def convert_object_columns_advanced(df, detect_threshold=0.6):
            df = df.copy()
            report = {'converted': [], 'skipped': []}
            
            CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
            
            def parse_alphanumeric_to_numeric(s):
                if pd.isna(s):
                    return np.nan
                orig = str(s).strip()
                if orig == '' or orig.lower() in {'nan', 'none', 'null', 'na'}:
                    return np.nan
                tmp = re.sub(CURRENCY_SYMBOLS_REGEX, '', orig)
                if tmp.strip().endswith('%'):
                    try:
                        return float(tmp.strip().rstrip('%').replace(',', '').replace(' ', '')) / 100.0
                    except Exception:
                        pass
                tmp = tmp.replace(',', '').replace(' ', '')
                try:
                    return float(tmp)
                except Exception:
                    return np.nan
            
            obj_cols = [c for c in df.columns if df[c].dtype == 'object' or str(df[c].dtype).startswith('string')]
            
            for col in obj_cols:
                ser = df[col].astype(object)
                total_non_null = ser.notna().sum()
                if total_non_null == 0:
                    continue
                
                parsed = ser.map(parse_alphanumeric_to_numeric)
                frac = parsed.notna().sum() / float(total_non_null)
                
                if frac >= detect_threshold:
                    df[col + "_orig"] = df[col]
                    df[col] = parsed
                    report['converted'].append({'col': col, 'parsable_fraction': frac})
            
            return df, report

        def apply_cleaning_to_test(test_df, target_col, cleaning_meta):
            logger.info("=" * 80)
            logger.info("APPLYING DATA CLEANING TO TEST DATA")
            logger.info("=" * 80)
            
            df = test_df.copy()
            
            # Handle target column(s)
            if isinstance(target_col, str):
                # Single target - extract as Series to avoid 2D array issues
                y = df[target_col].copy()
                X = df.drop(columns=[target_col])
            else:
                # Multiple targets - keep as DataFrame
                y = df[target_col].copy()
                X = df.drop(columns=target_col)
            
            logger.info(f"Original X shape: {X.shape}")
            
            expected_cols = set(cleaning_meta.get('column_names', []))
            logger.info(f"Expected {len(expected_cols)} columns from training")
            
            logger.info("Step 1: Detecting and deduplicating date columns...")
            exclude_cols = [target_col] if isinstance(target_col, str) else target_col
            X, date_report = detect_and_deduplicate_dates(X, exclude_cols=exclude_cols)
            
            if date_report['duplicate_dates_dropped']:
                for dup in date_report['duplicate_dates_dropped']:
                    logger.info(f"  Kept: {dup['kept']}, Dropped: {dup['dropped']}")
            
            logger.info("Step 2: Converting object columns to numeric...")
            X, conv_report = convert_object_columns_advanced(X, detect_threshold=0.6)
            logger.info(f"  Converted {len(conv_report['converted'])} columns")
            
            orig_cols = [c for c in X.columns if c.endswith('_orig')]
            if orig_cols:
                X = X.drop(columns=orig_cols)
                logger.info(f"Step 3: Dropped {len(orig_cols)} _orig columns")
            
            logger.info("Step 4: Aligning with training columns...")
            current_cols = set(X.columns)
            missing_cols = expected_cols - current_cols
            extra_cols = current_cols - expected_cols
            
            if missing_cols:
                logger.warning(f"  Missing {len(missing_cols)} columns, adding with zeros:")
                logger.warning(f"    {list(missing_cols)[:5]}...")
                for col in missing_cols:
                    X[col] = 0.0
            
            if extra_cols:
                logger.warning(f"  Extra {len(extra_cols)} columns, dropping:")
                logger.warning(f"    {list(extra_cols)[:5]}...")
                X = X.drop(columns=list(extra_cols))
            
            X = X[list(expected_cols)]
            
            logger.info(f"Final cleaned X shape: {X.shape}")
            logger.info("=" * 80)
            
            return X, y

        # ============================================
        # HELPER FUNCTIONS
        # ============================================
        
        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                path = max(files, key=lambda p: os.path.getsize(p))
            ext=os.path.splitext(path)[1].lower()
            if ext in (".parquet", ".pq"):
                return pd.read_parquet(path)
            if ext == ".csv":
                return pd.read_csv(path)
            try:
                return pd.read_parquet(path)
            except Exception:
                return pd.read_csv(path)

        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def load_pickle_any(path):
            try:
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)
            except Exception:
                pass
            try:
                with open(path, "rb") as f:
                    return cloudpickle.load(f)
            except Exception:
                pass
            return joblib.load(path)

        def unwrap_model(obj, max_depth=10):
            if max_depth <= 0:
                return obj
            
            # If it has predict method, it's a model
            if hasattr(obj, 'predict'):
                return obj
            
            # If it's a dict, try to unwrap
            if isinstance(obj, dict):
                # Common keys that might contain the model
                for key in ['model', 'estimator', 'best_estimator_', 'final_estimator']:
                    if key in obj:
                        return unwrap_model(obj[key], max_depth - 1)
                
                # If dict has only one item, unwrap that
                if len(obj) == 1:
                    return unwrap_model(list(obj.values())[0], max_depth - 1)
                
                # Try each value until we find one with predict
                for val in obj.values():
                    if hasattr(val, 'predict'):
                        return val
                    unwrapped = unwrap_model(val, max_depth - 1)
                    if hasattr(unwrapped, 'predict'):
                        return unwrapped
            
            return obj

        # ============================================
        # MAIN FUNCTION
        # ============================================
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--test_data", type=str, required=True)
            parser.add_argument("--target_column", type=str, required=True)
            parser.add_argument("--cleaning_metadata", type=str, required=True)
            parser.add_argument("--preprocessor", type=str, required=True)
            parser.add_argument("--feature_selector", type=str, default="")
            parser.add_argument("--pca", type=str, default="")
            parser.add_argument("--model_pickle", type=str, required=True)
            parser.add_argument("--model_type", type=str, default="classification")
            parser.add_argument("--preprocess_metadata", type=str, default="")
            parser.add_argument("--metrics_json", type=str, required=True)
            parser.add_argument("--predictions", type=str, required=True)
            args = parser.parse_args()

            try:
                logger.info("=" * 80)
                logger.info("EVALUATION STARTED - v15.4.1 Fixed Classification")
                logger.info("=" * 80)

                task = args.model_type.strip().lower()
                target_col = args.target_column.strip()
                
                logger.info("Loading cleaning metadata...")
                with open(args.cleaning_metadata, 'r') as f:
                    cleaning_meta = json.load(f)
                logger.info(f"  Expected columns: {len(cleaning_meta.get('column_names', []))}")
                
                logger.info("Loading raw test data...")
                test_df_raw = read_with_pandas(args.test_data)
                logger.info(f"  Raw shape: {test_df_raw.shape}")
                
                X_cleaned, y = apply_cleaning_to_test(test_df_raw, target_col, cleaning_meta)
                
                logger.info("Loading preprocessor...")
                pre = load_pickle_any(args.preprocessor)
                
                fs = None
                if args.feature_selector and os.path.exists(args.feature_selector):
                    logger.info("Loading feature selector...")
                    fs = load_pickle_any(args.feature_selector)
                
                pca_obj = None
                if args.pca and os.path.exists(args.pca):
                    size = os.path.getsize(args.pca)
                    if size > 0:
                        try:
                            pca_obj = load_pickle_any(args.pca)
                            if pca_obj is not None:
                                logger.info(f"PCA loaded: {type(pca_obj)}")
                        except Exception as e:
                            logger.warning(f"PCA load failed: {e}")
                
                logger.info("Loading model...")
                model = load_pickle_any(args.model_pickle)
                logger.info(f"  Model type: {type(model)}")
                
                logger.info("=" * 80)
                logger.info("PREPROCESSING")
                logger.info("=" * 80)
                
                test_full = X_cleaned.copy()
                
                # Add target back - handle Series vs DataFrame
                if isinstance(y, pd.Series):
                    test_full[target_col] = y
                else:
                    for col in y.columns:
                        test_full[col] = y[col]
                
                logger.info("Applying preprocessor...")
                Xp_full = pre.transform(test_full, training_mode=False) if hasattr(pre.transform, '__code__') and 'training_mode' in pre.transform.__code__.co_varnames else pre.transform(test_full)
                
                if isinstance(Xp_full, pd.DataFrame):
                    Xp = Xp_full.drop(columns=[target_col], errors='ignore')
                else:
                    Xp = pd.DataFrame(Xp_full)
                
                logger.info(f"  After preprocessing: {Xp.shape}")
                
                if fs is not None:
                    logger.info("Applying feature selector...")
                    if hasattr(fs, "selected_features"):
                        Xs = Xp.reindex(columns=list(fs.selected_features), fill_value=0.0)
                    else:
                        Xs = fs.transform(Xp)
                    logger.info(f"  After selection: {Xs.shape}")
                else:
                    Xs = Xp
                
                if pca_obj is not None:
                    logger.info("Applying PCA...")
                    Xs_arr = Xs.values if isinstance(Xs, pd.DataFrame) else np.asarray(Xs)
                    Xs_pca = pca_obj.transform(Xs_arr)
                    Xs = pd.DataFrame(Xs_pca, columns=[f"PC{i+1}" for i in range(Xs_pca.shape[1])])
                    logger.info(f"  After PCA: {Xs.shape}")
                
                logger.info("=" * 80)
                logger.info("MAKING PREDICTIONS")
                logger.info("=" * 80)
                
                is_model_dict = isinstance(model, dict) and not hasattr(model, 'predict')
                
                if is_model_dict:
                    target_names = list(model.keys())
                    logger.info(f"  Model dict with {len(target_names)} targets: {target_names}")
                    
                    all_preds = {}
                    all_ytrue = {}
                    
                    for tgt in target_names:
                        logger.info(f"  Predicting for target: {tgt}")
                        
                        # Unwrap nested structure
                        model_obj = unwrap_model(model[tgt])
                        logger.info(f"    Unwrapped model type: {type(model_obj).__name__}")
                        
                        if not hasattr(model_obj, 'predict'):
                            raise ValueError(f"Could not find predict method for target {tgt}. Type: {type(model_obj)}")
                        
                        yhat = np.asarray(model_obj.predict(Xs)).ravel()
                        all_preds[tgt] = yhat
                        
                        if isinstance(y, pd.DataFrame) and tgt in y.columns:
                            all_ytrue[tgt] = y[tgt].values
                        elif isinstance(y, pd.Series):
                            all_ytrue[tgt] = y.values
                        else:
                            # Fallback
                            y_vals = y.values if hasattr(y, 'values') else np.array(y)
                            all_ytrue[tgt] = y_vals.ravel() if len(y_vals.shape) > 1 else y_vals
                    
                    logger.info("Computing metrics for each target...")
                    metrics_list = []
                    
                    for tgt in target_names:
                        y_pred = all_preds[tgt]
                        y_true_vals = all_ytrue[tgt]
                        
                        unwrapped = unwrap_model(model[tgt])
                        model_name = f"{type(unwrapped).__name__}:{tgt}"
                        metrics = {"model_name": model_name, "model_type": task}
                        
                        if task == "regression":
                            y_true_num = pd.to_numeric(y_true_vals, errors='coerce')
                            valid_mask = ~np.isnan(y_true_num)
                            if valid_mask.sum() > 0:
                                yt = y_true_num[valid_mask]
                                yp = y_pred[valid_mask]
                                mse_val = mean_squared_error(yt, yp)
                                r2_val = r2_score(yt, yp)
                                metrics.update({
                                    "r2_score": round(float(r2_val), 6),
                                    "mse": round(float(mse_val), 6),
                                    "rmse_score": round(float(np.sqrt(mse_val)), 6)
                                })
                                logger.info(f"  {tgt}: R²={r2_val:.6f}, RMSE={np.sqrt(mse_val):.6f}")
                        else:
                            acc = accuracy_score(y_true_vals, y_pred)
                            metrics.update({
                                "accuracy": round(float(acc), 6),
                                "precision_score": round(float(precision_score(y_true_vals, y_pred, average="weighted", zero_division=0)), 6),
                                "recall": round(float(recall_score(y_true_vals, y_pred, average="weighted", zero_division=0)), 6),
                                "f1_score": round(float(f1_score(y_true_vals, y_pred, average="weighted", zero_division=0)), 6)
                            })
                            logger.info(f"  {tgt}: Accuracy={acc:.6f}")
                        
                        metrics_list.append(metrics)
                    
                    ensure_dir_for(args.metrics_json)
                    with open(args.metrics_json, "w") as f:
                        json.dump(metrics_list, f, indent=2)
                    
                    pred_cols = {f"y_pred_{tgt}": all_preds[tgt] for tgt in target_names}
                    true_cols = {f"y_true_{tgt}": all_ytrue[tgt] for tgt in target_names}
                    out_df = pd.DataFrame({**true_cols, **pred_cols})
                    
                else:
                    # Single model or dict with predict method
                    model_obj = unwrap_model(model)
                    logger.info(f"  Single model: {type(model_obj).__name__}")
                    
                    y_pred = np.asarray(model_obj.predict(Xs)).ravel()
                    logger.info(f"  Predictions: {len(y_pred)}")
                    
                    logger.info("Computing metrics...")
                    metrics = {"model_name": str(type(model_obj).__name__), "model_type": task}
                    
                    if task == "regression":
                        y_vals = y.values if isinstance(y, (pd.Series, pd.DataFrame)) else np.array(y)
                        if len(y_vals.shape) > 1:
                            y_vals = y_vals.ravel()
                        y_true_num = pd.to_numeric(y_vals, errors='coerce')
                        valid_mask = ~np.isnan(y_true_num)
                        if valid_mask.sum() > 0:
                            yt = y_true_num[valid_mask]
                            yp = y_pred[valid_mask]
                            mse_val = mean_squared_error(yt, yp)
                            metrics.update({
                                "r2_score": round(float(r2_score(yt, yp)), 6),
                                "mse": round(float(mse_val), 6),
                                "rmse_score": round(float(np.sqrt(mse_val)), 6)
                            })
                    else:
                        y_vals = y.values if isinstance(y, (pd.Series, pd.DataFrame)) else np.array(y)
                        y_true_arr = y_vals.ravel() if len(y_vals.shape) > 1 else y_vals
                        metrics.update({
                            "accuracy": round(float(accuracy_score(y_true_arr, y_pred)), 6),
                            "precision_score": round(float(precision_score(y_true_arr, y_pred, average="weighted", zero_division=0)), 6),
                            "recall": round(float(recall_score(y_true_arr, y_pred, average="weighted", zero_division=0)), 6),
                            "f1_score": round(float(f1_score(y_true_arr, y_pred, average="weighted", zero_division=0)), 6)
                        })
                    
                    logger.info(f"  Metrics: {json.dumps(metrics, indent=2)}")
                    
                    ensure_dir_for(args.metrics_json)
                    with open(args.metrics_json, "w") as f:
                        json.dump([metrics], f, indent=2)
                    
                    y_vals = y.values if isinstance(y, (pd.Series, pd.DataFrame)) else np.array(y)
                    y_true_vals = y_vals.ravel() if len(y_vals.shape) > 1 else y_vals
                    out_df = pd.DataFrame({
                        "y_true": y_true_vals,
                        "y_pred": y_pred
                    })
                
                ensure_dir_for(args.predictions)
                out_df.to_parquet(args.predictions, index=False)
                
                logger.info("=" * 80)
                logger.info("SUCCESS")
                logger.info("=" * 80)
                logger.info(f"Samples evaluated: {len(Xs)}")
                logger.info(f"Metrics saved: {args.metrics_json}")
                logger.info(f"Predictions saved: {args.predictions}")
                logger.info("=" * 80)

            except Exception as e:
                logger.error(f"EVALUATION FAILED: {e}")
                traceback.print_exc()
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --test_data
      - {inputPath: test_data}
      - --target_column
      - {inputValue: target_column}
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca
      - {inputPath: pca}
      - --model_pickle
      - {inputPath: model_pickle}
      - --model_type
      - {inputValue: model_type}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --metrics_json
      - {outputPath: metrics_json}
      - --predictions
      - {outputPath: predictions}
