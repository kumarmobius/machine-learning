name: Evaluate Model v1.8
inputs:
  - {name: X_test, type: Data, description: "Path to test feature file or directory (raw, not preprocessed)"}
  - {name: y_test, type: Data, description: "Path to test target file (single-column file or multi-column with target_column)"}
  - {name: target_column, type: String, description: "Target column in y_test if multi-column", optional: true, default: ""}
  - {name: preprocessor, type: Data, description: "Path to saved Preprocessor (cloudpickle or joblib). .gz supported"}
  - {name: feature_selector, type: Data, description: "Path to saved FeatureSelector (cloudpickle or joblib). Optional", optional: true}
  - {name: model_pickle, type: Data, description: "Path to saved model (joblib/pickle)"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: preprocess_metadata, type: Data, description: "JSON emitted at train time containing label_mapping for classification", optional: true}
outputs:
  - {name: metrics_json, type: Data, description: "Evaluation metrics JSON"}
  - {name: predictions, type: Data, description: "Parquet with y_true and y_pred (and proba if available)"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, io, gzip, zipfile, traceback, subprocess
        import pandas as pd, numpy as np, joblib
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
        import cloudpickle, gzip
        
        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            if txt[0] in ("{","["): return True
            if "{" in txt or "[" in txt: return True
            return False

        def read_with_pandas(path):
            import io, zipfile
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path = max(files, key=lambda p: os.path.getsize(p))
                print("[INFO] Directory input. Picked file: " + path)
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported format: " + path)

        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def load_pickle_any(path):
            if not os.path.exists(path):
                raise FileNotFoundError(f"File not found: {path}")
            
            # Try gzipped cloudpickle first
            try:
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)
            except Exception:
                pass
            
            # Try regular cloudpickle
            try:
                with open(path, "rb") as f:
                    return cloudpickle.load(f)
            except Exception:
                pass
            
            # Try joblib as fallback
            try:
                return joblib.load(path)
            except Exception as e:
                raise ValueError(f"Could not load pickle from {path}: {e}")

        parser = argparse.ArgumentParser()
        parser.add_argument("--X_test", type=str, required=True)
        parser.add_argument("--y_test", type=str, required=True)
        parser.add_argument("--target_column", type=str, default="")
        parser.add_argument("--preprocessor", type=str, required=True)
        parser.add_argument("--feature_selector", type=str, required=False, default="")
        parser.add_argument("--model_pickle", type=str, required=True)
        parser.add_argument("--model_type", type=str, default="classification")
        parser.add_argument("--preprocess_metadata", type=str, required=False, default="")
        parser.add_argument("--metrics_json", type=str, required=True)
        parser.add_argument("--predictions", type=str, required=True)
        args = parser.parse_args()

        try:
            print("="*80)
            print("EVALUATION STARTED")
            print("="*80)
            
            X_path = args.X_test
            y_path = args.y_test
            preprocessor_path = args.preprocessor
            feature_selector_path = args.feature_selector or ""
            model_path = args.model_pickle
            metrics_path = args.metrics_json
            preds_path = args.predictions
            task = str(args.model_type).strip().lower()
            target_col = str(args.target_column or "").strip()
            meta_path = (args.preprocess_metadata or "").strip()

            print(f"[INFO] Task: {task}")
            print(f"[INFO] X_test path: {X_path}")
            print(f"[INFO] y_test path: {y_path}")
            print(f"[INFO] Target column: {target_col if target_col else '(auto-detect)'}")
            
            # Load artifacts
            print("[INFO] Loading preprocessor...")
            pre = load_pickle_any(preprocessor_path)
            print("[INFO] Preprocessor loaded successfully")
            
            print("[INFO] Loading feature selector...")
            if feature_selector_path and os.path.exists(feature_selector_path):
                fs = load_pickle_any(feature_selector_path)
                print("[INFO] Feature selector loaded successfully")
            else:
                fs = None
                print("[INFO] No feature selector provided")
            
            print("[INFO] Loading model...")
            try:
                model = joblib.load(model_path)
            except Exception:
                model = load_pickle_any(model_path)
            print("[INFO] Model loaded successfully")

            # Load test data
            print("[INFO] Loading test data...")
            X_raw = read_with_pandas(X_path)
            print(f"[INFO] X_test shape: {X_raw.shape}")
            print(f"[INFO] X_test columns: {list(X_raw.columns)}")
            
            y_df = read_with_pandas(y_path)
            
            # Extract target column
            if isinstance(y_df, pd.DataFrame):
                if target_col:
                    if target_col not in y_df.columns:
                        raise ValueError(f"--target_column '{target_col}' not in y_test. Available columns: {list(y_df.columns)}")
                    y_raw = y_df[target_col].copy()
                elif y_df.shape[1] == 1:
                    y_raw = y_df.iloc[:, 0].copy()
                else:
                    raise ValueError(f"y_test has {y_df.shape[1]} columns. Please provide --target_column. Available: {list(y_df.columns)}")
            else:
                y_raw = pd.Series(y_df).copy()

            if len(y_raw) == 0:
                raise ValueError("y_test has 0 rows. Check the input file.")
            
            print(f"[INFO] y_test shape: {y_raw.shape}")
            print(f"[INFO] y_test unique values: {y_raw.nunique()}")
            print(f"[INFO] y_test sample values: {y_raw.head(10).tolist()}")

            # Load metadata and label mapping
            print("[INFO] Loading preprocessing metadata...")
            label_mapping = None
            if meta_path and os.path.exists(meta_path):
                try:
                    with open(meta_path, "r", encoding="utf-8") as f:
                        meta = json.load(f)
                    label_mapping = meta.get("label_mapping", None)
                    if label_mapping:
                        print(f"[INFO] Label mapping loaded from metadata: {label_mapping}")
                    else:
                        print("[WARN] No label_mapping found in metadata")
                except Exception as e:
                    print(f"[WARN] Could not read metadata: {e}")
            else:
                print("[WARN] No metadata file provided")
            
            # Fallback: try to get label_mapping from preprocessor
            if label_mapping is None:
                try:
                    gm = getattr(pre, "global_metadata", {}) or {}
                    if "label_mapping" in gm:
                        label_mapping = gm["label_mapping"]
                        print(f"[INFO] Label mapping loaded from preprocessor: {label_mapping}")
                except Exception:
                    pass

            # ============================================================================
            # CRITICAL FIX: Reconstruct full dataframe for preprocessing
            # ============================================================================
            print("="*80)
            print("RECONSTRUCTING TEST DATA FOR PREPROCESSING")
            print("="*80)
            
            # Get target column name from preprocessor metadata
            try:
                target_col_name = pre.global_metadata.get('target_col', target_col)
                if not target_col_name:
                    target_col_name = target_col if target_col else 'target'
                print(f"[INFO] Target column name from preprocessor: {target_col_name}")
            except Exception as e:
                target_col_name = target_col if target_col else 'target'
                print(f"[WARN] Could not get target column from preprocessor: {e}")
                print(f"[WARN] Using: {target_col_name}")

            # Reconstruct full dataframe (preprocessor expects this structure during training)
            # The target values don't matter for feature engineering, we just need the column
            test_full = X_raw.copy()
            test_full[target_col_name] = y_raw.values

            print(f"[INFO] X_raw shape: {X_raw.shape}")
            print(f"[INFO] X_raw columns: {list(X_raw.columns)}")
            print(f"[INFO] Reconstructed test_full shape: {test_full.shape}")
            print(f"[INFO] Reconstructed columns: {list(test_full.columns)}")
            print("="*80)

            # Transform using reconstructed dataframe
            print("[INFO] Applying preprocessor to reconstructed test data...")
            print("[INFO] Note: Target column added temporarily for structural consistency")
            
            Xp_full = pre.transform(test_full, training_mode=False)
            
            print(f"[INFO] X after preprocessing: {Xp_full.shape}")
            print(f"[INFO] Columns after preprocessing: {list(Xp_full.columns)}")

            # Remove target column if it's still present after preprocessing
            if target_col_name in Xp_full.columns:
                print(f"[INFO] Removing target column '{target_col_name}' from processed features")
                Xp = Xp_full.drop(columns=[target_col_name])
            else:
                print(f"[INFO] Target column already removed during preprocessing")
                Xp = Xp_full

            print(f"[INFO] Final Xp shape: {Xp.shape}")
            print(f"[INFO] Final Xp columns: {list(Xp.columns)}")
            print("="*80)

            # Apply feature selector if available
            if fs is not None:
                print("[INFO] Applying feature selector...")
                if hasattr(fs, "selected_features"):
                    want_cols = list(fs.selected_features)
                    print(f"[INFO] Selecting {len(want_cols)} features")
                    print(f"[INFO] Selected features: {want_cols}")
                    
                    # Check if all required features are present
                    missing_cols = set(want_cols) - set(Xp.columns)
                    if missing_cols:
                        print(f"[WARN] Missing features after preprocessing: {missing_cols}")
                        print(f"[WARN] These will be filled with zeros")
                    
                    Xs = Xp.reindex(columns=want_cols, fill_value=0.0)
                else:
                    Xs = fs.transform(Xp)
                print(f"[INFO] X after feature selection: {Xs.shape}")
            else:
                Xs = Xp
                print("[INFO] No feature selection applied")

            # Align y_test labels to training encoding for classification
            y_original = y_raw.copy()  # Keep original for reference
            
            if task != "regression" and label_mapping is not None:
                print("="*80)
                print("APPLYING LABEL MAPPING TO TEST DATA")
                print("="*80)
                print(f"[INFO] Label mapping: {label_mapping}")
                
                # Convert y_test to match the format used in training
                y_str = y_raw.astype(str)
                
                # Map to encoded integers
                y_encoded = y_str.map(lambda val: label_mapping.get(val, None))
                
                # Check for unmapped labels
                unmapped_mask = y_encoded.isna()
                if unmapped_mask.any():
                    unmapped_count = int(unmapped_mask.sum())
                    unmapped_values = y_str[unmapped_mask].unique().tolist()
                    print(f"[WARN] Found {unmapped_count} unmapped labels in y_test: {unmapped_values}")
                    print(f"[WARN] Available mappings: {list(label_mapping.keys())}")
                    print(f"[WARN] These rows will be excluded from evaluation")
                    
                    # Filter out unmapped rows
                    keep_mask = ~unmapped_mask
                    Xs = Xs.iloc[keep_mask.values].reset_index(drop=True)
                    y = y_encoded[keep_mask].astype(int).reset_index(drop=True)
                    y_original = y_original[keep_mask].reset_index(drop=True)
                else:
                    y = y_encoded.astype(int).reset_index(drop=True)
                
                print(f"[INFO] y_test after encoding - unique values: {y.unique()}")
                print(f"[INFO] y_test after encoding - value counts:")
                print(y.value_counts().sort_index())
                print(f"[INFO] y_test after encoding - sample: {y.head(10).tolist()}")
                print("="*80)
                
            elif task == "regression":
                print("[INFO] Regression task - converting y_test to numeric...")
                y = pd.to_numeric(y_raw, errors='coerce')
                
                # Drop NaN values
                valid_mask = y.notna()
                if not valid_mask.all():
                    invalid_count = (~valid_mask).sum()
                    print(f"[WARN] Dropping {invalid_count} rows with non-numeric targets")
                    Xs = Xs.iloc[valid_mask.values].reset_index(drop=True)
                    y = y[valid_mask].reset_index(drop=True)
                    y_original = y_original[valid_mask].reset_index(drop=True)
                    
            else:
                print("[WARN] Classification task but no label_mapping found!")
                print("[WARN] Assuming y_test labels already match training encoding")
                y = y_raw
                
                # Try to convert to numeric if possible
                try:
                    y = pd.to_numeric(y, errors='coerce')
                    if y.isna().any():
                        print(f"[ERROR] Could not convert some labels to numeric!")
                        print(f"[ERROR] This will cause evaluation to fail")
                except Exception as e:
                    print(f"[WARN] Could not convert labels to numeric: {e}")

            # Final length check
            if len(y) != len(Xs):
                raise ValueError(f"Length mismatch after preprocessing: X={len(Xs)} vs y={len(y)}")
            
            print(f"[INFO] Final evaluation set size: {len(y)} samples")
            print(f"[INFO] Final X shape: {Xs.shape}")
            print(f"[INFO] Final y shape: {y.shape}")

            # Make predictions
            print("="*80)
            print("MAKING PREDICTIONS")
            print("="*80)
            
            y_pred = model.predict(Xs)
            print(f"[INFO] Predictions shape: {y_pred.shape}")
            print(f"[INFO] Predictions dtype: {y_pred.dtype}")
            print(f"[INFO] Predictions unique values: {np.unique(y_pred)}")
            print(f"[INFO] Predictions sample (first 10): {y_pred[:10]}")
            
            # Get probability predictions if available
            y_proba = None
            if hasattr(model, "predict_proba"):
                try:
                    y_proba = model.predict_proba(Xs)
                    print(f"[INFO] Probability predictions shape: {y_proba.shape}")
                    print(f"[INFO] Probability predictions sample (first 3):")
                    print(y_proba[:3])
                except Exception as e:
                    print(f"[WARN] Could not get probability predictions: {e}")

            # Compute metrics
            print("="*80)
            metrics = {}
            
            if task == "regression":
                print("REGRESSION METRICS")
                print("="*80)
                
                y_true_num = pd.to_numeric(y, errors="coerce")
                metrics["r2"] = float(r2_score(y_true_num, y_pred))
                metrics["rmse"] = float(np.sqrt(mean_squared_error(y_true_num, y_pred)))
                metrics["mae"] = float(mean_absolute_error(y_true_num, y_pred))
                
                print(f"RÂ² Score: {metrics['r2']:.6f}")
                print(f"RMSE: {metrics['rmse']:.6f}")
                print(f"MAE: {metrics['mae']:.6f}")
                
            else:  # Classification
                print("CLASSIFICATION METRICS")
                print("="*80)
                
                y_true = y
                
                # Ensure both are same type for comparison
                try:
                    if pd.api.types.is_numeric_dtype(y_true) and not pd.api.types.is_numeric_dtype(y_pred):
                        y_pred = pd.Series(y_pred).astype(y_true.dtype)
                    elif not pd.api.types.is_numeric_dtype(y_true) and pd.api.types.is_numeric_dtype(y_pred):
                        y_true = y_true.astype(str)
                        y_pred = pd.Series(y_pred).astype(str)
                except Exception as e:
                    print(f"[WARN] Type alignment issue: {e}")
                
                metrics["accuracy"] = float(accuracy_score(y_true, y_pred))
                metrics["precision_weighted"] = float(precision_score(y_true, y_pred, average="weighted", zero_division=0))
                metrics["recall_weighted"] = float(recall_score(y_true, y_pred, average="weighted", zero_division=0))
                metrics["f1_weighted"] = float(f1_score(y_true, y_pred, average="weighted", zero_division=0))
                
                print(f"Accuracy: {metrics['accuracy']:.6f}")
                print(f"Precision (weighted): {metrics['precision_weighted']:.6f}")
                print(f"Recall (weighted): {metrics['recall_weighted']:.6f}")
                print(f"F1 Score (weighted): {metrics['f1_weighted']:.6f}")

                # Confusion matrix
                try:
                    cm = confusion_matrix(y_true, y_pred)
                    print("="*80)
                    print("CONFUSION MATRIX")
                    print("="*80)
                    print("Rows = Actual, Columns = Predicted")
                    cm_df = pd.DataFrame(
                        cm, 
                        index=[f"Actual_{i}" for i in range(cm.shape[0])],
                        columns=[f"Pred_{i}" for i in range(cm.shape[1])]
                    )
                    print(cm_df)
                    print(f"Unique true labels: {np.unique(y_true)}")
                    print(f"Unique predicted labels: {np.unique(y_pred)}")
                    
                    # Add confusion matrix to metrics
                    metrics["confusion_matrix"] = cm.tolist()
                except Exception as e:
                    print(f"[WARN] Could not compute confusion matrix: {e}")

            print("="*80)

            # Save metrics
            print(f"[INFO] Saving metrics to: {metrics_path}")
            ensure_dir_for(metrics_path)
            with open(metrics_path, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            print(f"[INFO] Metrics saved successfully")

            # Save predictions
            print(f"[INFO] Saving predictions to: {preds_path}")
            ensure_dir_for(preds_path)
            
            out = pd.DataFrame({
                "y_true": y,
                "y_pred": y_pred,
                "y_true_original": y_original  # Keep original labels for reference
            })
            
            # Add probabilities if available
            if isinstance(y_proba, np.ndarray):
                try:
                    out["y_proba_max"] = y_proba.max(axis=1)
                    # Add individual class probabilities
                    for i in range(y_proba.shape[1]):
                        out[f"y_proba_class_{i}"] = y_proba[:, i]
                    print(f"[INFO] Added probability columns for {y_proba.shape[1]} classes")
                except Exception as e:
                    print(f"[WARN] Could not save probabilities: {e}")
            
            out.to_parquet(preds_path, index=False)
            print(f"[INFO] Predictions saved successfully")
            print(f"[INFO] Predictions dataframe shape: {out.shape}")
            print(f"[INFO] Predictions dataframe columns: {list(out.columns)}")

            print("="*80)
            print("SUCCESS: Evaluation completed successfully")
            print("="*80)
            print(f"Total samples evaluated: {len(y)}")
            print(f"Metrics saved to: {metrics_path}")
            print(f"Predictions saved to: {preds_path}")
            print("="*80)
            
        except Exception as e:
            print("="*80, file=sys.stderr)
            print("ERROR DURING EVALUATION", file=sys.stderr)
            print("="*80, file=sys.stderr)
            print(f"Error: {e}", file=sys.stderr)
            print("Full traceback:", file=sys.stderr)
            traceback.print_exc()
            print("="*80, file=sys.stderr)
            sys.exit(1)
    args:
      - --X_test
      - {inputPath: X_test}
      - --y_test
      - {inputPath: y_test}
      - --target_column
      - {inputValue: target_column}
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --model_pickle
      - {inputPath: model_pickle}
      - --model_type
      - {inputValue: model_type}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --metrics_json
      - {outputPath: metrics_json}
      - --predictions
      - {outputPath: predictions}
