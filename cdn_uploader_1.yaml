name: Upload to CDN v4
inputs:
  - {name: test_data, type: Dataset, description: "Test dataset file (CSV or Parquet)"}
  - {name: train_x, type: Dataset, """Sanitize filename by encoding special characters"""description: "Training features dataset (CSV or Parquet)"}
  - {name: train_y, type: Dataset, description: "Training labels dataset (CSV or Parquet)"}
  - {name: preprocesser_pickle, type: Data, description: "Preprocessor pickle file"}
  - {name: preprocesser_metadata, type: Data, description: "Preprocessor metadata JSON"}
  - {name: feature_selector, type: Data, description: "Feature selector pickle"}
  - {name: pca_model, type: Data, description: "PCA model pickle file"}
  - {name: pca_metadata, type: Data, description: "PCA metadata JSON"}
  - {name: bearer_token, type: string, description: "Bearer token for CDN authentication"}
  - {name: domain, type: String, description: "Upload service base domain"}
  - {name: get_cdn, type: String, description: "Public CDN base domain"}

outputs:
  - {name: train_x_cdn_url, type: String}
  - {name: train_y_cdn_url, type: String}
  - {name: preprocesser_pickle_cdn_url, type: String}
  - {name: preprocesser_metadata_cdn_url, type: String}
  - {name: feature_selector_cdn_url, type: String}
  - {name: pca_model_cdn_url, type: String}
  - {name: pca_metadata_cdn_url, type: String}
  - {name: test_data_cdn_url, type: String}
  - {name: schema_json, type: String}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -ec
      - |
        if ! command -v curl >/dev/null 2>&1; then
          apt-get update >/dev/null && apt-get install -y curl >/dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import time
        import tempfile
        import shutil
        import pandas as pd

        parser = argparse.ArgumentParser()

        parser.add_argument('--test_data', required=True)
        parser.add_argument('--train_x', required=True)
        parser.add_argument('--train_y', required=True)
        parser.add_argument('--preprocesser_pickle', required=True)
        parser.add_argument('--preprocesser_metadata', required=True)
        parser.add_argument('--feature_selector', required=True)
        parser.add_argument('--pca_model', required=True)
        parser.add_argument('--pca_metadata', required=True)

        parser.add_argument('--bearer_token', required=True)
        parser.add_argument('--domain', required=True)
        parser.add_argument('--get_cdn', required=True)

        parser.add_argument('--test_data_cdn_url', required=True)
        parser.add_argument('--train_x_cdn_url', required=True)
        parser.add_argument('--train_y_cdn_url', required=True)
        parser.add_argument('--preprocesser_pickle_cdn_url', required=True)
        parser.add_argument('--preprocesser_metadata_cdn_url', required=True)
        parser.add_argument('--feature_selector_cdn_url', required=True)
        parser.add_argument('--pca_model_cdn_url', required=True)
        parser.add_argument('--pca_metadata_cdn_url', required=True)
        parser.add_argument('--schema_json', required=True)

        args = parser.parse_args()

        def encode_special_chars(text):
            text = text.replace("$", "%24")
            text = text.replace("(", "%28")
            text = text.replace(")", "%29")
            text = text.replace("[", "%5B")
            text = text.replace("]", "%5D")
            text = text.replace("{", "%7B")
            text = text.replace("}", "%7D")
            return text

        with open(args.bearer_token, "r") as f:
            bearer_token = f.read().strip()

        upload_url = (
            f"{args.domain}"
            "/mobius-content-service/v1.0/content/upload"
            "?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
        )

        def ensure_csv(path):
            # If path already ends with .csv, return as-is
            if path.endswith(".csv"):
                return path
            
            # If path ends with .parquet, convert to CSV
            if path.endswith(".parquet"):
                df = pd.read_parquet(path)
                tmp_csv = tempfile.mkstemp(suffix=".csv")[1]
                df.to_csv(tmp_csv, index=False)
                return tmp_csv
            
            # For files without extension (like 'data'), try to detect format
            # First, try reading as parquet (most common in ML pipelines)
            try:
                df = pd.read_parquet(path)
                tmp_csv = tempfile.mkstemp(suffix=".csv")[1]
                df.to_csv(tmp_csv, index=False)
                return tmp_csv
            except Exception:
                pass
            
            # If parquet fails, try reading as CSV
            try:
                # Test if it's already a valid CSV
                pd.read_csv(path, nrows=1)
                return path
            except Exception:
                pass
            
            raise ValueError(f"Unsupported dataset format or unable to read: {path}")

        def sanitize_file_for_upload(file_path):
            original_name = os.path.basename(file_path)
            sanitized_name = encode_special_chars(original_name)
            
            if sanitized_name != original_name:
                # Create a copy with sanitized name
                temp_dir = tempfile.gettempdir()
                sanitized_path = os.path.join(temp_dir, sanitized_name)
                shutil.copy2(file_path, sanitized_path)
                return sanitized_path, sanitized_path  # return path and temp file to cleanup
            
            return file_path, None

        def upload(file_path, output_path):
            sanitized_path, temp_file = sanitize_file_for_upload(file_path)
            
            try:
                cmd = [
                    "curl",
                    "--location", upload_url,
                    "--header", f"Authorization: Bearer {bearer_token}",
                    "--form", f"file=@{sanitized_path}",
                    "--fail",
                    "--show-error"
                ]

                result = subprocess.run(cmd, capture_output=True, check=True)
                response = json.loads(result.stdout.decode())

                relative_url = response.get("cdnUrl")
                if not relative_url:
                    raise RuntimeError(f"cdnUrl missing in response: {response}")

                full_url = f"{args.get_cdn}{relative_url}"

                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, "w") as f:
                    f.write(full_url)

                return full_url
            
            finally:
                if temp_file and os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except Exception:
                        pass

        cdn_map = {}
        cdn_map["train_x_cdn"] = upload(ensure_csv(args.train_x), args.train_x_cdn_url)
        cdn_map["train_y_cdn"] = upload(ensure_csv(args.train_y), args.train_y_cdn_url)
        cdn_map["test_data_cdn"] = upload(ensure_csv(args.test_data), args.test_data_cdn_url)
        cdn_map["preprocessor_cdn"] = upload(args.preprocesser_pickle, args.preprocesser_pickle_cdn_url)
        cdn_map["preprocessor_metadata_cdn"] = upload(args.preprocesser_metadata, args.preprocesser_metadata_cdn_url)
        cdn_map["feature_selector_cdn"] = upload(args.feature_selector, args.feature_selector_cdn_url)
        cdn_map["pca_cdn"] = upload(args.pca_model, args.pca_model_cdn_url)
        cdn_map["pca_metadata_cdn"] = upload(args.pca_metadata, args.pca_metadata_cdn_url)

        schema = {
            "timestamp": int(time.time()),
            **cdn_map
        }

        os.makedirs(os.path.dirname(args.schema_json), exist_ok=True)
        with open(args.schema_json, "w") as f:
            json.dump(schema, f, indent=2)

    args:
      - --test_data
      - {inputPath: test_data}
      - --train_x
      - {inputPath: train_x}
      - --train_y
      - {inputPath: train_y}
      - --preprocesser_pickle
      - {inputPath: preprocesser_pickle}
      - --preprocesser_metadata
      - {inputPath: preprocesser_metadata}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca_model
      - {inputPath: pca_model}
      - --pca_metadata
      - {inputPath: pca_metadata}
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --train_x_cdn_url
      - {outputPath: train_x_cdn_url}
      - --train_y_cdn_url
      - {outputPath: train_y_cdn_url}
      - --preprocesser_pickle_cdn_url
      - {outputPath: preprocesser_pickle_cdn_url}
      - --preprocesser_metadata_cdn_url
      - {outputPath: preprocesser_metadata_cdn_url}
      - --feature_selector_cdn_url
      - {outputPath: feature_selector_cdn_url}
      - --pca_model_cdn_url
      - {outputPath: pca_model_cdn_url}
      - --pca_metadata_cdn_url
      - {outputPath: pca_metadata_cdn_url}
      - --test_data_cdn_url
      - {outputPath: test_data_cdn_url}
      - --schema_json
      - {outputPath: schema_json}
