name: Upload to CDN v6
inputs:
  - {name: test_data, type: Dataset, description: "Test dataset file (CSV or Parquet)"}
  - {name: train_x, type: Dataset, description: "Training features dataset (CSV or Parquet)"}
  - {name: train_y, type: Dataset, description: "Training labels dataset (CSV or Parquet)"}
  - {name: preprocesser_pickle, type: Data, description: "Preprocessor pickle file"}
  - {name: preprocesser_metadata, type: Data, description: "Preprocessor metadata JSON"}
  - {name: cleaning_metadata, type: Data, description: "Cleaning metadata JSON"}
  - {name: feature_selector, type: Data, description: "Feature selector pickle"}
  - {name: pca_model, type: Data, description: "PCA model pickle file"}
  - {name: pca_metadata, type: Data, description: "PCA metadata JSON"}
  - {name: bearer_token, type: string, description: "Bearer token for CDN authentication"}
  - {name: domain, type: String, description: "Upload service base domain"}
  - {name: get_cdn, type: String, description: "Public CDN base domain"}

outputs:
  - {name: train_x_cdn_url, type: String}
  - {name: train_y_cdn_url, type: String}
  - {name: test_data_cdn_url, type: String}
  - {name: preprocesser_pickle_cdn_url, type: String}
  - {name: preprocesser_metadata_cdn_url, type: String}
  - {name: cleaning_metadata_cdn_url, type: String}
  - {name: feature_selector_cdn_url, type: String}
  - {name: pca_model_cdn_url, type: String}
  - {name: pca_metadata_cdn_url, type: String}
  - {name: schema_json, type: String}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -ec
      - |
        if ! command -v curl >/dev/null 2>&1; then
          apt-get update >/dev/null && apt-get install -y curl >/dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import time
        import tempfile
        import shutil
        import pandas as pd

        parser = argparse.ArgumentParser()

        parser.add_argument('--test_data', required=True)
        parser.add_argument('--train_x', required=True)
        parser.add_argument('--train_y', required=True)
        parser.add_argument('--preprocesser_pickle', required=True)
        parser.add_argument('--preprocesser_metadata', required=True)
        parser.add_argument('--cleaning_metadata', required=True)
        parser.add_argument('--feature_selector', required=True)
        parser.add_argument('--pca_model', required=True)
        parser.add_argument('--pca_metadata', required=True)

        parser.add_argument('--bearer_token', required=True)
        parser.add_argument('--domain', required=True)
        parser.add_argument('--get_cdn', required=True)

        parser.add_argument('--train_x_cdn_url', required=True)
        parser.add_argument('--train_y_cdn_url', required=True)
        parser.add_argument('--test_data_cdn_url', required=True)
        parser.add_argument('--preprocesser_pickle_cdn_url', required=True)
        parser.add_argument('--preprocesser_metadata_cdn_url', required=True)
        parser.add_argument('--cleaning_metadata_cdn_url', required=True)
        parser.add_argument('--feature_selector_cdn_url', required=True)
        parser.add_argument('--pca_model_cdn_url', required=True)
        parser.add_argument('--pca_metadata_cdn_url', required=True)
        parser.add_argument('--schema_json', required=True)

        args = parser.parse_args()

        def encode_special_chars(text):
            return (
                text.replace("$", "%24")
                    .replace("(", "%28")
                    .replace(")", "%29")
                    .replace("[", "%5B")
                    .replace("]", "%5D")
                    .replace("{", "%7B")
                    .replace("}", "%7D")
            )

        with open(args.bearer_token, "r") as f:
            bearer_token = f.read().strip()

        upload_url = (
            f"{args.domain}"
            "/mobius-content-service/v1.0/content/upload"
            "?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
        )

        def ensure_csv(path):
            if path.endswith(".csv"):
                return path
            if path.endswith(".parquet"):
                df = pd.read_parquet(path)
                tmp = tempfile.mkstemp(suffix=".csv")[1]
                df.to_csv(tmp, index=False)
                return tmp
            try:
                df = pd.read_parquet(path)
                tmp = tempfile.mkstemp(suffix=".csv")[1]
                df.to_csv(tmp, index=False)
                return tmp
            except Exception:
                pass
            try:
                pd.read_csv(path, nrows=1)
                return path
            except Exception:
                pass
            raise ValueError(f"Unsupported dataset format: {path}")

        def sanitize_file(file_path):
            name = os.path.basename(file_path)
            safe = encode_special_chars(name)
            if name != safe:
                temp_path = os.path.join(tempfile.gettempdir(), safe)
                shutil.copy2(file_path, temp_path)
                return temp_path, temp_path
            return file_path, None

        def upload(file_path, output_path):
            safe_path, temp = sanitize_file(file_path)
            try:
                result = subprocess.run(
                    [
                        "curl",
                        "--location", upload_url,
                        "--header", f"Authorization: Bearer {bearer_token}",
                        "--form", f"file=@{safe_path}",
                        "--fail",
                        "--show-error",
                    ],
                    capture_output=True,
                    check=True,
                )
                response = json.loads(result.stdout.decode())
                rel = response.get("cdnUrl")
                if not rel:
                    raise RuntimeError(f"cdnUrl missing: {response}")

                full_url = f"{args.get_cdn}{encode_special_chars(rel)}"
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, "w") as f:
                    f.write(full_url)
                return full_url
            finally:
                if temp and os.path.exists(temp):
                    os.remove(temp)

        cdn_map = {
            "train_x_cdn": upload(ensure_csv(args.train_x), args.train_x_cdn_url),
            "train_y_cdn": upload(ensure_csv(args.train_y), args.train_y_cdn_url),
            "test_data_cdn": upload(ensure_csv(args.test_data), args.test_data_cdn_url),
            "preprocessor_cdn": upload(args.preprocesser_pickle, args.preprocesser_pickle_cdn_url),
            "preprocessor_metadata_cdn": upload(args.preprocesser_metadata, args.preprocesser_metadata_cdn_url),
            "cleaning_metadata_cdn": upload(args.cleaning_metadata, args.cleaning_metadata_cdn_url),
            "feature_selector_cdn": upload(args.feature_selector, args.feature_selector_cdn_url),
            "pca_cdn": upload(args.pca_model, args.pca_model_cdn_url),
            "pca_metadata_cdn": upload(args.pca_metadata, args.pca_metadata_cdn_url),
        }

        schema = {
            "timestamp": int(time.time()),
            **cdn_map
        }

        os.makedirs(os.path.dirname(args.schema_json), exist_ok=True)
        with open(args.schema_json, "w") as f:
            json.dump(schema, f, indent=2)

    args:
      - --test_data
      - {inputPath: test_data}
      - --train_x
      - {inputPath: train_x}
      - --train_y
      - {inputPath: train_y}
      - --preprocesser_pickle
      - {inputPath: preprocesser_pickle}
      - --preprocesser_metadata
      - {inputPath: preprocesser_metadata}
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca_model
      - {inputPath: pca_model}
      - --pca_metadata
      - {inputPath: pca_metadata}
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --train_x_cdn_url
      - {outputPath: train_x_cdn_url}
      - --train_y_cdn_url
      - {outputPath: train_y_cdn_url}
      - --test_data_cdn_url
      - {outputPath: test_data_cdn_url}
      - --preprocesser_pickle_cdn_url
      - {outputPath: preprocesser_pickle_cdn_url}
      - --preprocesser_metadata_cdn_url
      - {outputPath: preprocesser_metadata_cdn_url}
      - --cleaning_metadata_cdn_url
      - {outputPath: cleaning_metadata_cdn_url}
      - --feature_selector_cdn_url
      - {outputPath: feature_selector_cdn_url}
      - --pca_model_cdn_url
      - {outputPath: pca_model_cdn_url}
      - --pca_metadata_cdn_url
      - {outputPath: pca_metadata_cdn_url}
      - --schema_json
      - {outputPath: schema_json}
