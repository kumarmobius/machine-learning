name: Supervised Training v1.0
inputs:
  - {name: X_data, type: Dataset, description: "Parquet file for features (DataFrame)"}
  - {name: y_data, type: Dataset, description: "Parquet file for target (Series or single-column DataFrame) - already encoded"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enable_balancing, type: String, description: "true/false for class imbalance handling (classification only)", optional: true, default: "true"}
  - {name: balancing_technique, type: String, description: "class_weight, sample_weight, or none (classification only)", optional: true, default: "class_weight"}
  - {name: target_column, type: String, description: "Optional. Column name inside y parquet if it has multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata from preprocessing (for reference)", optional: true}
  - {name: model_name, type: String, description: "Model to train: knn | decisiontree | extratrees | svc | linearsvc | nusvc | svr | linearsvr", optional: true, default: "decisiontree"}
  - {name: n_neighbors, type: Integer, description: "Number of neighbors (for KNN). Used only when model_name=knn", optional: true, default: "5"}
  - {name: dt_max_depth, type: Integer, description: "Max depth for DecisionTree (None for automatic). Used only when model_name=decisiontree", optional: true, default: "5"}
  - {name: n_estimators, type: Integer, description: "Number of estimators (for tree ensembles)", optional: true, default: "100"}
  - {name: min_samples_leaf, type: Integer, description: "min_samples_leaf for tree models", optional: true, default: "1"}
  - {name: n_jobs, type: Integer, description: "n_jobs for parallel models (-1 for all cores)", optional: true, default: "-1"}
  - {name: svm_kernel, type: String, description: "kernel for SVC/SVR (rbf|linear|poly|sigmoid)", optional: true, default: "rbf"}
  - {name: svm_C, type: Float, description: "C parameter for SVMs", optional: true, default: "1.0"}
  - {name: svm_gamma, type: String, description: "gamma for SVC/SVR ('scale','auto' or float)", optional: true, default: "scale"}
  - {name: svm_nu, type: Float, description: "nu parameter for NuSVC (0<nu<=1). Used only for nusvc", optional: true, default: "0.5"}
  - {name: svm_probability, type: String, description: "true/false: enable probability=True for SVC (slower)", optional: true, default: "false"}
  - {name: random_state, type: Integer, description: "Random seed for reproducibility", optional: true, default: "48"}
  - {name: cv_folds, type: Integer, description: "Number of folds for cross-validation", optional: true, default: "5"}
  - {name: max_iter_svm, type: Integer, description: "Maximum iterations for SVM convergence", optional: true, default: "10000"}
  - {name: early_stopping, type: String, description: "Enable early stopping for tree models (true/false)", optional: true, default: "false"}

outputs:
  - {name: model_pickle, type: Model, description: "Pickled trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}
  - {name: model_config, type: Data, description: "Comprehensive model configuration JSON with all hyperparameters and metadata"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, logging, time, hashlib
        import pandas as pd, numpy as np
        import cloudpickle
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        from sklearn.preprocessing import LabelEncoder
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error, classification_report, confusion_matrix
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score, train_test_split
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        logger = logging.getLogger(__name__)
        
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")
        
        def load_parquet_df(path):
            logger.info(f"Loading parquet file: {path}")
            return pd.read_parquet(path)
        
        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load metadata as JSON from {path}: {e}")
                return None
        
        def compute_data_signature(X, y):
            combined = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)
            return hashlib.sha256(pd.util.hash_pandas_object(combined).values.tobytes()).hexdigest()[:16]
        
        def save_model_config(config, path):
            ensure_dir_for(path)
            with open(path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"Model configuration saved to {path}")
        
        ap = argparse.ArgumentParser(description="Production Model Training")
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enable_balancing', type=str, default="true")
        ap.add_argument('--balancing_technique', type=str, default="class_weight")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--model_name', type=str, default="decisiontree")
        ap.add_argument('--n_neighbors', type=int, default=5)
        ap.add_argument('--dt_max_depth', type=str, default="5")
        ap.add_argument('--n_estimators', type=int, default=100)
        ap.add_argument('--min_samples_leaf', type=int, default=1)
        ap.add_argument('--n_jobs', type=int, default=-1)
        ap.add_argument('--svm_kernel', type=str, default="rbf")
        ap.add_argument('--svm_C', type=float, default=1.0)
        ap.add_argument('--svm_gamma', type=str, default="scale")
        ap.add_argument('--svm_nu', type=float, default=0.5)
        ap.add_argument('--svm_probability', type=str, default="false")
        ap.add_argument('--random_state', type=int, default=48)
        ap.add_argument('--cv_folds', type=int, default=5)
        ap.add_argument('--max_iter_svm', type=int, default=10000)
        ap.add_argument('--early_stopping', type=str, default="false")
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        ap.add_argument('--model_config', type=str, required=True)
        args = ap.parse_args()
        
        start_time = time.time()
        
        try:
            logger.info("="*80)
            logger.info("PRODUCTION MODEL TRAINING STARTED")
            logger.info(f"Model: {args.model_name}, Type: {args.model_type}")
            logger.info(f"Enable balancing: {args.enable_balancing}, Method: {args.balancing_technique}")
            logger.info("="*80)
            
            # Load data
            X = load_parquet_df(args.X_data)
            logger.info(f"X shape: {X.shape}")
            logger.info(f"X columns ({len(X.columns)}): {list(X.columns)}")
            logger.info(f"X dtypes: {X.dtypes.to_dict()}")
            
            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    logger.info(f"Auto-detected single column: {y_df.columns[0]}")
                else:
                    col = args.target_column.strip()
                    if not col:
                        raise ValueError(f"y parquet has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
                    y = y_df[col].copy()
            else:
                y = pd.Series(y_df).copy()
            
            # Robust data alignment
            if len(X) != len(y):
                logger.warning(f"Length mismatch detected: X={len(X)}, y={len(y)}")
                common_idx = X.index.intersection(y.index)
                if len(common_idx) > 0:
                    X = X.loc[common_idx].sort_index()
                    y = y.loc[common_idx].sort_index()
                    logger.info(f"Aligned using common indices: {len(common_idx)} samples")
                else:
                    m = min(len(X), len(y))
                    X = X.reset_index(drop=True).iloc[:m, :].copy()
                    y = y.reset_index(drop=True).iloc[:m].copy()
                    logger.info(f"Aligned by index reset: {m} samples")
            
            # Data validation
            logger.info("DATA VALIDATION")
            logger.info(f"Final X shape: {X.shape}, y shape: {y.shape}")
            
            # Check for NaN in features
            nan_count = X.isna().sum().sum()
            if nan_count > 0:
                logger.warning(f"Found {nan_count} NaN values in features")
            
            # Log target distribution
            logger.info("TARGET DISTRIBUTION ANALYSIS")
            try:
                value_counts = y.value_counts(dropna=False)
                logger.info(f"Value counts:{value_counts.to_string()}")
                logger.info(f"Unique classes: {int(y.nunique())}")
                if args.model_type == "classification":
                    dist_pct = y.value_counts(dropna=False, normalize=True) * 100
                    logger.info(f"Class distribution %:{dist_pct.round(2).to_string()}")
            except Exception as e:
                logger.warning(f"Could not analyze target distribution: {e}")
            
            # Compute data signature
            data_signature = compute_data_signature(X, y)
            logger.info(f"Data signature: {data_signature}")
            
            # Initialize metrics and config
            task = args.model_type.strip().lower()
            metrics = {
                "task": task,
                "samples": int(len(y)),
                "features": int(X.shape[1]),
                "data_signature": data_signature,
                "training_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            model_name = args.model_name.strip().lower()
            model_obj = None
            
            # Comprehensive model configuration
            model_config = {
                "model_info": {
                    "model_name": model_name,
                    "model_type": task,
                    "model_version": "1.0.0",
                    "training_date": time.strftime("%Y-%m-%d"),
                    "training_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                },
                "training_parameters": {
                    "model_type": args.model_type,
                    "enable_balancing": args.enable_balancing,
                    "balancing_technique": args.balancing_technique,
                    "target_column": args.target_column,
                    "model_name": args.model_name,
                    "n_neighbors": args.n_neighbors,
                    "dt_max_depth": args.dt_max_depth,
                    "n_estimators": args.n_estimators,
                    "min_samples_leaf": args.min_samples_leaf,
                    "n_jobs": args.n_jobs,
                    "svm_kernel": args.svm_kernel,
                    "svm_C": args.svm_C,
                    "svm_gamma": args.svm_gamma,
                    "svm_nu": args.svm_nu,
                    "svm_probability": args.svm_probability,
                    "random_state": args.random_state,
                    "cv_folds": args.cv_folds,
                    "max_iter_svm": args.max_iter_svm,
                    "early_stopping": args.early_stopping
                },
                "data_info": {
                    "samples": int(len(y)),
                    "features": int(X.shape[1]),
                    "target_shape": y.shape,
                    "data_signature": data_signature,
                    "feature_names": X.columns.tolist(),
                    "feature_dtypes": X.dtypes.astype(str).to_dict(),
                    "has_nan_features": bool(nan_count > 0),
                    "nan_count": int(nan_count)
                },
                "hyperparameters": {},
                "performance_metrics": {},
                "environment": {
                    "python_version": sys.version,
                    "numpy_version": np.__version__,
                    "pandas_version": pd.__version__
                }
            }
            
            # Classification task
            if task == "classification":
                logger.info("CLASSIFICATION TASK INITIALIZED")
                meta = try_load_metadata(args.preprocess_metadata)
                label_mapping = meta.get("label_mapping") if meta else None
                
                # Target encoding
                try:
                    y_prepared = pd.Series(y).astype(int)
                    map_info = {"mode": "already_encoded", "label_mapping_reference": label_mapping}
                    logger.info("Target already encoded as integers")
                except Exception as e:
                    logger.info(f"Encoding target with LabelEncoder: {e}")
                    le = LabelEncoder()
                    y_prepared = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
                    map_info = {
                        "mode": "label_encoder_fallback", 
                        "classes": le.classes_.tolist(),
                        "n_classes": len(le.classes_)
                    }
                    model_config["label_encoder"] = {
                        "classes": le.classes_.tolist(),
                        "n_classes": len(le.classes_)
                    }
                
                # Handle invalid values
                valid_mask = y_prepared.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_prepared = y_prepared[valid_mask].reset_index(drop=True)
                    logger.warning(f"Dropped {dropped} invalid rows")
                
                # Initialize balancing parameters
                use_class_weight = False
                sample_weight = None
                
                # Apply balancing only if enabled and for classification
                if bool_from_str(args.enable_balancing) and task == "classification":
                    balancing_method = args.balancing_technique.strip().lower()
                    
                    if balancing_method == "class_weight":
                        logger.info("Using class_weight='balanced' for imbalance handling")
                        use_class_weight = True
                        model_config["training_parameters"]["class_weight_used"] = "balanced"
                        
                    elif balancing_method == "sample_weight":
                        logger.info("Computing sample weights for imbalance handling")
                        try:
                            sample_weight = compute_sample_weight(class_weight='balanced', y=y_prepared)
                            model_config["training_parameters"]["sample_weight_used"] = True
                            logger.info(f"Sample weights computed: shape {sample_weight.shape}")
                        except Exception as e:
                            logger.warning(f"Could not compute sample weights: {e}")
                            sample_weight = None
                    
                    elif balancing_method == "none":
                        logger.info("Balancing disabled as per configuration")
                    else:
                        logger.warning(f"Unknown balancing_method: {balancing_method}, defaulting to 'none'")
                
                # Model selection and configuration
                if model_name == "knn":
                    logger.info(f"Training KNeighborsClassifier (n_neighbors={args.n_neighbors})")
                    clf = KNeighborsClassifier(
                        n_neighbors=int(args.n_neighbors), 
                        n_jobs=int(args.n_jobs),
                        algorithm='auto'
                    )
                    model_config["hyperparameters"] = {
                        "n_neighbors": int(args.n_neighbors),
                        "n_jobs": int(args.n_jobs),
                        "algorithm": "auto",
                        "weights": "uniform",
                        "metric": "minkowski",
                        "p": 2
                    }
                
                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip().lower() in ("", "none", "null") else int(args.dt_max_depth)
                    early_stopping = bool_from_str(args.early_stopping)
                    logger.info(f"Training DecisionTreeClassifier (max_depth={max_depth}, early_stopping={early_stopping})")
                    
                    # Apply class_weight if requested and method is class_weight
                    class_weight_param = 'balanced' if (use_class_weight and balancing_method == "class_weight") else None
                    
                    clf = DecisionTreeClassifier(
                        max_depth=max_depth,
                        min_samples_leaf=int(args.min_samples_leaf),
                        class_weight=class_weight_param,
                        random_state=args.random_state
                    )
                    model_config["hyperparameters"] = {
                        "max_depth": max_depth,
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "class_weight": class_weight_param,
                        "criterion": "gini",
                        "splitter": "best",
                        "min_samples_split": 2,
                        "min_weight_fraction_leaf": 0.0,
                        "max_features": None,
                        "max_leaf_nodes": None,
                        "ccp_alpha": 0.0
                    }
                
                elif model_name == "extratrees":
                    logger.info(f"Training ExtraTreesClassifier (n_estimators={args.n_estimators})")
                    
                    # Apply class_weight if requested and method is class_weight
                    class_weight_param = 'balanced' if (use_class_weight and balancing_method == "class_weight") else None
                    
                    clf = ExtraTreesClassifier(
                        n_estimators=int(args.n_estimators),
                        min_samples_leaf=int(args.min_samples_leaf),
                        n_jobs=int(args.n_jobs),
                        random_state=args.random_state,
                        class_weight=class_weight_param,
                        bootstrap=True
                    )
                    model_config["hyperparameters"] = {
                        "n_estimators": int(args.n_estimators),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "n_jobs": int(args.n_jobs),
                        "class_weight": class_weight_param,
                        "criterion": "gini",
                        "max_depth": None,
                        "min_samples_split": 2,
                        "max_features": "sqrt",
                        "bootstrap": True,
                        "oob_score": False,
                        "warm_start": False,
                        "ccp_alpha": 0.0
                    }
                
                elif model_name in ("svc", "nusvc", "linearsvc"):
                    svm_prob = bool_from_str(args.svm_probability)
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    nu = float(args.svm_nu)
                    
                    # Apply class_weight if requested and method is class_weight
                    class_weight_param = 'balanced' if (use_class_weight and balancing_method == "class_weight") else None
                    
                    logger.info(f"Training SVM classifier ({model_name}) with kernel={kernel}, C={C}, gamma={gamma}")
                    
                    if model_name == "svc":
                        gamma_val = None if gamma.lower() in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        clf = SVC(
                            kernel=kernel,
                            C=C,
                            gamma=gamma_val,
                            probability=svm_prob,
                            class_weight=class_weight_param,
                            random_state=args.random_state,
                            max_iter=args.max_iter_svm
                        )
                    elif model_name == "nusvc":
                        gamma_val = None if gamma.lower() in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        clf = NuSVC(
                            kernel=kernel,
                            nu=nu,
                            C=C,
                            gamma=gamma_val,
                            probability=svm_prob,
                            class_weight=class_weight_param,
                            random_state=args.random_state,
                            max_iter=args.max_iter_svm
                        )
                    else:  # linearsvc
                        clf = LinearSVC(
                            C=C,
                            class_weight=class_weight_param,
                            random_state=args.random_state,
                            max_iter=args.max_iter_svm,
                            dual='auto'
                        )
                    
                    model_config["hyperparameters"] = {
                        "svm_type": model_name,
                        "kernel": kernel,
                        "C": C,
                        "gamma": gamma,
                        "nu": nu if model_name == "nusvc" else None,
                        "probability": svm_prob,
                        "class_weight": class_weight_param,
                        "max_iter": args.max_iter_svm,
                        "decision_function_shape": "ovr",
                        "shrinking": True,
                        "tol": 1e-3,
                        "cache_size": 200,
                        "break_ties": False
                    }
                
                else:
                    raise ValueError(f"Unsupported model_name '{model_name}' for classification")
                
                # Cross-validation
                try:
                    logger.info(f"Performing {args.cv_folds}-fold cross validation")
                    cv_scores = cross_val_score(
                        clf, X, y_prepared, 
                        cv=int(args.cv_folds), 
                        scoring='accuracy', 
                        n_jobs=int(args.n_jobs)
                    )
                    metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                    metrics["cv_accuracy_std"] = float(cv_scores.std())
                    metrics["cv_scores"] = cv_scores.tolist()
                    logger.info(f"CV Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})")
                except Exception as e:
                    logger.warning(f"Cross-validation failed: {e}")
                
                # Model training
                try:
                    logger.info("Fitting model to training data")
                    fit_start = time.time()
                    
                    # Pass sample_weight to fit() if sample_weight method is selected
                    if sample_weight is not None and balancing_method == "sample_weight":
                        logger.info("Using sample_weight in model.fit()")
                        clf.fit(X, y_prepared, sample_weight=sample_weight)
                    else:
                        clf.fit(X, y_prepared)
                    
                    fit_time = time.time() - fit_start
                    logger.info(f"Model fitting completed in {fit_time:.2f} seconds")
                    
                    # Store model attributes
                    if hasattr(clf, 'feature_importances_'):
                        model_config["feature_importances"] = dict(zip(X.columns, clf.feature_importances_.tolist()))
                    if hasattr(clf, 'n_features_in_'):
                        model_config["model_info"]["n_features_in"] = int(clf.n_features_in_)
                    
                except Exception as e:
                    logger.error(f"Model fitting failed: {e}")
                    raise
                
                # Predictions and metrics
                y_pred = clf.predict(X)
                
                # Detailed classification report
                try:
                    class_report = classification_report(y_prepared, y_pred, output_dict=True, zero_division=0)
                    metrics["classification_report"] = class_report
                except Exception as e:
                    logger.warning(f"Could not generate classification report: {e}")
                
                # Confusion matrix
                try:
                    cm = confusion_matrix(y_prepared, y_pred)
                    metrics["confusion_matrix"] = cm.tolist()
                except Exception as e:
                    logger.warning(f"Could not generate confusion matrix: {e}")
                
                # Core metrics
                metrics.update({
                    "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                    "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "model": model_name,
                    "balancing_enabled": bool_from_str(args.enable_balancing),
                    "balancing_technique": args.balancing_technique,
                    "label_info": map_info,
                    "training_time_seconds": fit_time
                })
                
                model_obj = clf
            
            # Regression task
            elif task == "regression":
                logger.info("REGRESSION TASK INITIALIZED")
                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    logger.warning(f"Dropping {dropped} rows with non-numeric target")
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)
                
                # Target statistics
                target_stats = {
                    "mean": float(y_num.mean()),
                    "std": float(y_num.std()),
                    "min": float(y_num.min()),
                    "max": float(y_num.max()),
                    "median": float(y_num.median())
                }
                logger.info(f"Target statistics: {target_stats}")
                metrics["target_statistics"] = target_stats
                
                if model_name == "knn":
                    logger.info(f"Training KNeighborsRegressor (n_neighbors={args.n_neighbors})")
                    reg = KNeighborsRegressor(
                        n_neighbors=int(args.n_neighbors), 
                        n_jobs=int(args.n_jobs),
                        algorithm='auto'
                    )
                    model_config["hyperparameters"] = {
                        "n_neighbors": int(args.n_neighbors),
                        "n_jobs": int(args.n_jobs),
                        "algorithm": "auto",
                        "weights": "uniform",
                        "metric": "minkowski",
                        "p": 2,
                        "leaf_size": 30
                    }
                
                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip().lower() in ("", "none", "null") else int(args.dt_max_depth)
                    early_stopping = bool_from_str(args.early_stopping)
                    logger.info(f"Training DecisionTreeRegressor (max_depth={max_depth})")
                    reg = DecisionTreeRegressor(
                        max_depth=max_depth,
                        min_samples_leaf=int(args.min_samples_leaf),
                        random_state=args.random_state
                    )
                    model_config["hyperparameters"] = {
                        "max_depth": max_depth,
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "criterion": "squared_error",
                        "splitter": "best",
                        "min_samples_split": 2,
                        "min_weight_fraction_leaf": 0.0,
                        "max_features": None,
                        "max_leaf_nodes": None,
                        "ccp_alpha": 0.0
                    }
                
                elif model_name == "extratrees":
                    logger.info(f"Training ExtraTreesRegressor (n_estimators={args.n_estimators})")
                    reg = ExtraTreesRegressor(
                        n_estimators=int(args.n_estimators),
                        min_samples_leaf=int(args.min_samples_leaf),
                        n_jobs=int(args.n_jobs),
                        random_state=args.random_state,
                        bootstrap=True
                    )
                    model_config["hyperparameters"] = {
                        "n_estimators": int(args.n_estimators),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "n_jobs": int(args.n_jobs),
                        "criterion": "squared_error",
                        "max_depth": None,
                        "min_samples_split": 2,
                        "max_features": "sqrt",
                        "bootstrap": True,
                        "oob_score": False,
                        "warm_start": False,
                        "ccp_alpha": 0.0
                    }
                
                elif model_name in ("svr", "linearsvr"):
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    
                    logger.info(f"Training SVM regressor ({model_name}) with kernel={kernel}, C={C}")
                    
                    if model_name == "svr":
                        gamma_val = None if gamma.lower() in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        reg = SVR(
                            kernel=kernel,
                            C=C,
                            gamma=gamma_val,
                            max_iter=args.max_iter_svm
                        )
                    else:
                        reg = LinearSVR(
                            C=C,
                            max_iter=args.max_iter_svm,
                            random_state=args.random_state,
                            dual='auto'
                        )
                    
                    model_config["hyperparameters"] = {
                        "svm_type": model_name,
                        "kernel": kernel,
                        "C": C,
                        "gamma": gamma,
                        "max_iter": args.max_iter_svm,
                        "epsilon": 0.1 if model_name == "svr" else 0.0,
                        "tol": 1e-3,
                        "shrinking": True if model_name == "svr" else None,
                        "cache_size": 200 if model_name == "svr" else None
                    }
                
                else:
                    raise ValueError(f"Unsupported model_name '{model_name}' for regression")
                
                # Cross-validation for regression
                try:
                    logger.info(f"Performing {args.cv_folds}-fold cross validation")
                    cv_scores = cross_val_score(
                        reg, X, y_num, 
                        cv=int(args.cv_folds), 
                        scoring='r2', 
                        n_jobs=int(args.n_jobs)
                    )
                    metrics["cv_r2_mean"] = float(cv_scores.mean())
                    metrics["cv_r2_std"] = float(cv_scores.std())
                    metrics["cv_scores"] = cv_scores.tolist()
                    logger.info(f"CV R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})")
                except Exception as e:
                    logger.warning(f"Cross-validation failed: {e}")
                
                # Model training
                try:
                    logger.info("Fitting model to training data")
                    fit_start = time.time()
                    reg.fit(X, y_num)
                    fit_time = time.time() - fit_start
                    logger.info(f"Model fitting completed in {fit_time:.2f} seconds")
                    
                    # Store model attributes
                    if hasattr(reg, 'feature_importances_'):
                        model_config["feature_importances"] = dict(zip(X.columns, reg.feature_importances_.tolist()))
                    if hasattr(reg, 'n_features_in_'):
                        model_config["model_info"]["n_features_in"] = int(reg.n_features_in_)
                    
                except Exception as e:
                    logger.error(f"Model fitting failed: {e}")
                    raise
                
                # Predictions and metrics
                y_pred = reg.predict(X)
                metrics.update({
                    "r2_train": float(r2_score(y_num, y_pred)),
                    "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                    "mae_train": float(mean_absolute_error(y_num, y_pred)),
                    "mse_train": float(mean_squared_error(y_num, y_pred)),
                    "model": model_name,
                    "training_time_seconds": fit_time
                })
                
                # Calculate relative errors
                y_mean = y_num.mean()
                if y_mean != 0:
                    metrics["rmse_relative"] = float(np.sqrt(mean_squared_error(y_num, y_pred)) / abs(y_mean))
                    metrics["mae_relative"] = float(mean_absolute_error(y_num, y_pred) / abs(y_mean))
                
                model_obj = reg
            
            else:
                raise ValueError(f"Unsupported model_type '{args.model_type}'")
            
            # Finalize model configuration
            total_time = time.time() - start_time
            model_config["performance_metrics"] = metrics
            model_config["model_info"]["training_duration_seconds"] = total_time
            model_config["model_info"]["model_object_type"] = type(model_obj).__name__
            
            # Save model using cloudpickle
            logger.info(f"Saving model to {args.model_pickle}")
            ensure_dir_for(args.model_pickle)
            with open(args.model_pickle, 'wb') as f:
                cloudpickle.dump(model_obj, f)
            
            # Save metrics
            logger.info(f"Saving metrics to {args.metrics_json}")
            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            # Save comprehensive model config
            save_model_config(model_config, args.model_config)
            
            logger.info("="*80)
            logger.info("TRAINING COMPLETED SUCCESSFULLY")
            logger.info(f"Total time: {total_time:.2f} seconds")
            logger.info(f"Model: {model_name}")
            logger.info(f"Samples: {len(y)}, Features: {X.shape[1]}")
            logger.info("="*80)
            
        except Exception as e:
            logger.error("CRITICAL ERROR DURING TRAINING")
            logger.error(f"Error type: {type(e).__name__}")
            logger.error(f"Error message: {str(e)}")
            logger.error("Traceback:")
            logger.error(traceback.format_exc())
            logger.error("="*80)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enable_balancing
      - {inputValue: enable_balancing}
      - --balancing_technique
      - {inputValue: balancing_technique}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_name
      - {inputValue: model_name}
      - --n_neighbors
      - {inputValue: n_neighbors}
      - --dt_max_depth
      - {inputValue: dt_max_depth}
      - --n_estimators
      - {inputValue: n_estimators}
      - --min_samples_leaf
      - {inputValue: min_samples_leaf}
      - --n_jobs
      - {inputValue: n_jobs}
      - --svm_kernel
      - {inputValue: svm_kernel}
      - --svm_C
      - {inputValue: svm_C}
      - --svm_gamma
      - {inputValue: svm_gamma}
      - --svm_nu
      - {inputValue: svm_nu}
      - --svm_probability
      - {inputValue: svm_probability}
      - --random_state
      - {inputValue: random_state}
      - --cv_folds
      - {inputValue: cv_folds}
      - --max_iter_svm
      - {inputValue: max_iter_svm}
      - --early_stopping
      - {inputValue: early_stopping}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
