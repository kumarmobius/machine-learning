name: model training
inputs:
  - {name: X_data, type: Data, description: "Parquet file for features (DataFrame)"}
  - {name: y_data, type: Data, description: "Parquet file for target (Series or single-column DataFrame) - already encoded"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enbalance, type: String, description: "true/false for class imbalance handling (classification only)", optional: true, default: "true"}
  - {name: target_column, type: String, description: "Optional. Column name inside y parquet if it has multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata from preprocessing (for reference)", optional: true}
  - {name: model_name, type: String, description: "Model to train: knn | decisiontree", optional: true, default: "decisiontree"}
  - {name: n_neighbors, type: Integer, description: "Number of neighbors (for KNN). Used only when model_name=knn", optional: true, default: "5"}
  - {name: dt_max_depth, type: Integer, description: "Max depth for DecisionTree (None for automatic). Used only when model_name=decisiontree", optional: true, default: "5"}
  - {name: random_state, type: Integer, description: "Random seed for reproducibility", optional: true, default: "48"}

outputs:
  - {name: model_pickle, type: Data, description: "Pickled trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}
  - {name: model_config, type: Data, description: "Model configuration JSON (hyperparameters & metadata)"}

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback
        import pandas as pd, numpy as np, joblib
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.preprocessing import LabelEncoder
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")

        def load_parquet_df(path):
            return pd.read_parquet(path, engine="auto")

        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
            try:
                return joblib.load(path)
            except Exception:
                return None

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enbalance', type=str, default="true")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--model_name', type=str, default="decisiontree")
        ap.add_argument('--n_neighbors', type=int, default=5)
        ap.add_argument('--dt_max_depth', type=str, default="")
        ap.add_argument('--random_state', type=int, default=48)
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        ap.add_argument('--model_config', type=str, required=True)
        args = ap.parse_args()

        try:
            print("="*80)
            print("TRAINING STARTED (Model: KNN or DecisionTree)")
            print("="*80)

            X = load_parquet_df(args.X_data)
            print(f"[INFO] X shape: {X.shape}")
            print("[INFO] X.head():")
            print(X.head())

            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    print(f"[INFO] Auto-detected single column: {y_df.columns[0]}")
                else:
                    col = args.target_column.strip()
                    if not col:
                        raise ValueError(f"y parquet has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
                    y = y_df[col].copy()
            else:
                y = pd.Series(y_df).copy()

            # align lengths/indices if mismatch
            if len(X) != len(y):
                print(f"[WARN] Length mismatch: X={len(X)}, y={len(y)}")
                common_idx = X.index.intersection(y.index)
                if len(common_idx) > 0:
                    X = X.loc[common_idx].sort_index()
                    y = y.loc[common_idx].sort_index()
                else:
                    m = min(len(X), len(y))
                    X = X.reset_index(drop=True).iloc[:m, :].copy()
                    y = y.reset_index(drop=True).iloc[:m].copy()

            print("===== TARGET VARIABLE DISTRIBUTION (FULL DATASET) =====")
            try:
                print(y.value_counts(dropna=False).to_string())
            except Exception:
                print("[INFO] Could not print value_counts (non-hashable types).")
            try:
                print("Unique classes:", int(y.nunique()))
            except Exception:
                print("Unique classes: unknown")
            try:
                print("Class distribution percentages:")
                print((y.value_counts(dropna=False, normalize=True) * 100).to_string())
            except Exception:
                pass

            task = args.model_type.strip().lower()
            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1])}
            model_name = args.model_name.strip().lower()
            model_obj = None

            # prepare model config skeleton
            model_config = {
                "model_name": model_name,
                "task": task,
                "random_state": int(args.random_state),
                "params": {}
            }

            if task == "classification":
                print("[INFO] Classification task detected")
                meta = try_load_metadata(args.preprocess_metadata)
                label_mapping = meta.get("label_mapping") if meta else None

                # try integer encoded labels, otherwise fallback to LabelEncoder
                try:
                    y_prepared = pd.Series(y).astype(int)
                    map_info = {"mode": "already_encoded", "label_mapping_reference": label_mapping}
                except Exception:
                    le = LabelEncoder()
                    y_prepared = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
                    map_info = {"mode": "label_encoder_fallback", "classes": le.classes_.tolist()}

                valid_mask = y_prepared.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_prepared = y_prepared[valid_mask].reset_index(drop=True)
                    print(f"[WARN] Dropped {dropped} invalid rows")

                sample_weight = None
                if bool_from_str(args.enbalance):
                    print("[INFO] Computing sample weights for imbalance")
                    try:
                        sample_weight = compute_sample_weight(class_weight='balanced', y=y_prepared)
                    except Exception as e:
                        print(f"[WARN] Could not compute sample weights: {e}")

                if model_name == "knn":
                    print(f"[INFO] Training KNeighborsClassifier (n_neighbors={args.n_neighbors})")
                    clf = KNeighborsClassifier(n_neighbors=int(args.n_neighbors), n_jobs=-1)
                    model_config["params"] = {"n_neighbors": int(args.n_neighbors)}
                    # cross-val
                    try:
                        cv_scores = cross_val_score(clf, X, y_prepared, cv=5, scoring='accuracy', n_jobs=-1)
                        metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                        metrics["cv_accuracy_std"] = float(cv_scores.std())
                        print(f"[INFO] CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                    except Exception as e:
                        print(f"[WARN] CV failed: {e}")
                    clf.fit(X, y_prepared)
                    y_pred = clf.predict(X)
                    metrics.update({
                        "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                        "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "model": "knn",
                        "n_neighbors": int(args.n_neighbors),
                        "balanced": bool_from_str(args.enbalance),
                        "label_info": map_info
                    })
                    model_obj = clf

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    print(f"[INFO] Training DecisionTreeClassifier (max_depth={max_depth})")
                    clf = DecisionTreeClassifier(max_depth=max_depth, class_weight='balanced' if bool_from_str(args.enbalance) else None, random_state=args.random_state)
                    model_config["params"] = {"max_depth": max_depth, "class_weight": 'balanced' if bool_from_str(args.enbalance) else None}
                    try:
                        cv_scores = cross_val_score(clf, X, y_prepared, cv=5, scoring='accuracy', n_jobs=-1)
                        metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                        metrics["cv_accuracy_std"] = float(cv_scores.std())
                        print(f"[INFO] CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                    except Exception as e:
                        print(f"[WARN] CV failed: {e}")
                    clf.fit(X, y_prepared, sample_weight=sample_weight)
                    y_pred = clf.predict(X)
                    metrics.update({
                        "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                        "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                        "model": "decisiontree",
                        "max_depth": max_depth,
                        "balanced": bool_from_str(args.enbalance),
                        "label_info": map_info
                    })
                    model_obj = clf

                else:
                    raise ValueError("Unsupported model_name for classification. Use 'knn' or 'decisiontree'.")

            elif task == "regression":
                print("[INFO] Regression task detected")
                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    print(f"[WARN] Dropping {dropped} rows with non-numeric target")
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)

                if model_name == "knn":
                    print(f"[INFO] Training KNeighborsRegressor (n_neighbors={args.n_neighbors})")
                    reg = KNeighborsRegressor(n_neighbors=int(args.n_neighbors), n_jobs=-1)
                    model_config["params"] = {"n_neighbors": int(args.n_neighbors)}
                    try:
                        cv_scores = cross_val_score(reg, X, y_num, cv=5, scoring='r2', n_jobs=-1)
                        metrics["cv_r2_mean"] = float(cv_scores.mean())
                        metrics["cv_r2_std"] = float(cv_scores.std())
                        print(f"[INFO] CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                    except Exception as e:
                        print(f"[WARN] CV failed: {e}")
                    reg.fit(X, y_num)
                    y_pred = reg.predict(X)
                    metrics.update({
                        "r2_train": float(r2_score(y_num, y_pred)),
                        "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                        "mae_train": float(mean_absolute_error(y_num, y_pred)),
                        "model": "knn",
                        "n_neighbors": int(args.n_neighbors)
                    })
                    model_obj = reg

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    print(f"[INFO] Training DecisionTreeRegressor (max_depth={max_depth})")
                    reg = DecisionTreeRegressor(max_depth=max_depth, random_state=args.random_state)
                    model_config["params"] = {"max_depth": max_depth}
                    try:
                        cv_scores = cross_val_score(reg, X, y_num, cv=5, scoring='r2', n_jobs=-1)
                        metrics["cv_r2_mean"] = float(cv_scores.mean())
                        metrics["cv_r2_std"] = float(cv_scores.std())
                        print(f"[INFO] CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                    except Exception as e:
                        print(f"[WARN] CV failed: {e}")
                    reg.fit(X, y_num)
                    y_pred = reg.predict(X)
                    metrics.update({
                        "r2_train": float(r2_score(y_num, y_pred)),
                        "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                        "mae_train": float(mean_absolute_error(y_num, y_pred)),
                        "model": "decisiontree",
                        "max_depth": max_depth
                    })
                    model_obj = reg

                else:
                    raise ValueError("Unsupported model_name for regression. Use 'knn' or 'decisiontree'.")

            else:
                raise ValueError(f"Unsupported model_type '{args.model_type}'")

            # finalize model_config
            model_config["random_state"] = int(args.random_state)
            model_config["trained_samples"] = int(len(X))
            model_config["trained_features"] = int(X.shape[1])

            # save model, metrics and config
            ensure_dir_for(args.model_pickle)
            joblib.dump(model_obj, args.model_pickle)

            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False)

            print("="*80)
            print("SUCCESS: Training completed")
            print(json.dumps(metrics, indent=2))
            print("="*80)

        except Exception as e:
            print("="*80, file=sys.stderr)
            print("ERROR DURING TRAINING", file=sys.stderr)
            traceback.print_exc()
            print("="*80, file=sys.stderr)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enbalance
      - {inputValue: enbalance}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_name
      - {inputValue: model_name}
      - --n_neighbors
      - {inputValue: n_neighbors}
      - --dt_max_depth
      - {inputValue: dt_max_depth}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
