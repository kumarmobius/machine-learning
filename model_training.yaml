name: model training
inputs:
  - {name: X_data, type: Dataset, description: "Parquet file for features (DataFrame)"}
  - {name: y_data, type: Dataset, description: "Parquet file for target (Series or single-column DataFrame) - already encoded"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enbalance, type: String, description: "true/false for class imbalance handling (classification only)", optional: true, default: "true"}
  - {name: target_column, type: String, description: "Optional. Column name inside y parquet if it has multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata from preprocessing (for reference)", optional: true}
  - {name: model_name, type: String, description: "Model to train: knn | decisiontree | extratrees | svc | linearsvc | nusvc | svr | linearsvr", optional: true, default: "decisiontree"}
  - {name: n_neighbors, type: Integer, description: "Number of neighbors (for KNN). Used only when model_name=knn", optional: true, default: "5"}
  - {name: dt_max_depth, type: Integer, description: "Max depth for DecisionTree (None for automatic). Used only when model_name=decisiontree", optional: true, default: "5"}
  - {name: n_estimators, type: Integer, description: "Number of estimators (for tree ensembles)", optional: true, default: "100"}
  - {name: min_samples_leaf, type: Integer, description: "min_samples_leaf for tree models", optional: true, default: "1"}
  - {name: n_jobs, type: Integer, description: "n_jobs for parallel models (-1 for all cores)", optional: true, default: "-1"}
  - {name: use_scaler, type: String, description: "true/false. If true, apply StandardScaler for SVM/linear models", optional: true, default: "true"}
  - {name: svm_kernel, type: String, description: "kernel for SVC/SVR (rbf|linear|poly|sigmoid)", optional: true, default: "rbf"}
  - {name: svm_C, type: Float, description: "C parameter for SVMs", optional: true, default: "1.0"}
  - {name: svm_gamma, type: String, description: "gamma for SVC/SVR ('scale','auto' or float)", optional: true, default: "scale"}
  - {name: svm_nu, type: Float, description: "nu parameter for NuSVC (0<nu<=1). Used only for nusvc", optional: true, default: "0.5"}
  - {name: svm_probability, type: String, description: "true/false: enable probability=True for SVC (slower)", optional: true, default: "false"}
  - {name: random_state, type: Integer, description: "Random seed for reproducibility", optional: true, default: "48"}

outputs:
  - {name: model_pickle, type: Model, description: "Pickled trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}
  - {name: model_config, type: Data, description: "Model configuration JSON (hyperparameters & metadata)"}

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback
        import pandas as pd, numpy as np, joblib
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        from sklearn.preprocessing import LabelEncoder, StandardScaler
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.pipeline import Pipeline
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")

        def load_parquet_df(path):
            return pd.read_parquet(path, engine="auto")

        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
            try:
                return joblib.load(path)
            except Exception:
                return None

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enbalance', type=str, default="true")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--model_name', type=str, default="decisiontree")
        ap.add_argument('--n_neighbors', type=int, default=5)
        ap.add_argument('--dt_max_depth', type=str, default="")
        ap.add_argument('--n_estimators', type=int, default=100)
        ap.add_argument('--min_samples_leaf', type=int, default=1)
        ap.add_argument('--n_jobs', type=int, default=-1)
        ap.add_argument('--use_scaler', type=str, default="true")
        ap.add_argument('--svm_kernel', type=str, default="rbf")
        ap.add_argument('--svm_C', type=float, default=1.0)
        ap.add_argument('--svm_gamma', type=str, default="scale")
        ap.add_argument('--svm_nu', type=float, default=0.5)
        ap.add_argument('--svm_probability', type=str, default="false")
        ap.add_argument('--random_state', type=int, default=48)
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        ap.add_argument('--model_config', type=str, required=True)
        args = ap.parse_args()

        try:
            print("="*80)
            print("TRAINING STARTED (models: knn | decisiontree | extratrees | svc | linearsvc | nusvc | svr | linearsvr)")
            print("="*80)

            X = load_parquet_df(args.X_data)
            print(f"[INFO] X shape: {X.shape}")
            print("[INFO] X.head():")
            print(X.head())

            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    print(f"[INFO] Auto-detected single column: {y_df.columns[0]}")
                else:
                    col = args.target_column.strip()
                    if not col:
                        raise ValueError(f"y parquet has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
                    y = y_df[col].copy()
            else:
                y = pd.Series(y_df).copy()

            # align lengths/indices if mismatch
            if len(X) != len(y):
                print(f"[WARN] Length mismatch: X={len(X)}, y={len(y)}")
                common_idx = X.index.intersection(y.index)
                if len(common_idx) > 0:
                    X = X.loc[common_idx].sort_index()
                    y = y.loc[common_idx].sort_index()
                else:
                    m = min(len(X), len(y))
                    X = X.reset_index(drop=True).iloc[:m, :].copy()
                    y = y.reset_index(drop=True).iloc[:m].copy()

            print("===== TARGET VARIABLE DISTRIBUTION (FULL DATASET) =====")
            try:
                print(y.value_counts(dropna=False).to_string())
            except Exception:
                print("[INFO] Could not print value_counts (non-hashable types).")
            try:
                print("Unique classes:", int(y.nunique()))
            except Exception:
                print("Unique classes: unknown")
            try:
                print("Class distribution percentages:")
                print((y.value_counts(dropna=False, normalize=True) * 100).to_string())
            except Exception:
                pass

            task = args.model_type.strip().lower()
            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1])}
            model_name = args.model_name.strip().lower()
            model_obj = None

            # prepare model config skeleton
            model_config = {
                "model_name": model_name,
                "task": task,
                "random_state": int(args.random_state),
                "params": {}
            }

            # For classification: try integer encoded labels, otherwise use LabelEncoder fallback
            if task == "classification":
                print("[INFO] Classification task detected")
                meta = try_load_metadata(args.preprocess_metadata)
                label_mapping = meta.get("label_mapping") if meta else None

                try:
                    y_prepared = pd.Series(y).astype(int)
                    map_info = {"mode": "already_encoded", "label_mapping_reference": label_mapping}
                except Exception:
                    le = LabelEncoder()
                    y_prepared = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
                    map_info = {"mode": "label_encoder_fallback", "classes": le.classes_.tolist()}

                valid_mask = y_prepared.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_prepared = y_prepared[valid_mask].reset_index(drop=True)
                    print(f"[WARN] Dropped {dropped} invalid rows")

                sample_weight = None
                if bool_from_str(args.enbalance):
                    print("[INFO] Computing sample weights for imbalance")
                    try:
                        sample_weight = compute_sample_weight(class_weight='balanced', y=y_prepared)
                    except Exception as e:
                        print(f"[WARN] Could not compute sample weights: {e}")

                # Choose and configure model for classification
                if model_name == "knn":
                    print(f"[INFO] Training KNeighborsClassifier (n_neighbors={args.n_neighbors})")
                    clf = KNeighborsClassifier(n_neighbors=int(args.n_neighbors), n_jobs=int(args.n_jobs))
                    model_config["params"] = {"n_neighbors": int(args.n_neighbors)}

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    print(f"[INFO] Training DecisionTreeClassifier (max_depth={max_depth})")
                    clf = DecisionTreeClassifier(max_depth=max_depth, class_weight='balanced' if bool_from_str(args.enbalance) else None, random_state=args.random_state)
                    model_config["params"] = {"max_depth": max_depth, "class_weight": 'balanced' if bool_from_str(args.enbalance) else None}

                elif model_name == "extratrees":
                    print(f"[INFO] Training ExtraTreesClassifier (n_estimators={args.n_estimators}, min_samples_leaf={args.min_samples_leaf})")
                    clf = ExtraTreesClassifier(n_estimators=int(args.n_estimators), min_samples_leaf=int(args.min_samples_leaf), n_jobs=int(args.n_jobs), random_state=args.random_state, class_weight='balanced' if bool_from_str(args.enbalance) else None)
                    model_config["params"] = {"n_estimators": int(args.n_estimators), "min_samples_leaf": int(args.min_samples_leaf), "n_jobs": int(args.n_jobs), "class_weight": 'balanced' if bool_from_str(args.enbalance) else None}

                elif model_name in ("svc", "nusvc", "linearsvc"):
                    svm_prob = bool_from_str(args.svm_probability)
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    nu = float(args.svm_nu)
                    print(f"[INFO] Training SVM classifier ({model_name}) kernel={kernel} C={C} gamma={gamma} probability={svm_prob}")
                    if model_name == "svc":
                        base = SVC(kernel=kernel, C=C, gamma=(None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)), probability=svm_prob, class_weight='balanced' if bool_from_str(args.enbalance) else None, random_state=args.random_state)
                    elif model_name == "nusvc":
                        base = NuSVC(kernel=kernel, nu=nu, C=C, gamma=(None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)), probability=svm_prob, class_weight='balanced' if bool_from_str(args.enbalance) else None, random_state=args.random_state)
                    else:  # linearsvc
                        base = LinearSVC(C=C, max_iter=10000, class_weight='balanced' if bool_from_str(args.enbalance) else None, random_state=args.random_state)
                    # scale if requested
                    if bool_from_str(args.use_scaler):
                        clf = Pipeline([('scaler', StandardScaler()), ('est', base)])
                    else:
                        clf = base
                    model_config["params"] = {"svm_type": model_name, "kernel": kernel, "C": C, "gamma": gamma, "nu": nu, "probability": svm_prob, "use_scaler": bool_from_str(args.use_scaler)}

                elif model_name in ("svr", "linearsvr"):
                    # although regression, user asked for SVM family; keep classification branch safe - user should pass model_type=regression for svr
                    raise ValueError("For SVR/LinearSVR use model_type=regression. Set --model_type regression and model_name svr or linearsvr")

                else:
                    raise ValueError("Unsupported model_name for classification. Use 'knn', 'decisiontree', 'extratrees', 'svc', 'nusvc', or 'linearsvc'.")

                # cross-val attempt
                try:
                    cv_scores = cross_val_score(clf, X, y_prepared, cv=5, scoring='accuracy', n_jobs=int(args.n_jobs))
                    metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                    metrics["cv_accuracy_std"] = float(cv_scores.std())
                    print(f"[INFO] CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                except Exception as e:
                    print(f"[WARN] CV failed: {e}")

                # fit with sample_weight when supported
                try:
                    # pipeline case: estimator is the last step
                    if hasattr(clf, "fit"):
                        # Try to pass sample_weight; many sklearn estimators accept it
                        try:
                            clf.fit(X, y_prepared, sample_weight=sample_weight)
                        except TypeError:
                            clf.fit(X, y_prepared)
                    else:
                        clf.fit(X, y_prepared)
                except Exception as e:
                    print(f"[ERROR] Fit failed: {e}")
                    raise

                y_pred = clf.predict(X)
                metrics.update({
                    "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                    "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "model": model_name,
                    "balanced": bool_from_str(args.enbalance),
                    "label_info": map_info
                })
                model_obj = clf

            elif task == "regression":
                print("[INFO] Regression task detected")
                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    print(f"[WARN] Dropping {dropped} rows with non-numeric target")
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)

                if model_name == "knn":
                    print(f"[INFO] Training KNeighborsRegressor (n_neighbors={args.n_neighbors})")
                    reg = KNeighborsRegressor(n_neighbors=int(args.n_neighbors), n_jobs=int(args.n_jobs))
                    model_config["params"] = {"n_neighbors": int(args.n_neighbors)}

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    print(f"[INFO] Training DecisionTreeRegressor (max_depth={max_depth})")
                    reg = DecisionTreeRegressor(max_depth=max_depth, random_state=args.random_state)
                    model_config["params"] = {"max_depth": max_depth}

                elif model_name == "extratrees":
                    print(f"[INFO] Training ExtraTreesRegressor (n_estimators={args.n_estimators}, min_samples_leaf={args.min_samples_leaf})")
                    reg = ExtraTreesRegressor(n_estimators=int(args.n_estimators), min_samples_leaf=int(args.min_samples_leaf), n_jobs=int(args.n_jobs), random_state=args.random_state)
                    model_config["params"] = {"n_estimators": int(args.n_estimators), "min_samples_leaf": int(args.min_samples_leaf), "n_jobs": int(args.n_jobs)}

                elif model_name in ("svr", "linearsvr"):
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    print(f"[INFO] Training SVM regressor ({model_name}) kernel={kernel} C={C} gamma={gamma}")
                    if model_name == "svr":
                        base = SVR(kernel=kernel, C=C, gamma=(None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)))
                    else:
                        base = LinearSVR(C=C, max_iter=10000)
                    if bool_from_str(args.use_scaler):
                        reg = Pipeline([('scaler', StandardScaler()), ('est', base)])
                    else:
                        reg = base
                    model_config["params"] = {"svm_type": model_name, "kernel": kernel, "C": C, "gamma": gamma, "use_scaler": bool_from_str(args.use_scaler)}

                else:
                    raise ValueError("Unsupported model_name for regression. Use 'knn', 'decisiontree', 'extratrees', 'svr', or 'linearsvr'.")

                # cross-val attempt
                try:
                    cv_scores = cross_val_score(reg, X, y_num, cv=5, scoring='r2', n_jobs=int(args.n_jobs))
                    metrics["cv_r2_mean"] = float(cv_scores.mean())
                    metrics["cv_r2_std"] = float(cv_scores.std())
                    print(f"[INFO] CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                except Exception as e:
                    print(f"[WARN] CV failed: {e}")

                # fit
                try:
                    reg.fit(X, y_num)
                except Exception as e:
                    print(f"[ERROR] Fit failed: {e}")
                    raise

                y_pred = reg.predict(X)
                metrics.update({
                    "r2_train": float(r2_score(y_num, y_pred)),
                    "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                    "mae_train": float(mean_absolute_error(y_num, y_pred)),
                    "model": model_name
                })
                model_obj = reg

            else:
                raise ValueError(f"Unsupported model_type '{args.model_type}'")

            # finalize model_config
            model_config["random_state"] = int(args.random_state)
            model_config["trained_samples"] = int(len(X))
            model_config["trained_features"] = int(X.shape[1])

            # save model, metrics and config
            ensure_dir_for(args.model_pickle)
            joblib.dump(model_obj, args.model_pickle)

            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False)

            print("="*80)
            print("SUCCESS: Training completed")
            print(json.dumps(metrics, indent=2))
            print("="*80)

        except Exception as e:
            print("="*80, file=sys.stderr)
            print("ERROR DURING TRAINING", file=sys.stderr)
            traceback.print_exc()
            print("="*80, file=sys.stderr)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enbalance
      - {inputValue: enbalance}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_name
      - {inputValue: model_name}
      - --n_neighbors
      - {inputValue: n_neighbors}
      - --dt_max_depth
      - {inputValue: dt_max_depth}
      - --n_estimators
      - {inputValue: n_estimators}
      - --min_samples_leaf
      - {inputValue: min_samples_leaf}
      - --n_jobs
      - {inputValue: n_jobs}
      - --use_scaler
      - {inputValue: use_scaler}
      - --svm_kernel
      - {inputValue: svm_kernel}
      - --svm_C
      - {inputValue: svm_C}
      - --svm_gamma
      - {inputValue: svm_gamma}
      - --svm_nu
      - {inputValue: svm_nu}
      - --svm_probability
      - {inputValue: svm_probability}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
