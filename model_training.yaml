name: Supervised Training
inputs:
  - {name: X_data, type: Dataset, description: "Parquet file for features (DataFrame)"}
  - {name: y_data, type: Dataset, description: "Parquet file for target (Series or single-column DataFrame) - already encoded"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enbalance, type: String, description: "true/false for class imbalance handling (classification only)", optional: true, default: "true"}
  - {name: target_column, type: String, description: "Optional. Column name inside y parquet if it has multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata from preprocessing (for reference)", optional: true}
  - {name: model_name, type: String, description: "Model to train: knn | decisiontree | extratrees | svc | linearsvc | nusvc | svr | linearsvr", optional: true, default: "decisiontree"}
  - {name: n_neighbors, type: Integer, description: "Number of neighbors (for KNN). Used only when model_name=knn", optional: true, default: "5"}
  - {name: dt_max_depth, type: Integer, description: "Max depth for DecisionTree (None for automatic). Used only when model_name=decisiontree", optional: true, default: "5"}
  - {name: n_estimators, type: Integer, description: "Number of estimators (for tree ensembles)", optional: true, default: "100"}
  - {name: min_samples_leaf, type: Integer, description: "min_samples_leaf for tree models", optional: true, default: "1"}
  - {name: n_jobs, type: Integer, description: "n_jobs for parallel models (-1 for all cores)", optional: true, default: "-1"}
  - {name: use_scaler, type: String, description: "true/false. If true, apply StandardScaler for SVM/linear models", optional: true, default: "true"}
  - {name: svm_kernel, type: String, description: "kernel for SVC/SVR (rbf|linear|poly|sigmoid)", optional: true, default: "rbf"}
  - {name: svm_C, type: Float, description: "C parameter for SVMs", optional: true, default: "1.0"}
  - {name: svm_gamma, type: String, description: "gamma for SVC/SVR ('scale','auto' or float)", optional: true, default: "scale"}
  - {name: svm_nu, type: Float, description: "nu parameter for NuSVC (0<nu<=1). Used only for nusvc", optional: true, default: "0.5"}
  - {name: svm_probability, type: String, description: "true/false: enable probability=True for SVC (slower)", optional: true, default: "false"}
  - {name: random_state, type: Integer, description: "Random seed for reproducibility", optional: true, default: "48"}

outputs:
  - {name: model_pickle, type: Model, description: "Pickled trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}
  - {name: model_config, type: Data, description: "Model configuration JSON (hyperparameters & metadata)"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, logging, time
        import pandas as pd, numpy as np
        import cloudpickle
        from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        from sklearn.preprocessing import LabelEncoder, StandardScaler
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.pipeline import Pipeline
        from sklearn.svm import SVC, LinearSVC, NuSVC, SVR, LinearSVR

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        logger = logging.getLogger(__name__)

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")

        def load_parquet_df(path):
            logger.info(f"Loading parquet file: {path}")
            return pd.read_csv(path)

        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load metadata as JSON from {path}: {e}")
            try:
                import joblib
                return joblib.load(path)
            except Exception as e:
                logger.warning(f"Failed to load metadata with joblib from {path}: {e}")
            return None

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enbalance', type=str, default="true")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--model_name', type=str, default="decisiontree")
        ap.add_argument('--n_neighbors', type=int, default=5)
        ap.add_argument('--dt_max_depth', type=str, default="")
        ap.add_argument('--n_estimators', type=int, default=100)
        ap.add_argument('--min_samples_leaf', type=int, default=1)
        ap.add_argument('--n_jobs', type=int, default=-1)
        ap.add_argument('--use_scaler', type=str, default="true")
        ap.add_argument('--svm_kernel', type=str, default="rbf")
        ap.add_argument('--svm_C', type=float, default=1.0)
        ap.add_argument('--svm_gamma', type=str, default="scale")
        ap.add_argument('--svm_nu', type=float, default=0.5)
        ap.add_argument('--svm_probability', type=str, default="false")
        ap.add_argument('--random_state', type=int, default=48)
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        ap.add_argument('--model_config', type=str, required=True)
        args = ap.parse_args()

        start_time = time.time()
        
        try:
            logger.info("="*80)
            logger.info("TRAINING STARTED")
            logger.info(f"Models supported: knn | decisiontree | extratrees | svc | linearsvc | nusvc | svr | linearsvr")
            logger.info("="*80)

            X = load_parquet_df(args.X_data)
            logger.info(f"X shape: {X.shape}")
            logger.info(f"X columns: {list(X.columns)}")

            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    logger.info(f"Auto-detected single column: {y_df.columns[0]}")
                else:
                    col = args.target_column.strip()
                    if not col:
                        raise ValueError(f"y parquet has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
                    y = y_df[col].copy()
            else:
                y = pd.Series(y_df).copy()

            # align lengths/indices if mismatch
            if len(X) != len(y):
                logger.warning(f"Length mismatch: X={len(X)}, y={len(y)}")
                common_idx = X.index.intersection(y.index)
                if len(common_idx) > 0:
                    X = X.loc[common_idx].sort_index()
                    y = y.loc[common_idx].sort_index()
                else:
                    m = min(len(X), len(y))
                    X = X.reset_index(drop=True).iloc[:m, :].copy()
                    y = y.reset_index(drop=True).iloc[:m].copy()

            logger.info("TARGET VARIABLE DISTRIBUTION (FULL DATASET)")
            try:
                value_counts = y.value_counts(dropna=False)
                logger.info(f"Value counts:{value_counts.to_string()}")
            except Exception as e:
                logger.warning(f"Could not print value_counts: {e}")
            
            try:
                logger.info(f"Unique classes: {int(y.nunique())}")
            except Exception:
                logger.info("Unique classes: unknown")
            
            try:
                dist_pct = y.value_counts(dropna=False, normalize=True) * 100
                logger.info(f"Class distribution percentages:{dist_pct.to_string()}")
            except Exception as e:
                logger.warning(f"Could not print distribution percentages: {e}")

            task = args.model_type.strip().lower()
            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1])}
            model_name = args.model_name.strip().lower()
            model_obj = None

            # Prepare comprehensive model config with all training parameters
            model_config = {
                "model_name": model_name,
                "task": task,
                "training_parameters": {
                    "model_type": args.model_type,
                    "enbalance": args.enbalance,
                    "target_column": args.target_column,
                    "model_name": args.model_name,
                    "n_neighbors": args.n_neighbors,
                    "dt_max_depth": args.dt_max_depth,
                    "n_estimators": args.n_estimators,
                    "min_samples_leaf": args.min_samples_leaf,
                    "n_jobs": args.n_jobs,
                    "use_scaler": args.use_scaler,
                    "svm_kernel": args.svm_kernel,
                    "svm_C": args.svm_C,
                    "svm_gamma": args.svm_gamma,
                    "svm_nu": args.svm_nu,
                    "svm_probability": args.svm_probability,
                    "random_state": args.random_state
                },
                "model_specific_params": {},
                "data_info": {
                    "samples": int(len(y)),
                    "features": int(X.shape[1]),
                    "target_shape": y.shape
                },
                "training_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }

            # For classification: try integer encoded labels, otherwise use LabelEncoder fallback
            if task == "classification":
                logger.info("Classification task detected")
                meta = try_load_metadata(args.preprocess_metadata)
                label_mapping = meta.get("label_mapping") if meta else None

                try:
                    y_prepared = pd.Series(y).astype(int)
                    map_info = {"mode": "already_encoded", "label_mapping_reference": label_mapping}
                    logger.info("Target already encoded as integers")
                except Exception as e:
                    logger.info(f"Encoding target with LabelEncoder: {e}")
                    le = LabelEncoder()
                    y_prepared = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
                    map_info = {"mode": "label_encoder_fallback", "classes": le.classes_.tolist()}

                valid_mask = y_prepared.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_prepared = y_prepared[valid_mask].reset_index(drop=True)
                    logger.warning(f"Dropped {dropped} invalid rows")

                sample_weight = None
                if bool_from_str(args.enbalance):
                    logger.info("Computing sample weights for class imbalance handling")
                    try:
                        sample_weight = compute_sample_weight(class_weight='balanced', y=y_prepared)
                    except Exception as e:
                        logger.warning(f"Could not compute sample weights: {e}")

                # Choose and configure model for classification
                if model_name == "knn":
                    logger.info(f"Training KNeighborsClassifier (n_neighbors={args.n_neighbors})")
                    clf = KNeighborsClassifier(n_neighbors=int(args.n_neighbors), n_jobs=int(args.n_jobs))
                    model_config["model_specific_params"] = {
                        "n_neighbors": int(args.n_neighbors),
                        "n_jobs": int(args.n_jobs),
                        "algorithm": "auto"
                    }

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    logger.info(f"Training DecisionTreeClassifier (max_depth={max_depth})")
                    class_weight = 'balanced' if bool_from_str(args.enbalance) else None
                    clf = DecisionTreeClassifier(
                        max_depth=max_depth, 
                        class_weight=class_weight, 
                        random_state=args.random_state,
                        min_samples_leaf=int(args.min_samples_leaf)
                    )
                    model_config["model_specific_params"] = {
                        "max_depth": max_depth,
                        "class_weight": class_weight,
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "criterion": "gini",
                        "splitter": "best"
                    }

                elif model_name == "extratrees":
                    logger.info(f"Training ExtraTreesClassifier (n_estimators={args.n_estimators}, min_samples_leaf={args.min_samples_leaf})")
                    class_weight = 'balanced' if bool_from_str(args.enbalance) else None
                    clf = ExtraTreesClassifier(
                        n_estimators=int(args.n_estimators), 
                        min_samples_leaf=int(args.min_samples_leaf), 
                        n_jobs=int(args.n_jobs), 
                        random_state=args.random_state, 
                        class_weight=class_weight
                    )
                    model_config["model_specific_params"] = {
                        "n_estimators": int(args.n_estimators),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "n_jobs": int(args.n_jobs),
                        "class_weight": class_weight,
                        "criterion": "gini",
                        "bootstrap": True
                    }

                elif model_name in ("svc", "nusvc", "linearsvc"):
                    svm_prob = bool_from_str(args.svm_probability)
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    nu = float(args.svm_nu)
                    use_scaler = bool_from_str(args.use_scaler)
                    class_weight = 'balanced' if bool_from_str(args.enbalance) else None
                    
                    logger.info(f"Training SVM classifier ({model_name}) kernel={kernel} C={C} gamma={gamma} probability={svm_prob} use_scaler={use_scaler}")
                    
                    if model_name == "svc":
                        gamma_val = None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        base = SVC(
                            kernel=kernel, 
                            C=C, 
                            gamma=gamma_val, 
                            probability=svm_prob, 
                            class_weight=class_weight, 
                            random_state=args.random_state
                        )
                    elif model_name == "nusvc":
                        gamma_val = None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        base = NuSVC(
                            kernel=kernel, 
                            nu=nu, 
                            C=C, 
                            gamma=gamma_val, 
                            probability=svm_prob, 
                            class_weight=class_weight, 
                            random_state=args.random_state
                        )
                    else:  # linearsvc
                        base = LinearSVC(
                            C=C, 
                            max_iter=10000, 
                            class_weight=class_weight, 
                            random_state=args.random_state
                        )
                    
                    # scale if requested
                    if use_scaler:
                        clf = Pipeline([('scaler', StandardScaler()), ('est', base)])
                    else:
                        clf = base
                    
                    model_config["model_specific_params"] = {
                        "svm_type": model_name,
                        "kernel": kernel,
                        "C": C,
                        "gamma": gamma,
                        "nu": nu,
                        "probability": svm_prob,
                        "use_scaler": use_scaler,
                        "class_weight": class_weight,
                        "max_iter": 10000 if model_name == "linearsvc" else -1
                    }

                elif model_name in ("svr", "linearsvr"):
                    raise ValueError("For SVR/LinearSVR use model_type=regression. Set --model_type regression and model_name svr or linearsvr")

                else:
                    raise ValueError("Unsupported model_name for classification. Use 'knn', 'decisiontree', 'extratrees', 'svc', 'nusvc', or 'linearsvc'.")

                # cross-val attempt
                try:
                    logger.info("Performing 5-fold cross validation")
                    cv_scores = cross_val_score(clf, X, y_prepared, cv=5, scoring='accuracy', n_jobs=int(args.n_jobs))
                    metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                    metrics["cv_accuracy_std"] = float(cv_scores.std())
                    logger.info(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                except Exception as e:
                    logger.warning(f"Cross-validation failed: {e}")

                # fit with sample_weight when supported
                try:
                    logger.info("Fitting model to training data")
                    fit_start = time.time()
                    
                    # Try to pass sample_weight; many sklearn estimators accept it
                    try:
                        clf.fit(X, y_prepared, sample_weight=sample_weight)
                    except TypeError:
                        clf.fit(X, y_prepared)
                    
                    fit_time = time.time() - fit_start
                    logger.info(f"Model fitting completed in {fit_time:.2f} seconds")
                except Exception as e:
                    logger.error(f"Model fitting failed: {e}")
                    raise

                y_pred = clf.predict(X)
                metrics.update({
                    "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                    "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "model": model_name,
                    "balanced": bool_from_str(args.enbalance),
                    "label_info": map_info,
                    "training_time_seconds": fit_time
                })
                model_obj = clf

            elif task == "regression":
                logger.info("Regression task detected")
                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    logger.warning(f"Dropping {dropped} rows with non-numeric target")
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)

                if model_name == "knn":
                    logger.info(f"Training KNeighborsRegressor (n_neighbors={args.n_neighbors})")
                    reg = KNeighborsRegressor(n_neighbors=int(args.n_neighbors), n_jobs=int(args.n_jobs))
                    model_config["model_specific_params"] = {
                        "n_neighbors": int(args.n_neighbors),
                        "n_jobs": int(args.n_jobs),
                        "algorithm": "auto",
                        "weights": "uniform"
                    }

                elif model_name == "decisiontree":
                    max_depth = None if args.dt_max_depth.strip() in ("", "none", "null") else int(args.dt_max_depth)
                    logger.info(f"Training DecisionTreeRegressor (max_depth={max_depth})")
                    reg = DecisionTreeRegressor(
                        max_depth=max_depth, 
                        random_state=args.random_state,
                        min_samples_leaf=int(args.min_samples_leaf)
                    )
                    model_config["model_specific_params"] = {
                        "max_depth": max_depth,
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "criterion": "squared_error",
                        "splitter": "best"
                    }

                elif model_name == "extratrees":
                    logger.info(f"Training ExtraTreesRegressor (n_estimators={args.n_estimators}, min_samples_leaf={args.min_samples_leaf})")
                    reg = ExtraTreesRegressor(
                        n_estimators=int(args.n_estimators), 
                        min_samples_leaf=int(args.min_samples_leaf), 
                        n_jobs=int(args.n_jobs), 
                        random_state=args.random_state
                    )
                    model_config["model_specific_params"] = {
                        "n_estimators": int(args.n_estimators),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "n_jobs": int(args.n_jobs),
                        "criterion": "squared_error",
                        "bootstrap": True
                    }

                elif model_name in ("svr", "linearsvr"):
                    kernel = args.svm_kernel
                    C = float(args.svm_C)
                    gamma = args.svm_gamma
                    use_scaler = bool_from_str(args.use_scaler)
                    
                    logger.info(f"Training SVM regressor ({model_name}) kernel={kernel} C={C} gamma={gamma} use_scaler={use_scaler}")
                    
                    if model_name == "svr":
                        gamma_val = None if gamma in ("", "none") else (float(gamma) if gamma not in ("scale","auto") else gamma)
                        base = SVR(kernel=kernel, C=C, gamma=gamma_val)
                    else:
                        base = LinearSVR(C=C, max_iter=10000, random_state=args.random_state)
                    
                    if use_scaler:
                        reg = Pipeline([('scaler', StandardScaler()), ('est', base)])
                    else:
                        reg = base
                    
                    model_config["model_specific_params"] = {
                        "svm_type": model_name,
                        "kernel": kernel,
                        "C": C,
                        "gamma": gamma,
                        "use_scaler": use_scaler,
                        "max_iter": 10000
                    }

                else:
                    raise ValueError("Unsupported model_name for regression. Use 'knn', 'decisiontree', 'extratrees', 'svr', or 'linearsvr'.")

                # cross-val attempt
                try:
                    logger.info("Performing 5-fold cross validation")
                    cv_scores = cross_val_score(reg, X, y_num, cv=5, scoring='r2', n_jobs=int(args.n_jobs))
                    metrics["cv_r2_mean"] = float(cv_scores.mean())
                    metrics["cv_r2_std"] = float(cv_scores.std())
                    logger.info(f"CV R2: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                except Exception as e:
                    logger.warning(f"Cross-validation failed: {e}")

                # fit
                try:
                    logger.info("Fitting model to training data")
                    fit_start = time.time()
                    reg.fit(X, y_num)
                    fit_time = time.time() - fit_start
                    logger.info(f"Model fitting completed in {fit_time:.2f} seconds")
                except Exception as e:
                    logger.error(f"Model fitting failed: {e}")
                    raise

                y_pred = reg.predict(X)
                metrics.update({
                    "r2_train": float(r2_score(y_num, y_pred)),
                    "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                    "mae_train": float(mean_absolute_error(y_num, y_pred)),
                    "model": model_name,
                    "training_time_seconds": fit_time
                })
                model_obj = reg

            else:
                raise ValueError(f"Unsupported model_type '{args.model_type}'")

            # finalize model_config with additional info
            model_config["random_state"] = int(args.random_state)
            model_config["trained_samples"] = int(len(X))
            model_config["trained_features"] = int(X.shape[1])
            model_config["feature_names"] = X.columns.tolist()
            model_config["training_time_seconds"] = time.time() - start_time

            # Save model using cloudpickle
            logger.info(f"Saving model to {args.model_pickle}")
            ensure_dir_for(args.model_pickle)
            with open(args.model_pickle, 'wb') as f:
                cloudpickle.dump(model_obj, f)

            # Save metrics
            logger.info(f"Saving metrics to {args.metrics_json}")
            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            # Save comprehensive model config
            logger.info(f"Saving model configuration to {args.model_config}")
            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False)

            total_time = time.time() - start_time
            logger.info("="*80)
            logger.info(f"SUCCESS: Training completed in {total_time:.2f} seconds")
            logger.info(f"Model saved: {args.model_pickle}")
            logger.info(f"Metrics saved: {args.metrics_json}")
            logger.info(f"Config saved: {args.model_config}")
            logger.info("="*80)

        except Exception as e:
            logger.error("ERROR DURING TRAINING")
            logger.error(traceback.format_exc())
            logger.error("="*80)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enbalance
      - {inputValue: enbalance}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_name
      - {inputValue: model_name}
      - --n_neighbors
      - {inputValue: n_neighbors}
      - --dt_max_depth
      - {inputValue: dt_max_depth}
      - --n_estimators
      - {inputValue: n_estimators}
      - --min_samples_leaf
      - {inputValue: min_samples_leaf}
      - --n_jobs
      - {inputValue: n_jobs}
      - --use_scaler
      - {inputValue: use_scaler}
      - --svm_kernel
      - {inputValue: svm_kernel}
      - --svm_C
      - {inputValue: svm_C}
      - --svm_gamma
      - {inputValue: svm_gamma}
      - --svm_nu
      - {inputValue: svm_nu}
      - --svm_probability
      - {inputValue: svm_probability}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
