name: Train RandomForest v1.3
description: |
  Reads X and y from parquet, trains RandomForestClassifier for classification or RandomForestRegressor for regression.
  Assumes y is already encoded from preprocessing (no re-encoding needed).
  Supports class imbalance when enbalance=true for classification.
  Prints target variable distribution before training.
  Saves trained model and training metrics.

inputs:
  - {name: X_data, type: Data, description: "Parquet file for features (DataFrame)"}
  - {name: y_data, type: Data, description: "Parquet file for target (Series or single-column DataFrame) - already encoded"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enbalance, type: String, description: "true/false for class imbalance handling (classification only)", optional: true, default: "true"}
  - {name: target_column, type: String, description: "Optional. Column name inside y parquet if it has multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata from preprocessing (for reference)", optional: true}
  - {name: n_estimators, type: Integer, description: "Number of trees in the RandomForest", optional: true, default: "100"}
  - {name: max_depth, type: Integer, description: "Maximum depth of trees (None for unlimited)", optional: true, default: "20"}
  - {name: random_state, type: Integer, description: "Random seed for reproducibility", optional: true, default: "48"}

outputs:
  - {name: model_pickle, type: Data, description: "Pickled trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback
        import pandas as pd, numpy as np
        import joblib
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        from sklearn.preprocessing import LabelEncoder
        from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")

        def load_parquet_df(path):
            return pd.read_parquet(path, engine="auto")

        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
            try:
                return joblib.load(path)
            except Exception:
                return None

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enbalance', type=str, default="true")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--n_estimators', type=int, default=100)
        ap.add_argument('--max_depth', type=str, default="")
        ap.add_argument('--random_state', type=int, default=42)
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        args = ap.parse_args()

        try:
            print("="*80)
            print("TRAINING STARTED (RandomForest)")
            print("="*80)
            
            X = load_parquet_df(args.X_data)
            print(f"[INFO] X shape: {X.shape}")

            y_df = load_parquet_df(args.y_data)
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    print(f"[INFO] Auto-detected single column: {y_df.columns[0]}")
                else:
                    col = args.target_column.strip()
                    if not col:
                        raise ValueError(f"y parquet has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
                    y = y_df[col].copy()
            else:
                y = pd.Series(y_df).copy()

            if len(X) != len(y):
                print(f"[WARN] Length mismatch: X={len(X)}, y={len(y)}")
                common_idx = X.index.intersection(y.index)
                if len(common_idx) > 0:
                    X = X.loc[common_idx].sort_index()
                    y = y.loc[common_idx].sort_index()
                else:
                    m = min(len(X), len(y))
                    X = X.reset_index(drop=True).iloc[:m, :]
                    y = y.reset_index(drop=True).iloc[:m]

            # === Print target variable distribution ===
            print("===== TARGET VARIABLE DISTRIBUTION (FULL DATASET) =====")
            print(y.value_counts(dropna=False).to_string())
            print("Unique classes:", y.nunique())
            print("Class distribution percentages:")
            print((y.value_counts(dropna=False, normalize=True) * 100).to_string())

            task = args.model_type.strip().lower()
            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1])}

            max_depth = None if args.max_depth.strip() in ("", "none", "null") else int(args.max_depth)

            if task == "classification":
                print("[INFO] Classification task detected")
                meta = try_load_metadata(args.preprocess_metadata)
                label_mapping = meta.get("label_mapping") if meta else None

                try:
                    y_prepared = pd.Series(y).astype(int)
                    map_info = {"mode": "already_encoded", "label_mapping_reference": label_mapping}
                except Exception:
                    le = LabelEncoder()
                    y_prepared = pd.Series(le.fit_transform(y.astype(str)), index=y.index)
                    map_info = {"mode": "label_encoder_fallback", "classes": le.classes_.tolist()}

                valid_mask = y_prepared.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_prepared = y_prepared[valid_mask].reset_index(drop=True)
                    print(f"[WARN] Dropped {dropped} invalid rows")

                sample_weight = None
                if bool_from_str(args.enbalance):
                    print("[INFO] Computing sample weights for imbalance")
                    try:
                        sample_weight = compute_sample_weight(class_weight='balanced', y=y_prepared)
                    except Exception as e:
                        print(f"[WARN] Could not compute sample weights: {e}")

                print(f"[INFO] Training RandomForestClassifier with {args.n_estimators} trees")
                clf = RandomForestClassifier(
                    n_estimators=int(args.n_estimators),
                    max_depth=max_depth,
                    random_state=int(args.random_state),
                    n_jobs=-1,
                    class_weight='balanced' if bool_from_str(args.enbalance) else None,
                    oob_score=True,
                    bootstrap=True,
                    max_features='sqrt',
                    min_samples_split=20,
                    min_samples_leaf=10    
                )

                print("[INFO] Running 5-fold cross-validation to check for overfitting...")
                cv_scores = cross_val_score(clf, X, y_prepared, cv=5, scoring='accuracy', n_jobs=-1)
                print(f"[INFO] CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
                metrics["cv_accuracy_mean"] = float(cv_scores.mean())
                metrics["cv_accuracy_std"] = float(cv_scores.std())
                clf.fit(X, y_prepared, sample_weight=sample_weight)

                y_pred = clf.predict(X)
                metrics.update({
                    "accuracy_train": float(accuracy_score(y_prepared, y_pred)),
                    "precision_train": float(precision_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "recall_train": float(recall_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "f1_train": float(f1_score(y_prepared, y_pred, average='weighted', zero_division=0)),
                    "n_estimators": int(args.n_estimators),
                    "max_depth": max_depth,
                    "balanced": bool_from_str(args.enbalance),
                    "label_info": map_info
                })

                model_obj = clf

            elif task == "regression":
                print("[INFO] Regression task detected")

                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    dropped = int((~valid_mask).sum())
                    print(f"[WARN] Dropping {dropped} rows with non-numeric target")
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)

                print(f"[INFO] Training RandomForestRegressor with {args.n_estimators} trees")
                reg = RandomForestRegressor(
                    n_estimators=int(args.n_estimators),
                    max_depth=max_depth,
                    random_state=int(args.random_state),
                    n_jobs=-1
                )
                reg.fit(X, y_num)

                y_pred = reg.predict(X)
                metrics.update({
                    "r2_train": float(r2_score(y_num, y_pred)),
                    "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                    "mae_train": float(mean_absolute_error(y_num, y_pred)),
                    "n_estimators": int(args.n_estimators),
                    "max_depth": max_depth
                })

                model_obj = reg

            else:
                raise ValueError(f"Unsupported model_type '{args.model_type}'")

            ensure_dir_for(args.model_pickle)
            joblib.dump(model_obj, args.model_pickle)

            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            print("="*80)
            print("SUCCESS: RandomForest training completed")
            print(json.dumps(metrics, indent=2))
            print("="*80)

        except Exception as e:
            print("="*80, file=sys.stderr)
            print("ERROR DURING TRAINING", file=sys.stderr)
            traceback.print_exc()
            print("="*80, file=sys.stderr)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enbalance
      - {inputValue: enbalance}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --n_estimators
      - {inputValue: n_estimators}
      - --max_depth
      - {inputValue: max_depth}
      - --random_state
      - {inputValue: random_state}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
