name: Load & Preprocess data
description: |
  Loads a dataset file (CSV/JSON/Excel/Parquet/Feather/ORC) produced by the CDN Generic Downloader,
  removes duplicates, drops rows with missing target, performs advanced alphanumeric numeric detection
  & conversion, identifies datetime columns and extracts common date features, optimizes numeric dtypes
  to the smallest safe dtype (int8/int16/float32/etc.), and returns the feature matrix X and target y
  as parquet files. This pipeline DOES NOT impute, scale, or perform outlier handling.
inputs:
  - {name: in_file, type: Data, description: "Path to the downloaded file (connect CDN Generic Downloader: out_file)"}
  - {name: target_column, type: String, description: "Name of the target column in the dataset (rows with null target will be dropped)"}
  - {name: numeric_detection_threshold, type: Float, description: "Minimum fraction (0-1) of parsable numeric values in an object column to coerce it to numeric", optional: true, default: "0.6"}
  - {name: advanced_alphanumeric, type: String, description: "Enable advanced alphanumeric parsing (true/false). Parses units, multipliers (k/M), percents, currencies", optional: true, default: "true"}
  - {name: date_parse, type: String, description: "Enable date parsing & extraction (true/false). Attempts to parse object columns as datetimes and extract features", optional: true, default: "true"}
  - {name: optimize_dtypes, type: String, description: "Optimize numeric dtypes after cleaning (true/false)", optional: true, default: "true"}
outputs:
  - {name: X, type: Data, description: "Feature matrix (parquet) after preprocessing"}
  - {name: y, type: Data, description: "Target column (parquet) after preprocessing"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile
        from datetime import datetime

        try:
            import pandas as pd, numpy as np
        except Exception:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input", "pandas", "numpy", "pyarrow", "fastparquet", "openpyxl"])
            import pandas as pd, numpy as np

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh:
                    return fh.read(n)
            except Exception:
                return b""

        def check_if_json_content(sample_bytes):
            if not sample_bytes:
                return False
            try:
                txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception:
                return False
            
            if len(txt) == 0:
                return False
            
            first_char = txt[0]
            if first_char in (chr(123), chr(91)):
                return True
            
            newline_open_brace = chr(10) + chr(123)
            newline_open_bracket = chr(10) + chr(91)
            if newline_open_brace in txt or newline_open_bracket in txt:
                return True
            
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries = [os.path.join(path, f) for f in os.listdir(path) if not f.startswith(".")]
                if not entries:
                    raise ValueError("Input directory is empty: " + path)
                
                files = [p for p in entries if os.path.isfile(p)]
                if not files:
                    raise ValueError("No regular files inside directory: " + path)
                
                if len(files) == 1:
                    path = files[0]
                else:
                    path = max(files, key=lambda p: os.path.getsize(p))
                print("Info: in_file was a directory - selected candidate file: " + path)

            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found or not a file: " + path)

            ext = os.path.splitext(path)[1].lower()

            if ext in (".gz",) or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path, "rt", encoding="utf-8", errors="ignore") as fh:
                        sample = fh.read(8192)
                        fh.seek(0)
                        if check_if_json_content(sample.encode() if isinstance(sample, str) else sample):
                            fh.seek(0)
                            try:
                                return pd.read_json(fh, lines=True)
                            except Exception:
                                fh.seek(0)
                                return pd.read_csv(fh)
                        else:
                            fh.seek(0)
                            return pd.read_csv(fh)
                except Exception as e:
                    pass

            if ext == ".zip":
                with zipfile.ZipFile(path, "r") as z:
                    names = [n for n in z.namelist() if not n.endswith("/")]
                    if not names:
                        raise ValueError("No files inside zip archive: " + path)
                    member = max(names, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample = fh.read(8192)
                        if check_if_json_content(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2, encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2, encoding="utf-8"))

            try:
                if ext == ".csv":
                    return pd.read_csv(path)
                if ext in (".tsv", ".tab"):
                    return pd.read_csv(path, sep=chr(9))
                if ext in (".json", ".ndjson", ".jsonl"):
                    try:
                        return pd.read_json(path, lines=True)
                    except ValueError:
                        return pd.read_json(path)
                if ext in (".xls", ".xlsx"):
                    return pd.read_excel(path)
                if ext in (".parquet", ".pq"):
                    return pd.read_parquet(path, engine="auto")
                if ext == ".feather":
                    return pd.read_feather(path)
                if ext == ".orc":
                    return pd.read_orc(path)
            except Exception:
                pass

            sample = sample_file_bytes(path, n=8192)
            
            try:
                return pd.read_parquet(path, engine="auto")
            except Exception:
                pass
            
            if check_if_json_content(sample):
                try:
                    return pd.read_json(path, lines=True)
                except Exception:
                    pass
            
            try:
                return pd.read_csv(path)
            except Exception:
                pass

            raise ValueError("Unsupported or unreadable file format for automatic loading: " + path)

        MULTIPLIER_MAP = dict()
        MULTIPLIER_MAP['k'] = 1000.0
        MULTIPLIER_MAP['m'] = 1000000.0
        MULTIPLIER_MAP['b'] = 1000000000.0
        MULTIPLIER_MAP['t'] = 1000000000000.0

        UNIT_SUFFIXES = [
            'km','m','cm','mm','kg','g','lbs','lb','oz','ft','feet','in','inch','mi','sqm','sqft','sq m','sqft',
            'pcs','pieces','units','hr','hrs','h','min','s'
        ]

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺₪¥]'
        TOKEN_RE = re.compile(
            r'^\s*'
            r'(?P<sign>[-+]?)'
            r'(?P<number>(?:\d' + chr(123) + r'1,3' + chr(125) + r'(?:[,\s]\d' + chr(123) + r'3' + chr(125) + r')+|\d+)(?:[.,]\d+)?)'
            r'\s*'
            r'(?P<mult>[kKmMbBtT])?'
            r'\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$'
        )

        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s):
                return np.nan
            orig = str(s).strip()
            if orig == '' or orig.lower() in ('nan','none','null','na'):
                return np.nan

            tmp = re.sub(CURRENCY_SYMBOLS_REGEX, '', orig)

            if tmp.strip().endswith('%'):
                try:
                    num = float(tmp.strip().rstrip('%').replace(',', '').replace(' ', ''))
                    return num / 100.0
                except Exception:
                    pass

            tmp = tmp.replace(chr(8722), '-').replace(chr(8211), '-').replace(chr(8212), '-')

            tmp = re.sub(r'[\u00A0\u202F]', '', tmp)
            tmp = tmp.strip()

            if decimal_comma:
                tmp = tmp.replace('.', '').replace(',', '.')
            else:
                tmp = tmp.replace(',', '').replace(' ', '')

            try:
                return float(tmp)
            except Exception:
                pass

            m = TOKEN_RE.match(tmp)
            if m:
                number = m.group('number')
                mult = m.group('mult')
                unit = (m.group('unit') or '').lower()
                number_clean = number.replace(',', '').replace(' ', '')
                try:
                    val = float(number_clean)
                except Exception:
                    try:
                        val = float(number_clean.replace(',', '.'))
                    except Exception:
                        return np.nan
                if mult:
                    mul = MULTIPLIER_MAP.get(mult.lower(), 1.0)
                    val = val * mul
                if unit and '%' in unit:
                    val = val / 100.0
                return val

            num_search = re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if num_search:
                try:
                    candidate = num_search.group(0).replace(',', '')
                    return float(candidate)
                except Exception:
                    return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df = df.copy()
            report_data = dict()
            report_data['converted'] = []
            report_data['skipped'] = []
            obj_cols = [c for c in df.columns if (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                ser = df[col].astype(object)
                total_non_null = ser.notna().sum()
                if total_non_null == 0:
                    report_data['skipped'].append(col)
                    continue

                if advanced:
                    parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma))
                else:
                    parsed = pd.to_numeric(ser, errors='coerce')
                
                parsable = parsed.notna().sum()
                frac = parsable / float(total_non_null)
                if frac >= detect_threshold:
                    df[col + "_orig"] = df[col]
                    df[col] = parsed
                    conv_entry = dict()
                    conv_entry['col'] = col
                    conv_entry['parsable_fraction'] = frac
                    report_data['converted'].append(conv_entry)
                else:
                    skip_entry = dict()
                    skip_entry['col'] = col
                    skip_entry['parsable_fraction'] = frac
                    report_data['skipped'].append(skip_entry)
            return df, report_data

        def detect_and_extract_dates(df, date_parse_enabled=True):
            df = df.copy()
            report_data = dict()
            report_data['date_columns'] = []
            report_data['skipped'] = []
            if not date_parse_enabled:
                return df, report_data
            candidate_cols = [c for c in df.columns if (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
            for col in candidate_cols:
                ser = df[col].astype(object)
                sample = ser.dropna().astype(str).head(500)
                if sample.empty:
                    report_data['skipped'].append(col)
                    continue
                parsed = pd.to_datetime(sample, errors='coerce', infer_datetime_format=True, utc=False)
                parsable_frac = parsed.notna().mean()
                if parsable_frac >= 0.6:
                    full_parsed = pd.to_datetime(ser, errors='coerce', infer_datetime_format=True, utc=False)
                    df[col + "_orig"] = df[col]
                    df[col] = full_parsed
                    df[col + "_year"] = df[col].dt.year
                    df[col + "_month"] = df[col].dt.month
                    df[col + "_day"] = df[col].dt.day
                    df[col + "_dayofweek"] = df[col].dt.dayofweek
                    df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col + "_is_month_start"] = df[col].dt.is_month_start.astype('Int64')
                    df[col + "_is_month_end"] = df[col].dt.is_month_end.astype('Int64')
                    df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                    date_entry = dict()
                    date_entry['col'] = col
                    date_entry['parsable_fraction'] = parsable_frac
                    report_data['date_columns'].append(date_entry)
                else:
                    skip_entry = dict()
                    skip_entry['col'] = col
                    skip_entry['parsable_fraction'] = parsable_frac
                    report_data['skipped'].append(skip_entry)
            return df, report_data

        def optimize_numeric_dtypes(df):
            df_optimized = df.copy()
            start_mem = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024

            for col in df_optimized.select_dtypes(include=[np.number]).columns:
                col_min = df_optimized[col].min()
                col_max = df_optimized[col].max()
                col_dtype = df_optimized[col].dtype

                if pd.api.types.is_integer_dtype(col_dtype):
                    if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                        df_optimized[col] = df_optimized[col].astype(np.int8)
                    elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                        df_optimized[col] = df_optimized[col].astype(np.int16)
                    elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                        df_optimized[col] = df_optimized[col].astype(np.int32)
                    else:
                        df_optimized[col] = df_optimized[col].astype(np.int64)

                elif pd.api.types.is_float_dtype(col_dtype):
                    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast="float")

            end_mem = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024
            try:
                reduction_pct = 100.0 * (start_mem - end_mem) / start_mem
            except Exception:
                reduction_pct = 0.0
            mem_msg = "Memory reduced from " + str(round(start_mem, 2)) + " MB to " + str(round(end_mem, 2)) + " MB (" + str(round(reduction_pct, 1)) + "% reduction)"
            print(mem_msg)
            return df_optimized

        parser = argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--advanced_alphanumeric', type=str, default="true")
        parser.add_argument('--date_parse', type=str, default="true")
        parser.add_argument('--optimize_dtypes', type=str, default="true")
        parser.add_argument('--X', type=str, required=True)
        parser.add_argument('--y', type=str, required=True)
        args = parser.parse_args()

        try:
            in_path = args.in_file
            target_col = args.target_column
            detect_thresh = float(args.numeric_detection_threshold)
            advanced_flag = str(args.advanced_alphanumeric).lower() in ("1","true","t","yes","y")
            date_parse_flag = str(args.date_parse).lower() in ("1","true","t","yes","y")
            optimize_flag = str(args.optimize_dtypes).lower() in ("1","true","t","yes","y")
            out_X = args.X
            out_y = args.y

            if not os.path.exists(in_path):
                print("ERROR: input file does not exist: " + in_path, file=sys.stderr)
                sys.exit(1)

            print("Loading: " + in_path)
            df = read_with_pandas(in_path)
            print("Loaded shape: " + str(df.shape))

            before = len(df)
            df = df.drop_duplicates(keep='first')
            dup_msg = "Removed duplicates: " + str(before - len(df)) + " rows dropped"
            print(dup_msg)

            if target_col not in df.columns:
                print("ERROR: target column '" + target_col + "' not found in data", file=sys.stderr)
                sys.exit(1)
            before = len(df)
            df = df[df[target_col].notna()].reset_index(drop=True)
            target_msg = "Dropped rows with null target '" + target_col + "': " + str(before - len(df)) + " rows dropped"
            print(target_msg)

            alpha_msg = "Running advanced alphanumeric numeric detection (enabled=" + str(advanced_flag) + ") with threshold=" + str(round(detect_thresh, 2))
            print(alpha_msg)
            df_conv, conv_report = convert_object_columns_advanced(df, detect_threshold=detect_thresh, decimal_comma=False, advanced=advanced_flag)
            df = df_conv
            print("Conversion report: " + json.dumps(conv_report, default=str)[:1000])

            date_msg = "Running date detection & feature extraction (enabled=" + str(date_parse_flag) + ")"
            print(date_msg)
            df, date_report = detect_and_extract_dates(df, date_parse_enabled=date_parse_flag)
            print("Date parse report: " + json.dumps(date_report, default=str)[:1000])

            if optimize_flag:
                print("Optimizing numeric dtypes...")
                df = optimize_numeric_dtypes(df)
            else:
                print("Skipping dtype optimization (optimize_dtypes=False).")

            y_ser = df[[target_col]].copy()
            X_df = df.drop(columns=[target_col]).copy()

            def makedirs(p):
                d = os.path.dirname(p)
                if d:
                    os.makedirs(d, exist_ok=True)
            
            makedirs(out_X)
            makedirs(out_y)

            X_df.to_parquet(out_X, index=False)
            y_ser.to_parquet(out_y, index=False)

            print("Saved X to: " + out_X)
            print("Saved y to: " + out_y)
            print("SUCCESS: Preprocessing complete.")

        except Exception as exc:
            print("ERROR during preprocessing: " + str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --advanced_alphanumeric
      - {inputValue: advanced_alphanumeric}
      - --date_parse
      - {inputValue: date_parse}
      - --optimize_dtypes
      - {inputValue: optimize_dtypes}
      - --X
      - {outputPath: X}
      - --y
      - {outputPath: y}
