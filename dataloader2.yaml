name: Data loader v9.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: split_size, type: Integer, default: "15", description: 'Test split size as integer percent (default 15) or fraction (e.g. 0.2)'}
  - {name: target_column, type: String, description: 'Name of target column used for anomaly labeling'}

outputs:
  - {name: train_set, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: model_type, type: String, description: 'Auto-detected model type: classification or regression'}

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--split_size', type=float, default=15)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--train_set', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--model_type', type=str, required=True)
        args = parser.parse_args()

        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("api_pagination")

        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }

        payload = {
            "dbType": "TIDB",
            "entityId": "",
            "entityIds": [],
            "ownedOnly": False,
            "projections": [],
            "filter": {},
            "startTime": 0,
            "endTime": 0
        }

        def extract_records(body):
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                # Common patterns for data containers
                for key in ["data", "content", "records", "results", "items"]:
                    if key in body and isinstance(body[key], list):
                        return body[key]
                # If dict has no list container, might be single record
                return [body]
            return []

        def discover_page_metadata(body):
            page_size = None
            total_pages = None
            total_instances = None

            if isinstance(body, dict):
                page_size = body.get("pageSize")
                total_pages = body.get("totalPages")
                total_instances = body.get("totalInstances")

                # Check nested "page" object
                if "page" in body and isinstance(body["page"], dict):
                    p = body["page"]
                    page_size = page_size or p.get("pageSize")
                    total_pages = total_pages or p.get("totalPages")
                    total_instances = total_instances or p.get("totalInstances")

            return page_size, total_pages, total_instances

        def detect_model_type(target_series):
            if target_series.empty:
                logger.warning("Empty target column, defaulting to classification")
                return "classification"
            
            # Remove nulls for analysis
            target_clean = target_series.dropna()
            
            if len(target_clean) == 0:
                logger.warning("Target column contains only nulls, defaulting to classification")
                return "classification"
            
            original_dtype = target_clean.dtype
            
            # Try to convert to numeric (handles string representations of numbers)
            target_numeric = pd.to_numeric(target_clean, errors='coerce')
            numeric_conversion_success = target_numeric.notna().sum() / len(target_clean) > 0.8
            
            if numeric_conversion_success:
                # Use numeric version for analysis
                target_clean = target_numeric.dropna()
                logger.info(f"Successfully converted {len(target_clean)} values to numeric")
            
            dtype = target_clean.dtype
            n_unique = target_clean.nunique()
            n_total = len(target_clean)
            unique_ratio = n_unique / n_total
            
            logger.info(f"Target analysis: original_dtype={original_dtype}, dtype={dtype}, unique_values={n_unique}, total={n_total}, ratio={unique_ratio:.4f}")
            
            # Rule 1: Boolean type
            if pd.api.types.is_bool_dtype(dtype):
                logger.info("Detected classification (boolean type)")
                return "classification"
            
            # Rule 2: Non-numeric string type (after conversion attempt)
            if (dtype == 'object' or pd.api.types.is_string_dtype(dtype)) and not numeric_conversion_success:
                logger.info("Detected classification (non-numeric string type)")
                return "classification"
            
            # Rule 3: Numeric with few unique values
            if n_unique <= 10:
                logger.info(f"Detected classification ({n_unique} unique values <= 10)")
                return "classification"
            
            # Rule 4: Moderate unique values (11-20) - check if they're sequential integers
            if 10 < n_unique <= 20:
                if pd.api.types.is_integer_dtype(dtype):
                    unique_vals = sorted(target_clean.unique())
                    is_sequential = all(unique_vals[i+1] - unique_vals[i] == 1 for i in range(len(unique_vals)-1))
                    if not is_sequential:
                        logger.info(f"Detected classification ({n_unique} discrete integer values)")
                        return "classification"
                else:
                    # Non-integer numeric with 11-20 unique values
                    logger.info(f"Detected regression ({n_unique} continuous values)")
                    return "regression"
            
            # Rule 5: Many unique values with high ratio
            if n_unique > 20 or unique_ratio > 0.05:
                logger.info(f"Detected regression ({n_unique} unique values, {unique_ratio:.2%} ratio)")
                return "regression"
            
            # Default to classification
            logger.info("Defaulting to classification")
            return "classification"

        # === STEP 1: Metadata Discovery ===
        initial_size = 2000
        base_url = args.api_url.rstrip('/')
        separator = '&' if '?' in base_url else '?'
        meta_url = f"{base_url}{separator}page=0&size={initial_size}&showPageableMetaData=true"

        logger.info("Step 1: Making metadata discovery request")
        try:
            resp_meta = session.post(meta_url, headers=headers, json=payload, timeout=30)
            resp_meta.raise_for_status()
            body_meta = resp_meta.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Metadata request failed: {e}")
            raise

        # === STEP 2: Extract Pagination Metadata ===
        detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
        
        logger.info(f"Detected: pageSize={detected_page_size}, totalPages={total_pages}, totalInstances={total_instances}")

        # === STEP 3: Decide Page Size ===
        if detected_page_size:
            page_size = min(detected_page_size, initial_size)
            logger.info(f"Using server pageSize: {page_size}")
        else:
            inferred_records = extract_records(body_meta)
            page_size = len(inferred_records) if inferred_records else initial_size
            logger.info(f"Inferred pageSize from response: {page_size}")

        # === STEP 4: Determine Total Pages ===
        if total_pages is None and total_instances:
            total_pages = (total_instances + page_size - 1) // page_size
            logger.info(f"Calculated totalPages from totalInstances: {total_pages}")

        # === STEP 5: Pagination Strategy ===
        all_records = []

        if total_pages is None:
            # Case A: No pagination metadata - take what we got and stop
            logger.warning("No pagination metadata found. Assuming single-page API.")
            all_records.extend(extract_records(body_meta))
        else:
            # Case B: Pagination exists - fetch all pages deterministically
            logger.info(f"Fetching {total_pages} pages with size {page_size}")
            
            for page in range(total_pages):
                page_url = f"{base_url}{separator}page={page}&size={page_size}&showPageableMetaData=true"
                
                logger.info(f"Fetching page {page + 1}/{total_pages}")
                try:
                    resp_page = session.post(page_url, headers=headers, json=payload, timeout=30)
                    resp_page.raise_for_status()
                    body_page = resp_page.json()
                    
                    records = extract_records(body_page)
                    all_records.extend(records)
                    logger.info(f"Page {page + 1}: Retrieved {len(records)} records")
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"Failed to fetch page {page}: {e}")
                    raise

        # === STEP 6: Stop Condition - Already Built In ===
        logger.info(f"Total records collected: {len(all_records)}")

        if not all_records:
            raise ValueError("No records retrieved from API")

        df = pd.DataFrame(all_records)

        # === Prepare Target ===
        if args.target_column in df.columns:
            target_values = df[args.target_column].copy()
        else:
            logger.warning(
                f"Target column '{args.target_column}' not found. Creating zero target values."
            )
            target_values = pd.Series(
                np.zeros(len(df), dtype=np.int32),
                index=df.index,
                name=args.target_column
            )

        # === DETECT MODEL TYPE ===
        detected_model_type = detect_model_type(target_values)
        logger.info(f"✓ Detected model type: {detected_model_type.upper()}")
        
        # === Convert target to numeric if regression detected ===
        if detected_model_type == "regression":
            target_numeric = pd.to_numeric(target_values, errors='coerce')
            if target_numeric.notna().sum() / len(target_values) > 0.8:
                df[args.target_column] = target_numeric
                target_values = target_numeric
                logger.info(f"Converted target column to numeric type for regression")
            else:
                logger.warning("Failed to convert target to numeric, keeping original values")

        # === Stratification Logic with Safety Check ===
        stratify_param = None
        if detected_model_type == "classification" and target_values.nunique() > 1:
            # Check minimum class count
            value_counts = target_values.value_counts()
            min_class_count = value_counts.min()
            
            if min_class_count >= 2:
                stratify_param = target_values
                logger.info(f"Using stratified split (min class count: {min_class_count})")
            else:
                logger.warning(
                    f"Skipping stratification: some classes have only {min_class_count} sample(s). "
                    f"Class distribution: {value_counts.to_dict()}"
                )
        elif detected_model_type == "regression":
            logger.info("Regression detected: using random split (no stratification)")

        # === Convert Split Size ===
        split_size_input = float(args.split_size)
        test_size = split_size_input / 100.0 if split_size_input >= 1.0 else split_size_input
        if not (0.0 < test_size < 1.0):
            raise ValueError("split_size must be between 0 and 1 or 1–99 (as percent)")

        # === Train-Test Split ===
        train_df, test_df = train_test_split(
            df,
            test_size=test_size,
            random_state=42,
            stratify=stratify_param
        )

        # === Diagnostics ===
        print("===== PAGINATION SUMMARY =====")
        print(f"Total records fetched: {len(all_records)}")
        print(f"Final dataset shape: {df.shape}")
        
        print(f"===== MODEL TYPE: {detected_model_type.upper()} =====")
        print(f"Target column: {args.target_column}")
        print(f"Unique values: {target_values.nunique()}")
        print(f"Data type: {target_values.dtype}")
        
        if detected_model_type == "classification":
            print(f"Class distribution:{target_values.value_counts().to_string()}")
        else:
            print(f"Target statistics:")
            print(f"  Min: {target_values.min()}")
            print(f"  Max: {target_values.max()}")
            print(f"  Mean: {target_values.mean():.4f}")
            print(f"  Median: {target_values.median():.4f}")
            print(f"  Std: {target_values.std():.4f}")
        
        print("===== TRAIN SET =====")
        print("Shape:", train_df.shape)
        if detected_model_type == "classification":
            print(f"Class distribution in train:{train_df[args.target_column].value_counts().to_string()}")
        else:
            print(f"Target range in train: [{train_df[args.target_column].min():.4f}, {train_df[args.target_column].max():.4f}]")
        print(train_df.head(3).to_string())

        print("===== TEST DATA =====")
        print("Shape:", test_df.shape)
        if detected_model_type == "classification":
            print(f"Class distribution in test:{test_df[args.target_column].value_counts().to_string()}")
        else:
            print(f"Target range in test: [{test_df[args.target_column].min():.4f}, {test_df[args.target_column].max():.4f}]")
        print(test_df.head(3).to_string())

        # === Save Outputs ===
        os.makedirs(os.path.dirname(args.train_set) or ".", exist_ok=True)
        train_df.to_parquet(args.train_set, index=False)

        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        test_df.to_parquet(args.test_data, index=False)

        # === Save Model Type Output ===
        os.makedirs(os.path.dirname(args.model_type) or ".", exist_ok=True)
        with open(args.model_type, 'w') as f:
            f.write(detected_model_type)
        
        logger.info(f"✓ Model type '{detected_model_type}' saved to output")
        logger.info(f"✓ Train set saved: {train_df.shape[0]} samples")
        logger.info(f"✓ Test set saved: {test_df.shape[0]} samples")
        logger.info("✓ Data loading and splitting completed successfully")

    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --split_size
      - {inputValue: split_size}
      - --target_column
      - {inputValue: target_column}
      - --train_set
      - {outputPath: train_set}
      - --test_data
      - {outputPath: test_data}
      - --model_type
      - {outputPath: model_type}
