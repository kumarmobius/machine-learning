name: Feature Selection v7
inputs:
  - {name: engineered_X, type: Dataset, description: "Engineered features dataset from Feature Engineering brick (parquet format)"}
  - {name: train_y, type: Dataset, description: "Target labels dataset corresponding to engineered features (parquet format)"}
  - {name: engineering_metadata, type: Data, description: "JSON metadata from Feature Engineering brick containing column information and preprocessing details"}
  - {name: preprocessor, type: Data, description: "Fitted preprocessor object from Feature Engineering brick (cloudpickle format)"}
  
  # Core feature selection control
  - {name: enable_feature_selection, type: String, description: "Enable or disable feature selection pipeline. Use 'true' to activate feature selection, 'false' to pass through all features", optional: true, default: "false"}
  - {name: model_type, type: String, description: "Type of machine learning task. Must be 'classification' or 'regression'. Affects correlation calculation methods."}
  
  # Target correlation filter parameters
  - {name: enable_target_corr_filter, type: String, description: "Enable supervised filtering based on correlation with target. Drops numeric features with low correlation to target. Use 'true' for regression problems or when target relationship is linear.", optional: true, default: "false"}
  - {name: target_corr_threshold, type: String, description: "Minimum absolute correlation value to keep a feature. Features with |correlation| < threshold are dropped. Range: 0.0 to 1.0. Lower values keep more features.", optional: true, default: "0.01"}
  - {name: corr_method, type: String, description: "Correlation calculation method for numeric features. 'pearson' for linear relationships, 'spearman' for monotonic relationships. Spearman is more robust to outliers.", optional: true, default: "spearman"}
  
  # Feature-to-feature correlation filter
  - {name: feature_corr_threshold, type: String, description: "Threshold for removing highly correlated features. Features with correlation > threshold are considered redundant. Range: 0.0 to 1.0. Higher values keep more correlated features.", optional: true, default: "0.95"}
  
  # Variance filter
  - {name: variance_threshold, type: String, description: "Minimum variance threshold for near-zero variance filter. Features with variance < threshold are removed. Use 0.0 to disable. Typical values: 1e-5 to 1e-8.", optional: true, default: "1e-5"}
  
  # Feature ranking and selection
  - {name: max_features, type: String, description: "Maximum number of features to keep after selection. Use '0' for automatic selection based on sqrt(n_features). Use positive integer to set exact limit.", optional: true, default: "0"}
  - {name: selection_method, type: String, description: "Method for ranking features: 'mi' for Mutual Information (works for both linear and nonlinear relationships), 'variance' for variance-based ranking (simpler but less accurate).", optional: true, default: "mi"}
  
  # Handling of non-numeric features
  - {name: keep_non_numeric, type: String, description: "Whether to preserve non-numeric features during target correlation filtering. Use 'true' to keep categorical/string features, 'false' to only process numeric features.", optional: true, default: "true"}
  
  # Advanced options
  - {name: random_state, type: String, description: "Random seed for reproducible feature selection. Use any integer value for consistent results across runs.", optional: true, default: "42"}
  - {name: use_mrmr, type: String, description: "Use Minimum Redundancy Maximum Relevance algorithm instead of simple MI ranking. More computationally expensive but may produce better feature subsets.", optional: true, default: "false"}

outputs:
  - {name: train_X, type: Dataset, description: "Final selected features dataset in parquet format. Contains only the features selected by the feature selection pipeline."}
  - {name: train_y, type: Dataset, description: "Target labels dataset (pass-through from input) in parquet format. Unchanged from input."}
  - {name: feature_selector, type: Data, description: "Fitted FeatureSelector object saved in cloudpickle format. Can be used later for transforming test data."}
  - {name: preprocessor, type: Data, description: "Preprocessor object (pass-through from input) in cloudpickle format. Unchanged from input."}
  - {name: preprocess_metadata, type: Data, description: "Comprehensive JSON metadata containing: selected features list, filtering statistics, correlation scores, and pipeline configuration."}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, gzip, math, shutil
        from datetime import datetime
        import pandas as pd, numpy as np
        from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, VarianceThreshold
        from scipy import stats
        import cloudpickle
        
        # Helper functions
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        # Simplified FeatureSelector Class
        class FeatureSelector:
            def __init__(self, task, 
                         enable_target_corr_filter=False, 
                         target_corr_threshold=0.01, 
                         corr_method="spearman",
                         variance_threshold=1e-5,
                         feature_corr_threshold=0.95,
                         max_features=0,
                         selection_method="mi",
                         keep_non_numeric=True,
                         random_state=42,
                         use_mrmr=False):
                
                self.task = str(task).lower()
                self.enable_target_corr_filter = bool(enable_target_corr_filter)
                self.target_corr_threshold = float(target_corr_threshold)
                self.corr_method = str(corr_method).lower()
                self.variance_threshold = float(variance_threshold)
                self.feature_corr_threshold = float(feature_corr_threshold)
                self.max_features = int(max_features)
                self.selection_method = str(selection_method).lower()
                self.keep_non_numeric = bool(keep_non_numeric)
                self.random_state = int(random_state)
                self.use_mrmr = bool(use_mrmr)
                
                # Results storage
                self.selected_features = []
                self._filter_stats = {}
                self._correlation_scores = {}
                self._mi_scores = {}
                self._feature_importance = {}
                
                # Validate inputs
                if self.corr_method not in ["pearson", "spearman"]:
                    raise ValueError(f"corr_method must be 'pearson' or 'spearman', got {self.corr_method}")
                if self.selection_method not in ["mi", "variance"]:
                    raise ValueError(f"selection_method must be 'mi' or 'variance', got {self.selection_method}")
                if self.target_corr_threshold < 0 or self.target_corr_threshold > 1:
                    raise ValueError(f"target_corr_threshold must be between 0 and 1, got {self.target_corr_threshold}")
                if self.feature_corr_threshold < 0 or self.feature_corr_threshold > 1:
                    raise ValueError(f"feature_corr_threshold must be between 0 and 1, got {self.feature_corr_threshold}")
            
            @staticmethod
            def _nzv_filter(X, threshold=1e-5):
                if X.shape[1] == 0:
                    return X, []
                
                vt = VarianceThreshold(threshold=threshold)
                vt.fit(X.values)
                mask = vt.get_support()
                kept_features = list(X.columns[mask])
                removed_features = list(X.columns[~mask])
                
                return X[kept_features], removed_features
            
            def _target_correlation_filter(self, X, y):
                if not self.enable_target_corr_filter:
                    return list(X.columns), {}
                
                print(f"[INFO] Computing target correlation ({self.corr_method})...")
                
                # Prepare data
                X_df = X.copy()
                y_series = pd.Series(y).reset_index(drop=True)
                
                # Separate numeric and non-numeric columns
                numeric_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()
                non_numeric_cols = [col for col in X_df.columns if col not in numeric_cols]
                
                # Compute correlation for numeric features
                corr_scores = {}
                for col in numeric_cols:
                    x_vals = X_df[col].reset_index(drop=True)
                    
                    if self.task == "regression":
                        # For regression: direct correlation
                        if self.corr_method == "pearson":
                            corr, _ = stats.pearsonr(x_vals.dropna(), y_series[x_vals.notna()])
                        else:  # spearman
                            corr, _ = stats.spearmanr(x_vals.dropna(), y_series[x_vals.notna()])
                        corr_scores[col] = abs(corr) if not np.isnan(corr) else 0.0
                    
                    else:  # classification
                        unique_classes = y_series.unique()
                        if len(unique_classes) == 2:
                            # Binary: point-biserial correlation
                            try:
                                y_binary = y_series.map({unique_classes[0]: 0, unique_classes[1]: 1})
                                corr, _ = stats.pointbiserialr(x_vals, y_binary)
                                corr_scores[col] = abs(corr) if not np.isnan(corr) else 0.0
                            except:
                                # Fallback: mean difference
                                class0_mean = x_vals[y_series == unique_classes[0]].mean()
                                class1_mean = x_vals[y_series == unique_classes[1]].mean()
                                corr_scores[col] = abs(class0_mean - class1_mean)
                        else:
                            # Multi-class: ANOVA F-value as proxy
                            try:
                                groups = [x_vals[y_series == cls].dropna() for cls in unique_classes]
                                if all(len(g) > 0 for g in groups):
                                    f_stat, _ = stats.f_oneway(*groups)
                                    corr_scores[col] = abs(f_stat) if not np.isnan(f_stat) else 0.0
                                else:
                                    corr_scores[col] = 0.0
                            except:
                                corr_scores[col] = 0.0
                
                # Store correlation scores
                self._correlation_scores = corr_scores
                
                # Determine which features to keep
                keep_features = []
                dropped_features = []
                
                for col in X_df.columns:
                    if col in corr_scores:
                        score = corr_scores[col]
                        if score >= self.target_corr_threshold:
                            keep_features.append(col)
                        else:
                            dropped_features.append(col)
                    else:
                        # Non-numeric features: keep if configured
                        if self.keep_non_numeric:
                            keep_features.append(col)
                        else:
                            dropped_features.append(col)
                
                print(f"[INFO] Target correlation filter: kept {len(keep_features)}, dropped {len(dropped_features)}")
                return keep_features, dropped_features
            
            @staticmethod
            def _remove_correlated_features(X, threshold=0.95):
                cols = list(X.columns)
                if len(cols) <= 1:
                    return cols, []
                
                # Compute correlation matrix
                corr = X.corr(method="spearman").abs()
                np.fill_diagonal(corr.values, 0.0)
                
                keep = set(cols)
                removed = []
                
                while True:
                    max_val = corr.loc[list(keep), list(keep)].values.max()
                    if max_val < threshold:
                        break
                    
                    # Find the pair with highest correlation
                    idx = np.unravel_index(np.argmax(corr.loc[list(keep), list(keep)].values), 
                                          corr.loc[list(keep), list(keep)].shape)
                    idx = (list(keep)[idx[0]], list(keep)[idx[1]])
                    
                    # Remove the feature with higher average correlation to others
                    avg_corr_a = corr.loc[idx[0], list(keep)].mean()
                    avg_corr_b = corr.loc[idx[1], list(keep)].mean()
                    
                    to_remove = idx[0] if avg_corr_a >= avg_corr_b else idx[1]
                    keep.remove(to_remove)
                    removed.append(to_remove)
                
                return list(keep), removed
            
            def _rank_features(self, X, y):
                if self.selection_method == "mi":
                    # Mutual Information ranking
                    if self.task == "classification":
                        scores = mutual_info_classif(X.values, y, random_state=self.random_state)
                    else:
                        y_num = pd.to_numeric(y, errors="ignore")
                        scores = mutual_info_regression(X.values, y_num, random_state=self.random_state)
                    
                    mi_series = pd.Series(scores, index=X.columns).fillna(0.0)
                    self._mi_scores = mi_series.to_dict()
                    return mi_series.sort_values(ascending=False)
                
                else:  # variance method
                    variances = X.var().sort_values(ascending=False)
                    self._feature_importance = variances.to_dict()
                    return variances
            
            def _select_top_features(self, X, rankings):
                n_features = X.shape[1]
                
                # Determine k
                if self.max_features > 0:
                    k = min(self.max_features, n_features)
                else:
                    # Automatic selection: sqrt(n_features)
                    k = max(10, int(math.sqrt(n_features)))
                
                # Ensure k is reasonable
                k = max(1, min(k, n_features))
                
                # Get top k features
                top_features = list(rankings.head(k).index)
                print(f"[INFO] Selecting top {k} features from {n_features} candidates")
                
                return top_features
            
            def fit(self, X, y):
                print(f"[INFO] Starting feature selection on {X.shape[1]} features")
                
                # Step 0: Store original info
                self._filter_stats = {
                    'original_features': X.shape[1],
                    'original_samples': X.shape[0]
                }
                
                # Step 1: Variance filter
                X_var, removed_var = self._nzv_filter(X, self.variance_threshold)
                self._filter_stats['after_variance'] = X_var.shape[1]
                self._filter_stats['removed_variance'] = removed_var
                print(f"[INFO] After variance filter: {X_var.shape[1]} features")
                
                # Step 2: Target correlation filter (if enabled)
                if self.enable_target_corr_filter:
                    keep_target, removed_target = self._target_correlation_filter(X_var, y)
                    X_target = X_var[keep_target]
                    self._filter_stats['after_target_corr'] = len(keep_target)
                    self._filter_stats['removed_target_corr'] = removed_target
                else:
                    X_target = X_var
                    self._filter_stats['after_target_corr'] = X_var.shape[1]
                
                # Step 3: Remove correlated features
                keep_corr, removed_corr = self._remove_correlated_features(X_target, self.feature_corr_threshold)
                X_corr = X_target[keep_corr]
                self._filter_stats['after_correlation'] = len(keep_corr)
                self._filter_stats['removed_correlation'] = removed_corr
                print(f"[INFO] After correlation filter: {len(keep_corr)} features")
                
                # Step 4: Rank and select features
                rankings = self._rank_features(X_corr, y)
                self.selected_features = self._select_top_features(X_corr, rankings)
                
                # Final statistics
                self._filter_stats['final_features'] = len(self.selected_features)
                self._filter_stats['reduction_percentage'] = round(
                    100 * (1 - len(self.selected_features) / X.shape[1]), 2
                )
                
                print(f"[INFO] Feature selection complete: {X.shape[1]} → {len(self.selected_features)} features")
                print(f"[INFO] Reduction: {self._filter_stats['reduction_percentage']}%")
                
                return self
            
            def transform(self, X):
                # Ensure all selected features exist in X
                missing_features = set(self.selected_features) - set(X.columns)
                if missing_features:
                    print(f"[WARNING] {len(missing_features)} features missing, filling with zeros")
                    for feat in missing_features:
                        X[feat] = 0.0
                
                return X[self.selected_features]
            
            def save(self, path):
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
            
            @staticmethod
            def load(path):
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)
        
        # Main execution
        parser = argparse.ArgumentParser()
        parser.add_argument('--engineered_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--engineering_metadata', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--enable_feature_selection', type=str, default="false")
        parser.add_argument('--model_type', type=str, required=True)
        parser.add_argument('--enable_target_corr_filter', type=str, default="false")
        parser.add_argument('--target_corr_threshold', type=str, default="0.01")
        parser.add_argument('--corr_method', type=str, default="spearman")
        parser.add_argument('--variance_threshold', type=str, default="1e-5")
        parser.add_argument('--feature_corr_threshold', type=str, default="0.95")
        parser.add_argument('--max_features', type=str, default="0")
        parser.add_argument('--selection_method', type=str, default="mi")
        parser.add_argument('--keep_non_numeric', type=str, default="true")
        parser.add_argument('--random_state', type=str, default="42")
        parser.add_argument('--use_mrmr', type=str, default="false")
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y_out', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--preprocessor_out', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args = parser.parse_args()
        
        try:
            print("="*80)
            print("FEATURE SELECTION v6.1")
            print("="*80)
            
            # Parse arguments
            enable_fs = str(args.enable_feature_selection).lower() in ("1", "true", "t", "yes", "y")
            model_type = args.model_type.strip().lower()
            
            # Load data
            print("[STEP 1/6] Loading data...")
            X_eng = pd.read_parquet(args.engineered_X)
            y_train = pd.read_parquet(args.train_y)
            print(f"  Features: {X_eng.shape[1]}, Samples: {X_eng.shape[0]}")
            print(f"  Target shape: {y_train.shape}")
            
            # Load metadata
            with open(args.engineering_metadata, 'r') as f:
                eng_meta = json.load(f)
            target_cols = eng_meta.get('target_columns', [])
            print(f"  Target columns: {target_cols}")
            
            # Feature selection
            if enable_fs:
                print("[STEP 2/6] Configuring feature selector...")
                print(f"  Model type: {model_type}")
                print(f"  Target correlation filter: {args.enable_target_corr_filter}")
                print(f"  Max features: {args.max_features}")
                
                # Initialize feature selector
                fs = FeatureSelector(
                    task=model_type,
                    enable_target_corr_filter=str(args.enable_target_corr_filter).lower() in ("1", "true", "t", "yes", "y"),
                    target_corr_threshold=float(args.target_corr_threshold),
                    corr_method=args.corr_method,
                    variance_threshold=float(args.variance_threshold),
                    feature_corr_threshold=float(args.feature_corr_threshold),
                    max_features=int(args.max_features),
                    selection_method=args.selection_method,
                    keep_non_numeric=str(args.keep_non_numeric).lower() in ("1", "true", "t", "yes", "y"),
                    random_state=int(args.random_state),
                    use_mrmr=str(args.use_mrmr).lower() in ("1", "true", "t", "yes", "y")
                )
                
                # Fit feature selector
                print("[STEP 3/6] Fitting feature selector...")
                y_for_fs = y_train[target_cols[0]] if isinstance(y_train, pd.DataFrame) else y_train
                fs.fit(X_eng, y_for_fs.values)
                
                # Transform data
                print("[STEP 4/6] Transforming features...")
                X_selected = fs.transform(X_eng)
                
                # Print feature importance
                if fs._mi_scores:
                    print("[INFO] Top 10 features by Mutual Information:")
                    sorted_mi = sorted(fs._mi_scores.items(), key=lambda x: x[1], reverse=True)[:10]
                    for i, (feat, score) in enumerate(sorted_mi, 1):
                        mark = "✓" if feat in fs.selected_features else "✗"
                        print(f"  {i:2d}. {mark} {feat}: {score:.4f}")
                
            else:
                print("[STEP 2/6] Feature selection disabled...")
                X_selected = X_eng.copy()
                fs = None
            
            # Save outputs
            print("[STEP 5/6] Saving outputs...")
            
            # Save selected features
            ensure_dir_for(args.train_X)
            X_selected.to_parquet(args.train_X, index=False)
            print(f"  Saved train_X: {args.train_X} ({X_selected.shape[1]} features)")
            
            # Save target (pass-through)
            ensure_dir_for(args.train_y_out)
            y_train.to_parquet(args.train_y_out, index=False)
            print(f"  Saved train_y: {args.train_y_out}")
            
            # Save feature selector
            ensure_dir_for(args.feature_selector)
            if fs is not None:
                fs.save(args.feature_selector)
                print(f"  Saved feature_selector: {args.feature_selector}")
            else:
                # Create minimal selector for consistency
                dummy_fs = type('DummySelector', (), {
                    'selected_features': list(X_selected.columns),
                    'transform': lambda self, X: X.reindex(columns=self.selected_features, fill_value=0.0)
                })()
                dummy_fs.selected_features = list(X_selected.columns)
                with gzip.open(args.feature_selector, "wb") as f:
                    cloudpickle.dump(dummy_fs, f)
                print(f"  Saved dummy feature_selector: {args.feature_selector}")
            
            # Pass-through preprocessor
            ensure_dir_for(args.preprocessor_out)
            shutil.copy(args.preprocessor, args.preprocessor_out)
            print(f"  Copied preprocessor: {args.preprocessor_out}")
            
            # Save comprehensive metadata
            print("[STEP 6/6] Saving metadata...")
            ensure_dir_for(args.preprocess_metadata)
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'model_type': model_type,
                'target_columns': target_cols,
                'feature_selection': {
                    'enabled': enable_fs,
                    'settings': {
                        'enable_target_corr_filter': args.enable_target_corr_filter,
                        'target_corr_threshold': float(args.target_corr_threshold),
                        'corr_method': args.corr_method,
                        'variance_threshold': float(args.variance_threshold),
                        'feature_corr_threshold': float(args.feature_corr_threshold),
                        'max_features': int(args.max_features),
                        'selection_method': args.selection_method,
                        'keep_non_numeric': args.keep_non_numeric,
                        'random_state': int(args.random_state),
                        'use_mrmr': args.use_mrmr
                    } if enable_fs else {},
                    'statistics': fs._filter_stats if fs else {
                        'original_features': X_eng.shape[1],
                        'final_features': X_selected.shape[1]
                    },
                    'selected_features': fs.selected_features if fs else list(X_selected.columns),
                    'correlation_scores': fs._correlation_scores if fs else {},
                    'mi_scores': fs._mi_scores if fs else {},
                    'feature_importance': fs._feature_importance if fs else {}
                },
                'dataset_info': {
                    'original_shape': {'rows': X_eng.shape[0], 'cols': X_eng.shape[1]},
                    'final_shape': {'rows': X_selected.shape[0], 'cols': X_selected.shape[1]},
                    'reduction_percentage': round(100 * (1 - X_selected.shape[1] / X_eng.shape[1]), 2) if X_eng.shape[1] > 0 else 0
                },
                'engineering_metadata': eng_meta
            }
            
            with open(args.preprocess_metadata, 'w') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            print(f"  Saved metadata: {args.preprocess_metadata}")
            
            # Final summary
            print("="*80)
            print("FEATURE SELECTION COMPLETE")
            print("="*80)
            print(f"Input features: {X_eng.shape[1]}")
            print(f"Output features: {X_selected.shape[1]}")
            print(f"Feature reduction: {X_eng.shape[1] - X_selected.shape[1]} features")
            print(f"Reduction percentage: {metadata['dataset_info']['reduction_percentage']}%")
            print(f"Selected features ({len(metadata['feature_selection']['selected_features'])}):")
            for i, feat in enumerate(metadata['feature_selection']['selected_features'][:20], 1):
                print(f"  {i:2d}. {feat}")
            if len(metadata['feature_selection']['selected_features']) > 20:
                print(f"  ... and {len(metadata['feature_selection']['selected_features']) - 20} more")
            print("="*80)
            
        except Exception as exc:
            print(f"ERROR: {exc}", file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --engineered_X
      - {inputPath: engineered_X}
      - --train_y
      - {inputPath: train_y}
      - --engineering_metadata
      - {inputPath: engineering_metadata}
      - --preprocessor
      - {inputPath: preprocessor}
      - --enable_feature_selection
      - {inputValue: enable_feature_selection}
      - --model_type
      - {inputValue: model_type}
      - --enable_target_corr_filter
      - {inputValue: enable_target_corr_filter}
      - --target_corr_threshold
      - {inputValue: target_corr_threshold}
      - --corr_method
      - {inputValue: corr_method}
      - --variance_threshold
      - {inputValue: variance_threshold}
      - --feature_corr_threshold
      - {inputValue: feature_corr_threshold}
      - --max_features
      - {inputValue: max_features}
      - --selection_method
      - {inputValue: selection_method}
      - --keep_non_numeric
      - {inputValue: keep_non_numeric}
      - --random_state
      - {inputValue: random_state}
      - --use_mrmr
      - {inputValue: use_mrmr}
      - --train_X
      - {outputPath: train_X}
      - --train_y_out
      - {outputPath: train_y}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocessor_out
      - {outputPath: preprocessor}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
