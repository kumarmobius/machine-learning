name: Data loader with cdn v5

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download dataset file (CSV, JSON, or Parquet)'}
  - {name: split_size, type: Integer, default: "15", description: 'Test split size percent or fraction'}
  - {name: target_column, type: String, description: 'Target column or comma-separated targets'}
  - {name: model_type, type: String, default: "classification", description: 'classification or regression'}

  # NEW
  - {name: use_column, type: String, optional: true, description: 'Comma-separated columns to keep'}
  - {name: drop_columns, type: String, optional: true, description: 'Comma-separated columns to drop'}

outputs:
  - {name: train_set, type: Dataset}
  - {name: test_data, type: Dataset}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import tempfile
        import logging
        import json
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        try:
            from urllib3.util import Retry
        except Exception:
            from urllib3 import Retry
        from sklearn.model_selection import train_test_split

        # ---------- CLI ----------
        parser = argparse.ArgumentParser()
        parser.add_argument('--cdn_url', required=True)
        parser.add_argument('--split_size', type=float, default=15)
        parser.add_argument('--target_column', required=True)
        parser.add_argument('--model_type', default="classification")
        parser.add_argument('--use_column', default=None)
        parser.add_argument('--drop_columns', default=None)
        parser.add_argument('--train_set', required=True)
        parser.add_argument('--test_data', required=True)
        args = parser.parse_args()

        # ---------- Logging ----------
        logging.basicConfig(
            stream=sys.stdout,
            level=logging.INFO,
            format="%(asctime)s %(levelname)s %(message)s"
        )
        logger = logging.getLogger("cdn_loader")

        # ---------- HTTP session ----------
        session = requests.Session()
        retry = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500,502,503,504],
            allowed_methods=frozenset(["GET","POST","HEAD"])
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ---------- Download ----------
        logger.info("Downloading dataset from CDN: %s", args.cdn_url)
        resp = session.get(args.cdn_url, timeout=30)
        resp.raise_for_status()

        fd, tmp_path = tempfile.mkstemp(suffix=".download")
        os.close(fd)
        with open(tmp_path, "wb") as f:
            f.write(resp.content)

        # ---------- Format detection ----------
        lower = args.cdn_url.lower()
        if lower.endswith(".parquet"):
            fmt = "parquet"
        elif lower.endswith(".csv"):
            fmt = "csv"
        elif lower.endswith(".json"):
            fmt = "json"
        else:
            ctype = resp.headers.get("Content-Type","").lower()
            if "parquet" in ctype:
                fmt = "parquet"
            elif "csv" in ctype or "text" in ctype:
                fmt = "csv"
            elif "json" in ctype:
                fmt = "json"
            else:
                fmt = None

        # ---------- Load ----------
        try:
            if fmt == "parquet":
                df = pd.read_parquet(tmp_path)
            elif fmt == "csv":
                df = pd.read_csv(tmp_path)
            elif fmt == "json":
                with open(tmp_path) as f:
                    raw = json.load(f)
                for key in ("data","items","results","records"):
                    if isinstance(raw, dict) and key in raw:
                        raw = raw[key]
                        break
                df = pd.json_normalize(raw if isinstance(raw, list) else [raw])
            else:
                raise ValueError("Unsupported dataset format")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

        logger.info("Dataset loaded: shape=%s", df.shape)
        logger.info("Columns: %s", df.columns.tolist())

        # ---------- COLUMN CONTROL ----------
        original_cols = set(df.columns)

        use_cols = (
            [c.strip() for c in args.use_column.split(",") if c.strip()]
            if args.use_column else None
        )

        drop_cols = (
            [c.strip() for c in args.drop_columns.split(",") if c.strip()]
            if args.drop_columns else []
        )

        if use_cols:
            missing = [c for c in use_cols if c not in df.columns]
            if missing:
                raise ValueError(
                    f"use_column contains missing columns: {missing}. "
                    f"Available columns: {list(df.columns)}"
                )
            df = df[use_cols]
            logger.info("use_column applied: %s", use_cols)

        if drop_cols:
            existing = [c for c in drop_cols if c in df.columns]
            df = df.drop(columns=existing, errors="ignore")
            logger.info("drop_columns applied: %s", existing)

        logger.info(
            "Final columns (%d): %s | Removed: %s",
            len(df.columns),
            df.columns.tolist(),
            list(original_cols - set(df.columns))
        )

        # ---------- Target handling ----------
        target_cols = [c.strip() for c in args.target_column.split(",") if c.strip()]
        missing_targets = [c for c in target_cols if c not in df.columns]
        if missing_targets:
            raise ValueError(f"Target columns missing: {missing_targets}")

        target_df = df[target_cols]

        stratify = None
        if args.model_type == "classification" and len(target_cols) == 1:
            if target_df[target_cols[0]].nunique() > 1:
                stratify = target_df[target_cols[0]]

        split = args.split_size / 100 if args.split_size >= 1 else args.split_size
        if not 0 < split < 1:
            raise ValueError("Invalid split_size")

        try:
            train_df, test_df = train_test_split(
                df, test_size=split, random_state=42, stratify=stratify
            )
        except ValueError:
            logger.warning("Stratified split failed, retrying without stratification")
            train_df, test_df = train_test_split(
                df, test_size=split, random_state=42, stratify=None
            )

        # ---------- Save ----------
        os.makedirs(os.path.dirname(args.train_set) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)

        train_df.to_parquet(args.train_set, index=False)
        test_df.to_parquet(args.test_data, index=False)

        logger.info("Finished processing CDN dataset successfully")

    args:
      - --cdn_url
      - {inputValue: cdn_url}
      - --split_size
      - {inputValue: split_size}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --use_column
      - {inputValue: use_column}
      - --drop_columns
      - {inputValue: drop_columns}
      - --train_set
      - {outputPath: train_set}
      - --test_data
      - {outputPath: test_data}
