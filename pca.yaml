name: PCA Transformer v3
inputs:
  - {name: input_data, type: Dataset, description: "Input dataset (CSV, Parquet, or JSONL)"}
  - {name: enable_pca, type: String, description: "Enable/disable PCA transformation (true/false)", optional: true, default: "false"}
  - {name: pca_variant, type: String, description: "PCA variant: pca, kernel, incremental, sparse, fa, ica", optional: true, default: "pca"}
  - {name: n_components, type: String, description: "Number of components or 'auto'", optional: true, default: "auto"}
  - {name: variance_threshold, type: Float, description: "Explained variance threshold for auto components in standard PCA (0â€“1)", optional: true, default: "0.95"}
  - {name: kernel, type: String, description: "Kernel for kernel PCA (e.g. rbf, poly, linear)", optional: true, default: "rbf"}
outputs:
  - {name: transformed_X, type: Dataset, description: "Reduced-dimension dataset parquet"}
  - {name: pca_model, type: Data, description: "Trained PCA model cloudpickle"}
  - {name: pca_metadata, type: Data, description: "PCA metadata JSON (shapes, variance, loadings, etc.)"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, logging
        from datetime import datetime

        import numpy as np
        import pandas as pd
        import cloudpickle

        from sklearn.decomposition import (
            PCA,
            KernelPCA,
            IncrementalPCA,
            SparsePCA,
            FactorAnalysis,
            FastICA
        )


        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        logger = logging.getLogger(__name__)

        def ensure_dir_for(path: str) -> None:
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
                logger.debug(f"Created directory: {d}")

        def load_table(path: str) -> pd.DataFrame:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Input file not found: {path}")
            
            ext = os.path.splitext(path)[1].lower()
            logger.info(f"Loading {ext} file: {path}")
            
            if ext == ".parquet":
                return pd.read_parquet(path)
            elif ext == ".csv":
                return pd.read_csv(path)
            elif ext in (".jsonl", ".ndjson"):
                return pd.read_json(path, lines=True)
            else:
                raise ValueError(f"Unsupported input file format '{ext}'. Only CSV, Parquet, or JSONL are allowed.")

        def clean_and_select_numeric(df: pd.DataFrame) -> pd.DataFrame:
            if df.empty:
                raise ValueError("Input dataset is empty.")
            
            num_df = df.select_dtypes(include=[np.number])
            
            if num_df.shape[1] == 0:
                raise ValueError("No numeric columns found after type filtering. PCA requires numeric features.")
            
            arr = num_df.to_numpy()
            if not np.isfinite(arr).all():
                raise ValueError("Numeric data contains NaNs or infinite values. Please clean the dataset before PCA.")
            
            logger.info(f"Selected {num_df.shape[1]} numeric columns from {df.shape[1]} total columns")
            return num_df

        def parse_n_components(n_str: str, variant: str, p: int, variance_threshold: float,
                               X: pd.DataFrame) -> tuple:
            n_str = str(n_str).strip().lower()
            aux = {}

            if n_str == "auto":
                if variant in ("pca", "standard", "standard_pca"):
                    # Auto for standard PCA: choose smallest k reaching variance_threshold
                    full_k = min(X.shape[0], X.shape[1])
                    logger.info(f"Auto mode: fitting full PCA with {full_k} components to determine optimal k")
                    
                    tmp_pca = PCA(n_components=full_k, svd_solver="full", random_state=42)
                    tmp_pca.fit(X.values)
                    evr = np.asarray(tmp_pca.explained_variance_ratio_, dtype=float)
                    cum = np.cumsum(evr)
                    k = int(np.searchsorted(cum, variance_threshold) + 1)
                    k = max(1, min(k, p))
                    
                    aux["auto_mode"] = "variance_threshold"
                    aux["full_explained_variance"] = tmp_pca.explained_variance_.tolist()
                    aux["full_explained_variance_ratio"] = evr.tolist()
                    aux["full_cumulative_variance_ratio"] = cum.tolist()
                    aux["chosen_components"] = k
                    
                    logger.info(f"Auto selected {k} components to reach {variance_threshold:.2%} variance threshold")
                    return k, aux
                else:
                    # Auto for other PCA variants: 50% of feature count
                    k = int(max(1, min(int(round(0.5 * p)), p)))
                    aux["auto_mode"] = "half_features"
                    aux["chosen_components"] = k
                    
                    logger.info(f"Auto selected {k} components (50% of {p} features)")
                    return k, aux
            else:
                # Manual integer value
                try:
                    n_val = int(float(n_str))
                except Exception:
                    raise ValueError(f"Invalid n_components value '{n_str}'. Use an integer or 'auto'.")
                
                if n_val < 1:
                    raise ValueError("n_components must be >= 1.")
                
                if n_val > p:
                    logger.warning(f"n_components ({n_val}) exceeds feature count ({p}), using {p} instead")
                    n_val = p
                
                aux["auto_mode"] = "manual"
                aux["chosen_components"] = n_val
                return n_val, aux

        def build_pca_model(variant: str, n_components: int, kernel: str):
            variant = str(variant).strip().lower()
            
            if variant in ("pca", "standard", "standard_pca"):
                logger.info(f"Building Standard PCA with {n_components} components")
                return PCA(n_components=n_components, svd_solver="full", random_state=42)
            
            elif variant in ("kernel", "kernel_pca", "kpca"):
                logger.info(f"Building Kernel PCA with {n_components} components and '{kernel}' kernel")
                return KernelPCA(n_components=n_components, kernel=kernel, fit_inverse_transform=False, n_jobs=-1)
            
            elif variant in ("incremental", "incremental_pca", "ipca"):
                logger.info(f"Building Incremental PCA with {n_components} components")
                return IncrementalPCA(n_components=n_components)
            
            elif variant in ("sparse", "sparse_pca", "spca"):
                logger.info(f"Building Sparse PCA with {n_components} components")
                return SparsePCA(n_components=n_components, random_state=42, n_jobs=-1)
            
            elif variant in ("fa", "factor_analysis", "factor"):
                logger.info(f"Building Factor Analysis with {n_components} components")
                return FactorAnalysis(n_components=n_components, random_state=42)
            
            elif variant in ("ica", "fastica"):
                logger.info(f"Building Fast ICA with {n_components} components")
                return FastICA(n_components=n_components, random_state=42)
            
            else:
                raise ValueError(
                    f"Unsupported pca_variant '{variant}'. "
                    f"Valid options: pca, kernel, incremental, sparse, fa, ica."
                )

        def extract_variance_stats(model, variant: str):
            explained_variance = None
            explained_variance_ratio = None
            cumulative_variance_ratio = None
            singular_values = None
            noise_variance = None

            # Standard attributes where available
            if hasattr(model, "explained_variance_"):
                ev = np.asarray(model.explained_variance_, dtype=float)
                explained_variance = ev.tolist()
                if hasattr(model, "explained_variance_ratio_"):
                    evr = np.asarray(model.explained_variance_ratio_, dtype=float)
                    explained_variance_ratio = evr.tolist()
                    cumulative_variance_ratio = np.cumsum(evr).tolist()
            elif isinstance(model, KernelPCA) and hasattr(model, "lambdas_"):
                # Kernel PCA: derive variance ratios from eigenvalues
                lambdas = np.asarray(model.lambdas_, dtype=float)
                total = float(lambdas.sum())
                explained_variance = lambdas.tolist()
                if total > 0.0:
                    evr = lambdas / total
                    explained_variance_ratio = evr.tolist()
                    cumulative_variance_ratio = np.cumsum(evr).tolist()

            if hasattr(model, "singular_values_"):
                singular_values = np.asarray(model.singular_values_, dtype=float).tolist()

            if hasattr(model, "noise_variance_"):
                # FactorAnalysis, probabilistic PCA, etc.
                nv = getattr(model, "noise_variance_")
                if nv is not None:
                    if isinstance(nv, np.ndarray):
                        noise_variance = nv.astype(float).tolist()
                    else:
                        try:
                            noise_variance = float(nv)
                        except Exception:
                            noise_variance = None

            return {
                "explained_variance": explained_variance,
                "explained_variance_ratio": explained_variance_ratio,
                "cumulative_variance_ratio": cumulative_variance_ratio,
                "singular_values": singular_values,
                "noise_variance": noise_variance,
            }

        def compute_top_loadings(model, feature_names, top_n: int = 10):
            if not hasattr(model, "components_"):
                return {}

            comps = np.asarray(model.components_, dtype=float)
            if comps.ndim != 2:
                return {}

            df_comp = pd.DataFrame(comps, columns=list(feature_names))
            top_n = int(max(1, min(top_n, df_comp.shape[1])))
            res = {}
            
            for idx in range(df_comp.shape[0]):
                comp = df_comp.iloc[idx]
                top_feats = comp.abs().sort_values(ascending=False).head(top_n).index
                res[f"PC{idx+1}"] = [
                    {"feature": str(f), "loading": float(comp[f])} for f in top_feats
                ]
            
            return res

        def main():
            parser = argparse.ArgumentParser(description="PCA Transformer v2 - Production")
            parser.add_argument("--input_data", type=str, required=True, help="Path to input dataset")
            parser.add_argument("--enable_pca", type=str, default="false", help="Enable PCA transformation (true/false)")
            parser.add_argument("--pca_variant", type=str, default="pca", help="PCA variant")
            parser.add_argument("--n_components", type=str, default="auto", help="Number of components or 'auto'")
            parser.add_argument("--variance_threshold", type=str, default="0.95", help="Variance threshold for auto mode")
            parser.add_argument("--kernel", type=str, default="rbf", help="Kernel for Kernel PCA")
            parser.add_argument("--output_data", type=str, required=True, help="Path to output dataset")
            parser.add_argument("--model_out", type=str, required=True, help="Path to output model")
            parser.add_argument("--metadata_out", type=str, required=True, help="Path to output metadata")
            args = parser.parse_args()

            try:
                # Parse enable_pca string
                enable_pca_str = str(args.enable_pca).strip().lower()
                enable_pca = enable_pca_str in ("true", "1", "yes")

                logger.info("=" * 80)
                logger.info("PCA TRANSFORMATION BRICK - PRODUCTION v2")
                logger.info("=" * 80)
                logger.info(f"PCA Enabled: {enable_pca} (input: '{args.enable_pca}')")

                logger.info("[STEP 1/6] Loading dataset...")
                df_raw = load_table(args.input_data)
                logger.info(f"Raw dataset shape: {df_raw.shape[0]} rows x {df_raw.shape[1]} columns")

                logger.info("[STEP 2/6] Selecting numeric columns and validating...")
                df_num = clean_and_select_numeric(df_raw)
                logger.info(f"Numeric dataset shape: {df_num.shape[0]} rows x {df_num.shape[1]} columns")

                if not enable_pca:
                    # PCA is disabled - pass through the original input data
                    logger.info("[INFO] PCA is DISABLED. Passing through original input data unchanged.")
                    
                    # Copy input file to output location
                    logger.info("[STEP 3/6] Copying input data to output location...")
                    ensure_dir_for(args.output_data)
                    df_raw.to_parquet(args.output_data, index=False)
                    logger.info(f"Saved original data to: {args.output_data}")
                    
                    # Create empty metadata
                    logger.info("[STEP 4/6] Creating empty metadata...")
                    metadata = {}
                    
                    # Save empty model (None)
                    logger.info("[STEP 5/6] Saving empty model...")
                    ensure_dir_for(args.model_out)
                    with open(args.model_out, "wb") as f:
                        cloudpickle.dump(None, f)
                    logger.info(f"Saved empty model to: {args.model_out}")
                    
                    # Save empty metadata JSON
                    logger.info("[STEP 6/6] Saving empty metadata...")
                    ensure_dir_for(args.metadata_out)
                    with open(args.metadata_out, "w") as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    logger.info(f"Saved empty metadata to: {args.metadata_out}")
                    
                    logger.info("=" * 80)
                    logger.info("PCA BRICK COMPLETE - SUMMARY (PASSTHROUGH MODE)")
                    logger.info("=" * 80)
                    logger.info(f"PCA Enabled:    {enable_pca}")
                    logger.info(f"Input shape:    {df_raw.shape[0]} rows x {df_raw.shape[1]} columns")
                    logger.info(f"Output shape:   {df_raw.shape[0]} rows x {df_raw.shape[1]} columns (unchanged)")
                    logger.info("Outputs:")
                    logger.info(f"  - transformed_X: {args.output_data} (copy of input)")
                    logger.info(f"  - pca_model:     {args.model_out} (empty/None)")
                    logger.info(f"  - pca_metadata:  {args.metadata_out} (empty JSON)")
                    logger.info("=" * 80)
                    logger.info("SUCCESS: Processing finished in passthrough mode.")
                    logger.info("=" * 80)
                    
                else:
                    # PCA is enabled - perform transformation
                    logger.info("[INFO] PCA is ENABLED. Performing transformation...")
                    
                    p = df_num.shape[1]
                    variance_threshold = float(args.variance_threshold)
                    
                    logger.info("[STEP 3/6] Resolving PCA configuration...")
                    n_comp, auto_info = parse_n_components(
                        args.n_components,
                        args.pca_variant,
                        p,
                        variance_threshold,
                        df_num
                    )

                    logger.info(f"PCA variant: {args.pca_variant}")
                    logger.info(f"n_components (resolved): {n_comp}")

                    logger.info("[STEP 4/6] Building and fitting PCA model...")
                    model = build_pca_model(args.pca_variant, n_comp, args.kernel)
                    X_num = df_num.values
                    X_output = model.fit_transform(X_num)
                    logger.info(f"Transformed shape: {X_output.shape[0]} rows x {X_output.shape[1]} components")

                    logger.info("[STEP 5/6] Generating metadata...")
                    variance_stats = extract_variance_stats(model, args.pca_variant)
                    feature_names = list(df_num.columns)
                    top_loadings = compute_top_loadings(model, feature_names, top_n=10)

                    output_columns = [f"PC{i+1}" for i in range(X_output.shape[1])]

                    metadata = {
                        "timestamp": datetime.utcnow().isoformat() + "Z",
                        "pca_enabled": True,
                        "pca_variant": str(args.pca_variant),
                        "kernel": str(args.kernel) if str(args.pca_variant).lower() in ("kernel", "kernel_pca", "kpca") else None,
                        "n_components_input": str(args.n_components),
                        "n_components_resolved": int(X_output.shape[1]),
                        "variance_threshold": variance_threshold if str(args.n_components).strip().lower() == "auto" and str(args.pca_variant).strip().lower() in ("pca", "standard", "standard_pca") else None,
                        "auto_info": auto_info,
                        "original_shape": {
                            "rows": int(df_raw.shape[0]),
                            "cols": int(df_raw.shape[1]),
                        },
                        "numeric_shape": {
                            "rows": int(df_num.shape[0]),
                            "cols": int(df_num.shape[1]),
                        },
                        "transformed_shape": {
                            "rows": int(X_output.shape[0]),
                            "cols": int(X_output.shape[1]),
                        },
                        "feature_names": feature_names,
                        "component_names": output_columns,
                        "variance": variance_stats,
                        "top_contributing_features_per_component": top_loadings,
                    }

                    logger.info("[STEP 6/6] Saving outputs...")
                    # Save output data
                    ensure_dir_for(args.output_data)
                    df_output = pd.DataFrame(X_output, columns=output_columns)
                    df_output.to_parquet(args.output_data, index=False)
                    logger.info(f"Saved transformed data to: {args.output_data}")

                    # Save model (cloudpickle)
                    ensure_dir_for(args.model_out)
                    with open(args.model_out, "wb") as f:
                        cloudpickle.dump(model, f)
                    logger.info(f"Saved PCA model to: {args.model_out}")

                    # Save metadata JSON
                    ensure_dir_for(args.metadata_out)
                    with open(args.metadata_out, "w") as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    logger.info(f"Saved metadata to: {args.metadata_out}")

                    logger.info("=" * 80)
                    logger.info("PCA BRICK COMPLETE - SUMMARY")
                    logger.info("=" * 80)
                    logger.info(f"PCA Enabled:    {enable_pca}")
                    logger.info(f"Original shape: {df_raw.shape[0]} rows x {df_raw.shape[1]} columns")
                    logger.info(f"Numeric shape:  {df_num.shape[0]} rows x {df_num.shape[1]} columns")
                    logger.info(f"Output shape:   {X_output.shape[0]} rows x {X_output.shape[1]} columns")
                    logger.info(f"PCA variant:    {args.pca_variant}")
                    logger.info(f"n_components:   {X_output.shape[1]}")
                    logger.info("Outputs:")
                    logger.info(f"  - transformed_X: {args.output_data}")
                    logger.info(f"  - pca_model:     {args.model_out}")
                    logger.info(f"  - pca_metadata:  {args.metadata_out}")
                    logger.info("=" * 80)
                    logger.info("SUCCESS: Processing finished.")
                    logger.info("=" * 80)

            except Exception as exc:
                logger.error(f"FATAL ERROR: {exc}", exc_info=True)
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --enable_pca
      - {inputValue: enable_pca}
      - --pca_variant
      - {inputValue: pca_variant}
      - --n_components
      - {inputValue: n_components}
      - --variance_threshold
      - {inputValue: variance_threshold}
      - --kernel
      - {inputValue: kernel}
      - --output_data
      - {outputPath: transformed_X}
      - --model_out
      - {outputPath: pca_model}
      - --metadata_out
      - {outputPath: pca_metadata}
