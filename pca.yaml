name: PCA_Transform v3
description: |
  Perform Principal Component Analysis (PCA) or its variants on preprocessed numeric datasets 
  for dimensionality reduction. Automatically determines optimal components based on variance 
  threshold or uses user-specified number. Assumes data is already preprocessed and scaled.

inputs:
  - {name: input_data_path, type: Dataset, description: 'Path to preprocessed input dataset (CSV, Parquet, or JSONL)'}
  - {name: n_components, type: String, default: 'auto', description: 'Number of components (e.g., 2, 5) or "auto" to determine based on variance_threshold'}
  - {name: variance_threshold, type: Float, default: '0.95', description: 'Cumulative variance threshold for auto mode (0.0-1.0)'}
  - {name: pca_variant, type: String, default: 'Standard PCA', description: 'Select PCA variant: Standard PCA | Kernel PCA | Incremental PCA | Sparse PCA | Factor Analysis (FA) | Independent Component Analysis (ICA)'}
  - {name: random_state, type: Integer, default: '42', description: 'Random seed for reproducibility'}

outputs:
  - {name: pca_transformed_data, type: Dataset, description: 'PCA transformed dataset'}
  - {name: pca_model_output, type: Model, description: 'Trained PCA model (pickle file)'}
  - {name: pca_metadata_output, type: String, description: 'PCA metadata and statistics'}

implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import logging
        import os
        import json
        import numpy as np
        import pandas as pd
        import cloudpickle
        from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA, SparsePCA, FactorAnalysis, FastICA

        parser = argparse.ArgumentParser()
        parser.add_argument('--input_data_path', required=True)
        parser.add_argument('--n_components', default='auto')
        parser.add_argument('--variance_threshold', type=float, default=0.95)
        parser.add_argument('--pca_variant', default='Standard PCA')
        parser.add_argument('--random_state', type=int, default=42)
        parser.add_argument('--pca_transformed_data', required=True)
        parser.add_argument('--pca_model_output', required=True)
        parser.add_argument('--pca_metadata_output', required=True)
        args = parser.parse_args()

        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        log = logging.getLogger("pca_transform")

        def load_dataset(path):
            log.info(f"Loading preprocessed dataset from: {path}")
            
            if path.endswith('.csv'):
                df = pd.read_csv(path)
            elif path.endswith('.parquet'):
                df = pd.read_parquet(path)
            elif path.endswith('.json') or path.endswith('.jsonl'):
                df = pd.read_json(path, lines=True)
            else:
                raise ValueError(f"Unsupported file format. Supported: .csv, .parquet, .json, .jsonl")
            
            log.info(f"Dataset loaded successfully. Shape: {df.shape}")
            return df

        df = load_dataset(args.input_data_path.strip())
        original_shape = df.shape

        log.info("Validating preprocessed data...")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        X = df[numeric_cols].values
        
        if len(numeric_cols) == 0:
            raise ValueError("No numeric columns found in dataset. PCA requires numeric features.")
        
        log.info(f"Using {len(numeric_cols)} numeric columns for PCA")
        log.info(f"Feature names: {numeric_cols[:10]}{'...' if len(numeric_cols) > 10 else ''}")

        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            raise ValueError("Data contains NaN or infinite values. Please ensure data is fully preprocessed.")

        variant = args.pca_variant.strip()
        n_comp_input = args.n_components.strip().lower()
        
        log.info(f"PCA Configuration:")
        log.info(f"  Variant: {variant}")
        log.info(f"  Component Selection: {n_comp_input}")
        
        if n_comp_input == 'auto':
            if variant == 'Standard PCA':
                log.info(f"  Auto mode: Finding components for {args.variance_threshold*100:.1f}% variance...")
                pca_temp = PCA(n_components=args.variance_threshold, random_state=args.random_state)
                pca_temp.fit(X)
                n_components = pca_temp.n_components_
                log.info(f"  ✓ Auto selected {n_components} components")
            else:
                n_components = min(X.shape[1], max(2, int(X.shape[1] * 0.5)))
                log.info(f"  Auto mode: Using {n_components} components for {variant}")
        else:
            try:
                n_components = int(n_comp_input)
                if n_components < 1:
                    raise ValueError("n_components must be positive")
                if n_components > X.shape[1]:
                    log.warning(f"n_components ({n_components}) > n_features ({X.shape[1]}). Using {X.shape[1]}")
                    n_components = X.shape[1]
                log.info(f"  Using specified n_components: {n_components}")
            except ValueError:
                raise ValueError(f"Invalid n_components: '{n_comp_input}'. Use 'auto' or a positive integer.")

        log.info(f"Initializing {variant}...")
        
        if variant == 'Standard PCA':
            pca_model = PCA(n_components=n_components, random_state=args.random_state)
        elif variant == 'Kernel PCA':
            pca_model = KernelPCA(n_components=n_components, kernel='rbf', random_state=args.random_state)
        elif variant == 'Incremental PCA':
            pca_model = IncrementalPCA(n_components=n_components)
        elif variant == 'Sparse PCA':
            pca_model = SparsePCA(n_components=n_components, random_state=args.random_state)
        elif variant == 'Factor Analysis (FA)':
            pca_model = FactorAnalysis(n_components=n_components, random_state=args.random_state)
        elif variant == 'Independent Component Analysis (ICA)':
            pca_model = FastICA(n_components=n_components, random_state=args.random_state)
        else:
            raise ValueError(f"Unknown PCA variant: {variant}")

        log.info("Fitting and transforming data...")
        X_transformed = pca_model.fit_transform(X)
        
        component_names = [f'PC{i+1}' for i in range(X_transformed.shape[1])]
        df_transformed = pd.DataFrame(X_transformed, columns=component_names)
        
        log.info(f"✓ Transformation complete: {original_shape} → {df_transformed.shape}")

        metadata = {
            'variant': variant,
            'n_components': n_components,
            'n_components_input': args.n_components,
            'variance_threshold': args.variance_threshold,
            'original_shape': list(original_shape),
            'transformed_shape': list(df_transformed.shape),
            'feature_names': numeric_cols,
            'component_names': component_names,
            'n_features': len(numeric_cols),
            'random_state': args.random_state
        }
        
        if hasattr(pca_model, 'explained_variance_ratio_'):
            explained_var = pca_model.explained_variance_ratio_
            cumulative_var = np.cumsum(explained_var)
            
            metadata['explained_variance_ratio'] = explained_var.tolist()
            metadata['cumulative_variance'] = cumulative_var.tolist()
            metadata['total_variance_explained'] = float(np.sum(explained_var))
            
            log.info("="*70)
            log.info("EXPLAINED VARIANCE BY COMPONENT")
            log.info("="*70)
            for i, (var, cumvar) in enumerate(zip(explained_var, cumulative_var)):
                log.info(f"  PC{i+1:2d}: {var*100:6.2f}%  |  Cumulative: {cumvar*100:6.2f}%")
            log.info("-"*70)
            log.info(f"  Total Variance Explained: {np.sum(explained_var)*100:.2f}%")
            log.info("="*70)
        
        if hasattr(pca_model, 'components_'):
            components = pca_model.components_
            metadata['component_loadings'] = {}
            
            log.info("Top Contributing Features per Component:")
            log.info("-"*70)
            for i, component in enumerate(components[:5]):
                top_indices = np.argsort(np.abs(component))[-5:][::-1]
                top_features = [(numeric_cols[idx], float(component[idx])) for idx in top_indices]
                metadata['component_loadings'][f'PC{i+1}'] = top_features
                
                log.info(f"  PC{i+1}:")
                for feat, load in top_features[:3]:
                    log.info(f"    {feat}: {load:+.4f}")
            if len(components) > 5:
                log.info(f"  ... and {len(components)-5} more components")
        
        if hasattr(pca_model, 'singular_values_'):
            metadata['singular_values'] = pca_model.singular_values_.tolist()
        
        if hasattr(pca_model, 'noise_variance_'):
            metadata['noise_variance'] = float(pca_model.noise_variance_)

        log.info("Saving outputs...")
        
        os.makedirs(os.path.dirname(args.pca_transformed_data) or '.', exist_ok=True)
        df_transformed.to_parquet(args.pca_transformed_data, index=False)
        log.info(f"✓ Transformed data saved: {args.pca_transformed_data}")
        
        model_data = {
            'pca_model': pca_model,
            'feature_names': numeric_cols,
            'component_names': component_names,
            'variant': variant,
            'n_components': n_components
        }
        
        os.makedirs(os.path.dirname(args.pca_model_output) or '.', exist_ok=True)
        with open(args.pca_model_output, 'wb') as f:
            cloudpickle.dump(model_data, f)
        log.info(f"✓ Model saved: {args.pca_model_output}")
        
        os.makedirs(os.path.dirname(args.pca_metadata_output) or '.', exist_ok=True)
        with open(args.pca_metadata_output, 'w') as f:
            json.dump(metadata, f, indent=2)
        log.info(f"✓ Metadata saved: {args.pca_metadata_output}")
        
        log.info("="*70)
        log.info("PCA TRANSFORMATION COMPLETED SUCCESSFULLY")
        log.info("="*70)

    args:
      - --input_data_path
      - {inputPath: input_data_path}
      - --n_components
      - {inputValue: n_components}
      - --variance_threshold
      - {inputValue: variance_threshold}
      - --pca_variant
      - {inputValue: pca_variant}
      - --random_state
      - {inputValue: random_state}
      - --pca_transformed_data
      - {outputPath: pca_transformed_data}
      - --pca_model_output
      - {outputPath: pca_model_output}
      - --pca_metadata_output
      - {outputPath: pca_metadata_output}
