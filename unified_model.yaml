name: Convert to Unified Pipeline v1.0
description: "Converts separate pickle files (preprocessor, feature_selector, pca, model) into a single unified pipeline"
inputs:
  - {name: preprocessor, type: Data, description: "Fitted Preprocessor pickle file"}
  - {name: feature_selector, type: Data, description: "Fitted FeatureSelector pickle file", optional: true}
  - {name: pca, type: Data, description: "Fitted PCA pickle file", optional: true}
  - {name: model_pickle, type: Model, description: "Trained model pickle file"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata JSON file"}
  - {name: model_type, type: String, description: "classification or regression"}
outputs:
  - {name: unified_pipeline, type: Model, description: "Single unified pipeline pickle file (all-in-one)"}
  - {name: pipeline_metadata, type: Data, description: "Pipeline metadata JSON"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, gzip, traceback
        import pandas as pd, numpy as np
        import joblib
        from sklearn.base import BaseEstimator, TransformerMixin
        from sklearn.pipeline import Pipeline
        from datetime import datetime

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def load_pickle_any(path):
            if not os.path.exists(path):
                return None
            
            # Check file size
            file_size = os.path.getsize(path)
            if file_size == 0:
                print(f"Warning: {path} is empty (0 bytes)")
                return None
            
            print(f"Loading {path} ({file_size / 1024:.2f} KB)...")
            
            # Try joblib first (most common)
            try:
                obj = joblib.load(path)
                print(f"  ✓ Loaded with joblib: {type(obj)}")
                return obj
            except Exception as e:
                print(f"  ✗ joblib failed: {e}")
            
            # Try gzipped cloudpickle
            try:
                with gzip.open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  ✓ Loaded with gzipped cloudpickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ gzipped cloudpickle failed: {e}")
            
            # Try regular cloudpickle
            try:
                with open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  ✓ Loaded with cloudpickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ cloudpickle failed: {e}")
            
            # Try regular pickle
            try:
                import pickle
                with open(path, "rb") as f:
                    obj = pickle.load(f)
                    print(f"  ✓ Loaded with pickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ pickle failed: {e}")
            
            raise ValueError(f"Could not load pickle from {path} with any method")

        # ============================================================================
        # WRAPPER CLASSES - Make components sklearn-compatible
        # ============================================================================

        class PreprocessorWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, preprocessor, target_column=None):
                self.preprocessor = preprocessor
                self.target_column = target_column
                print(f"    PreprocessorWrapper initialized (target: {target_column})")
            
            def fit(self, X, y=None):
                # Already fitted during training, just return self
                return self
            
            def transform(self, X):
                print(f"    Preprocessing: input shape {X.shape}")
                
                # If X is just features, add dummy target for preprocessing
                if self.target_column and self.target_column not in X.columns:
                    X_with_target = X.copy()
                    X_with_target[self.target_column] = 0  # Dummy target for transform
                    
                    try:
                        transformed = self.preprocessor.transform(X_with_target, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X_with_target)
                    
                    # Remove target if it's in output
                    if isinstance(transformed, pd.DataFrame) and self.target_column in transformed.columns:
                        transformed = transformed.drop(columns=[self.target_column])
                else:
                    try:
                        transformed = self.preprocessor.transform(X, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X)
                
                # Ensure DataFrame output
                if not isinstance(transformed, pd.DataFrame):
                    try:
                        cols = (getattr(self.preprocessor, "feature_names_out", None) or 
                               getattr(self.preprocessor, "output_columns", None) or
                               [f"f{i}" for i in range(transformed.shape[1])])
                        transformed = pd.DataFrame(transformed, columns=list(cols))
                    except Exception:
                        transformed = pd.DataFrame(transformed)
                
                print(f"    Preprocessing: output shape {transformed.shape}")
                return transformed

        class FeatureSelectorWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, feature_selector):
                self.feature_selector = feature_selector
                self.selected_features = getattr(feature_selector, 'selected_features', [])
                print(f"    FeatureSelectorWrapper initialized ({len(self.selected_features)} features)")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    Feature Selection: input shape {X.shape}")
                
                if hasattr(self.feature_selector, 'transform'):
                    result = self.feature_selector.transform(X)
                else:
                    # Manual selection using feature list
                    if isinstance(X, pd.DataFrame):
                        result = X.reindex(columns=self.selected_features, fill_value=0.0)
                    else:
                        result = X
                
                print(f"    Feature Selection: output shape {result.shape}")
                return result

        class PCAWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, pca_transformer):
                self.pca_transformer = pca_transformer
                print(f"    PCAWrapper initialized: {type(pca_transformer)}")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    PCA: input shape {X.shape}")
                
                # Extract transformer from Pipeline if needed
                from sklearn.pipeline import Pipeline as SKPipeline
                pca_obj = self.pca_transformer
                
                if isinstance(pca_obj, SKPipeline):
                    print("    PCA: Extracting from sklearn Pipeline...")
                    for nm, step in reversed(pca_obj.steps):
                        if hasattr(step, "transform"):
                            pca_obj = step
                            print(f"    PCA: Using step '{nm}': {type(step)}")
                            break
                
                # Apply transformation
                X_arr = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)
                X_pca = pca_obj.transform(X_arr)
                
                if X_pca.ndim == 1:
                    X_pca = X_pca.reshape(-1, 1)
                
                # Create DataFrame with PC names
                ncomp = X_pca.shape[1]
                pca_cols = [f"PC{i+1}" for i in range(ncomp)]
                result = pd.DataFrame(X_pca, columns=pca_cols)
                
                print(f"    PCA: output shape {result.shape}")
                return result

        class ModelWrapper(BaseEstimator):            
            def __init__(self, model):
                self.model = model
                print(f"    ModelWrapper initialized: {type(model)}")
            
            def fit(self, X, y=None):
                return self
            
            def predict(self, X):
                X_arr = X.values if isinstance(X, pd.DataFrame) else X
                predictions = self.model.predict(X_arr)
                print(f"    Model Prediction: {len(predictions)} predictions made")
                return predictions
            
            def predict_proba(self, X):
                if hasattr(self.model, 'predict_proba'):
                    X_arr = X.values if isinstance(X, pd.DataFrame) else X
                    return self.model.predict_proba(X_arr)
                else:
                    raise AttributeError("Model does not support predict_proba")

        # ============================================================================
        # MAIN EXECUTION
        # ============================================================================

        def main():
            parser = argparse.ArgumentParser(description="Convert separate pickles to unified pipeline")
            parser.add_argument('--preprocessor', type=str, required=True)
            parser.add_argument('--feature_selector', type=str, default="")
            parser.add_argument('--pca', type=str, default="")
            parser.add_argument('--model_pickle', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--model_type', type=str, required=True)
            parser.add_argument('--unified_pipeline', type=str, required=True)
            parser.add_argument('--pipeline_metadata', type=str, required=True)
            args = parser.parse_args()

            try:
                print("=" * 80)
                print("CONVERTING TO UNIFIED PIPELINE")
                print("=" * 80)
                print(f"Model Type: {args.model_type}")
                print(f"Output: {args.unified_pipeline}")
                print("")

                # Load metadata
                print("[STEP 1/6] Loading preprocessing metadata...")
                with open(args.preprocess_metadata, 'r') as f:
                    metadata = json.load(f)
                
                target_col = metadata.get('target_columns', ['target'])[0]
                print(f"  Target column: {target_col}")
                print(f"  Metadata keys: {list(metadata.keys())}")

                # Load preprocessor (REQUIRED)
                print("[STEP 2/6] Loading preprocessor...")
                preprocessor = load_pickle_any(args.preprocessor)
                if preprocessor is None:
                    raise ValueError("Preprocessor is required but could not be loaded")
                print(f"  ✓ Preprocessor loaded successfully")

                # Load feature selector (OPTIONAL)
                print("[STEP 3/6] Loading feature selector...")
                feature_selector = None
                if args.feature_selector and os.path.exists(args.feature_selector):
                    try:
                        feature_selector = load_pickle_any(args.feature_selector)
                        if feature_selector is not None:
                            if hasattr(feature_selector, 'selected_features'):
                                n_features = len(feature_selector.selected_features)
                                print(f"  ✓ Feature selector loaded ({n_features} features)")
                            else:
                                print(f"  ✓ Feature selector loaded (no selected_features attr)")
                        else:
                            print(f"  ⚠ Feature selector is None, skipping")
                    except Exception as e:
                        print(f"  ⚠ Could not load feature selector: {e}")
                else:
                    print("  ⊗ No feature selector provided")

                # Load PCA (OPTIONAL)
                print("[STEP 4/6] Loading PCA...")
                pca_transformer = None
                if args.pca and os.path.exists(args.pca):
                    file_size = os.path.getsize(args.pca)
                    if file_size > 0:
                        try:
                            pca_transformer = load_pickle_any(args.pca)
                            if pca_transformer is not None:
                                print(f"  ✓ PCA loaded successfully")
                            else:
                                print(f"  ⚠ PCA is None, skipping")
                        except Exception as e:
                            print(f"  ⚠ Could not load PCA: {e}")
                    else:
                        print(f"  ⊗ PCA file is empty (0 bytes), skipping")
                else:
                    print("  ⊗ No PCA provided")

                # Load model (REQUIRED)
                print("[STEP 5/6] Loading model...")
                model = load_pickle_any(args.model_pickle)
                if model is None:
                    raise ValueError("Model is required but could not be loaded")
                print(f"  ✓ Model loaded successfully")

                # Build unified pipeline
                print("[STEP 6/6] Building unified pipeline...")
                print("-" * 80)
                
                pipeline_steps = []
                
                # Step 1: Preprocessor (REQUIRED)
                pipeline_steps.append(
                    ('preprocessor', PreprocessorWrapper(preprocessor, target_column=target_col))
                )
                print("  [1] Preprocessor ✓")
                
                # Step 2: Feature Selector (OPTIONAL)
                if feature_selector is not None:
                    pipeline_steps.append(
                        ('feature_selector', FeatureSelectorWrapper(feature_selector))
                    )
                    print("  [2] Feature Selector ✓")
                else:
                    print("  [2] Feature Selector ⊗ (skipped)")
                
                # Step 3: PCA (OPTIONAL)
                if pca_transformer is not None:
                    pipeline_steps.append(
                        ('pca', PCAWrapper(pca_transformer))
                    )
                    print("  [3] PCA ✓")
                else:
                    print("  [3] PCA ⊗ (skipped)")
                
                # Step 4: Model (REQUIRED)
                pipeline_steps.append(
                    ('model', ModelWrapper(model))
                )
                print("  [4] Model ✓")

                print("-" * 80)
                print(f"Pipeline architecture ({len(pipeline_steps)} steps):")
                for i, (step_name, _) in enumerate(pipeline_steps, 1):
                    print(f"  {i}. {step_name}")

                # Create sklearn Pipeline
                unified_pipeline = Pipeline(pipeline_steps)
                print(f" Unified Pipeline object created: {type(unified_pipeline)}")

                # Save unified pipeline
                print(f"Saving unified pipeline to: {args.unified_pipeline}")
                ensure_dir_for(args.unified_pipeline)
                joblib.dump(unified_pipeline, args.unified_pipeline, compress=3)
                
                file_size = os.path.getsize(args.unified_pipeline)
                print(f"✓ Pipeline saved successfully ({file_size / 1024 / 1024:.2f} MB)")

                # Create pipeline metadata
                pipeline_meta = {
                    'created_at': datetime.utcnow().isoformat() + 'Z',
                    'model_type': args.model_type,
                    'target_column': target_col,
                    'pipeline_steps': [name for name, _ in pipeline_steps],
                    'has_preprocessor': True,
                    'has_feature_selector': feature_selector is not None,
                    'has_pca': pca_transformer is not None,
                    'has_model': True,
                    'n_features_selected': len(feature_selector.selected_features) if feature_selector and hasattr(feature_selector, 'selected_features') else None,
                    'original_metadata': metadata,
                    'file_size_mb': round(file_size / 1024 / 1024, 2)
                }

                print(f"Saving pipeline metadata to: {args.pipeline_metadata}")
                ensure_dir_for(args.pipeline_metadata)
                with open(args.pipeline_metadata, 'w') as f:
                    json.dump(pipeline_meta, f, indent=2)
                print(f"✓ Metadata saved")

                # Summary
                print("=" * 80)
                print("SUCCESS: UNIFIED PIPELINE CREATED!")
                print("=" * 80)
                print(f"Pipeline file: {args.unified_pipeline}")
                print(f"Metadata file: {args.pipeline_metadata}")
                print(f"File size: {file_size / 1024 / 1024:.2f} MB")
                print(f Pipeline flow: {' → '.join([name for name, _ in pipeline_steps])}")
                print(" Usage Example:")
                print("   import joblib")
                print("   import pandas as pd")
                print("")
                print(f"   # Load the unified pipeline")
                print(f"   pipeline = joblib.load('{os.path.basename(args.unified_pipeline)}')")
                print("")
                print(f"   # Make predictions on raw data (with '{target_col}' column)")
                print("   raw_data = pd.read_csv('your_data.csv')")
                print(f"   X_test = raw_data.drop(columns=['{target_col}'])  # Remove target")
                print("   predictions = pipeline.predict(X_test)")
                print("")
                print("   # That's it! No need to manually preprocess, select features, or apply PCA!")
                print("=" * 80)

            except Exception as e:
                print("=" * 80)
                print("ERROR: CONVERSION FAILED")
                print("=" * 80)
                print(f"Error: {e}")
                traceback.print_exc()
                print("=" * 80)
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca
      - {inputPath: pca}
      - --model_pickle
      - {inputPath: model_pickle}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_type
      - {inputValue: model_type}
      - --unified_pipeline
      - {outputPath: unified_pipeline}
      - --pipeline_metadata
      - {outputPath: pipeline_metadata}
