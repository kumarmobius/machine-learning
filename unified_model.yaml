name: Convert to Unified Pipeline v3.0
description: "Converts separate components into a single all-in-one pipeline that handles raw data"
inputs:
  - {name: cleaning_metadata, type: Data, description: "JSON metadata from Data Cleaning brick"}  # NEW
  - {name: preprocessor, type: Data, description: "Fitted Preprocessor pickle file"}
  - {name: feature_selector, type: Data, description: "Fitted FeatureSelector pickle file", optional: true}
  - {name: pca, type: Data, description: "Fitted PCA pickle file", optional: true}
  - {name: model_pickle, type: Model, description: "Trained model pickle file"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata JSON file"}
  - {name: model_type, type: String, description: "classification or regression"}
outputs:
  - {name: unified_pipeline, type: Model, description: "Single unified pipeline (raw data â†’ predictions)"}
  - {name: pipeline_metadata, type: Data, description: "Pipeline metadata JSON"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, gzip, traceback, re
        import pandas as pd, numpy as np
        import joblib
        from sklearn.base import BaseEstimator, TransformerMixin
        from sklearn.pipeline import Pipeline
        from datetime import datetime

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def load_pickle_any(path):
            if not os.path.exists(path):
                return None
            file_size = os.path.getsize(path)
            if file_size == 0:
                print(f"Warning: {path} is empty (0 bytes)")
                return None
            print(f"Loading {path} ({file_size / 1024:.2f} KB)...")
            try:
                obj = joblib.load(path)
                print(f"  âœ“ Loaded with joblib: {type(obj)}")
                return obj
            except Exception:
                pass
            try:
                with gzip.open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  âœ“ Loaded with gzipped cloudpickle: {type(obj)}")
                    return obj
            except Exception:
                pass
            try:
                with open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  âœ“ Loaded with cloudpickle: {type(obj)}")
                    return obj
            except Exception:
                pass
            raise ValueError(f"Could not load pickle from {path}")

        class DataCleaningWrapper(BaseEstimator, TransformerMixin):
            def __init__(self, cleaning_metadata, target_column=None):
                self.cleaning_metadata = cleaning_metadata
                self.target_column = target_column
                self.expected_columns = cleaning_metadata.get('column_names', [])
                self.date_report = cleaning_metadata.get('date_report', {})
                self.conversion_report = cleaning_metadata.get('conversion_report', {})
                
                print(f"    DataCleaningWrapper initialized")
                print(f"    Target column: {target_column}")
                print(f"    Expected output: {len(self.expected_columns)} columns")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    [Cleaning] Input: {X.shape}")
                
                df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)
                
                # Remove target if present
                target_cols = [self.target_column] if self.target_column else []
                if self.target_column and self.target_column in df.columns:
                    df = df.drop(columns=[self.target_column])
                    print(f"    [Cleaning] Removed target column '{self.target_column}'")
                
                # Step 1: Detect and deduplicate dates
                df = self._detect_and_deduplicate_dates(df, exclude_cols=target_cols)
                
                # Step 2: Convert object to numeric
                df = self._convert_object_columns(df)
                
                # Step 3: Drop _orig columns
                orig_cols = [c for c in df.columns if c.endswith('_orig')]
                if orig_cols:
                    df = df.drop(columns=orig_cols)
                    print(f"    [Cleaning] Dropped {len(orig_cols)} _orig columns")
                
                # Step 4: Align with training columns
                current_cols = set(df.columns)
                expected_cols = set(self.expected_columns)
                
                missing_cols = expected_cols - current_cols
                extra_cols = current_cols - expected_cols
                
                if missing_cols:
                    print(f"    [Cleaning] Adding {len(missing_cols)} missing columns")
                    for col in missing_cols:
                        df[col] = 0.0
                
                if extra_cols:
                    print(f"    [Cleaning] Dropping {len(extra_cols)} extra columns")
                    df = df.drop(columns=list(extra_cols))
                
                # Reorder to match training
                df = df[self.expected_columns]
                
                print(f"    [Cleaning] Output: {df.shape}")
                return df
            
            def _detect_and_deduplicate_dates(self, df, exclude_cols=None):
                exclude_cols = set(exclude_cols or [])
                cand = [c for c in df.columns if c not in exclude_cols and 
                        (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
                
                parsed_dates = {}
                for col in cand:
                    ser = df[col].astype(object)
                    sample = ser.dropna().astype(str).head(500)
                    if sample.empty:
                        continue
                    
                    try:
                        parsed = pd.to_datetime(sample, errors='coerce')
                        frac = parsed.notna().mean()
                    except Exception:
                        continue
                    
                    if frac >= 0.6:
                        try:
                            full = pd.to_datetime(ser, errors='coerce')
                            if not pd.api.types.is_datetime64_any_dtype(full) or full.notna().sum() == 0:
                                continue
                            parsed_dates[col] = {
                                'datetime': full,
                                'granularity': self._get_date_granularity(full, col)
                            }
                        except Exception:
                            continue
                
                if not parsed_dates:
                    return df
                
                # Group duplicates
                date_groups = []
                processed = set()
                
                for col1 in parsed_dates.keys():
                    if col1 in processed:
                        continue
                    group = [col1]
                    dt1 = parsed_dates[col1]['datetime']
                    
                    for col2 in parsed_dates.keys():
                        if col2 == col1 or col2 in processed:
                            continue
                        dt2 = parsed_dates[col2]['datetime']
                        if self._are_dates_duplicate(dt1, dt2):
                            group.append(col2)
                            processed.add(col2)
                    
                    processed.add(col1)
                    date_groups.append(group)
                
                # Keep best from each group
                cols_to_drop = []
                for group in date_groups:
                    if len(group) > 1:
                        best_col = max(group, key=lambda c: parsed_dates[c]['granularity'])
                        duplicates = [c for c in group if c != best_col]
                        cols_to_drop.extend(duplicates)
                        print(f"    [Cleaning] Date dedup: keeping '{best_col}', dropping {duplicates}")
                
                if cols_to_drop:
                    df = df.drop(columns=cols_to_drop)
                    for col in cols_to_drop:
                        del parsed_dates[col]
                
                # Extract date features
                for col, data in parsed_dates.items():
                    full = data['datetime']
                    try:
                        df[col + "_orig"] = df[col]
                        df[col] = full
                        df[col + "_year"] = df[col].dt.year.astype('Int64')
                        df[col + "_month"] = df[col].dt.month.astype('Int64')
                        df[col + "_day"] = df[col].dt.day.astype('Int64')
                        df[col + "_dayofweek"] = df[col].dt.dayofweek.astype('Int64')
                        df[col + "_quarter"] = df[col].dt.quarter.astype('Int64')
                        df[col + "_month_sin"] = np.sin(2 * np.pi * df[col].dt.month / 12)
                        df[col + "_month_cos"] = np.cos(2 * np.pi * df[col].dt.month / 12)
                        df[col + "_day_sin"] = np.sin(2 * np.pi * df[col].dt.dayofweek / 7)
                        df[col + "_day_cos"] = np.cos(2 * np.pi * df[col].dt.dayofweek / 7)
                        df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5, 6]).astype('Int64')
                        df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                        print(f"    [Cleaning] Extracted date features from '{col}'")
                    except Exception as e:
                        print(f"    [Cleaning] Failed to extract from '{col}': {e}")
                
                return df
            
            def _get_date_granularity(self, dt_series, col_name):
                valid_dates = dt_series.dropna()
                if len(valid_dates) > 0:
                    has_time = (valid_dates.dt.hour != 0).any() or (valid_dates.dt.minute != 0).any()
                    if has_time:
                        return 3
                    has_day_variation = valid_dates.dt.day.nunique() > 1
                    return 2 if has_day_variation else 1
                col_lower = col_name.lower()
                if 'timestamp' in col_lower:
                    return 3
                elif 'date' in col_lower:
                    return 2
                return 1
            
            def _are_dates_duplicate(self, dt1, dt2):
                valid_mask = dt1.notna() & dt2.notna()
                if valid_mask.sum() == 0:
                    return False
                dt1_valid = dt1[valid_mask]
                dt2_valid = dt2[valid_mask]
                match_rate = (
                    (dt1_valid.dt.year == dt2_valid.dt.year) &
                    (dt1_valid.dt.month == dt2_valid.dt.month) &
                    (dt1_valid.dt.day == dt2_valid.dt.day)
                ).mean()
                return match_rate >= 0.95
            
            def _convert_object_columns(self, df):
                CURRENCY_SYMBOLS_REGEX = r'[â‚¬Â£Â¥â‚¹$Â¢à¸¿â‚ªâ‚©â‚«â‚½â‚º]'
                
                def parse_alphanumeric_to_numeric(s):
                    if pd.isna(s):
                        return np.nan
                    orig = str(s).strip()
                    if orig == '' or orig.lower() in {'nan', 'none', 'null', 'na'}:
                        return np.nan
                    tmp = re.sub(CURRENCY_SYMBOLS_REGEX, '', orig)
                    if tmp.strip().endswith('%'):
                        try:
                            return float(tmp.strip().rstrip('%').replace(',', '').replace(' ', '')) / 100.0
                        except Exception:
                            pass
                    tmp = tmp.replace(',', '').replace(' ', '')
                    try:
                        return float(tmp)
                    except Exception:
                        return np.nan
                
                obj_cols = [c for c in df.columns if df[c].dtype == 'object' or str(df[c].dtype).startswith('string')]
                converted_count = 0
                
                for col in obj_cols:
                    ser = df[col].astype(object)
                    total_non_null = ser.notna().sum()
                    if total_non_null == 0:
                        continue
                    
                    parsed = ser.map(parse_alphanumeric_to_numeric)
                    frac = parsed.notna().sum() / float(total_non_null)
                    
                    if frac >= 0.6:
                        df[col + "_orig"] = df[col]
                        df[col] = parsed
                        converted_count += 1
                
                if converted_count > 0:
                    print(f"    [Cleaning] Converted {converted_count} columns to numeric")
                
                return df

        # ============================================================================
        # OTHER WRAPPER CLASSES
        # ============================================================================

        class PreprocessorWrapper(BaseEstimator, TransformerMixin):
            def __init__(self, preprocessor, target_column=None):
                self.preprocessor = preprocessor
                self.target_column = target_column
                print(f"    PreprocessorWrapper initialized (target: {target_column})")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    [Preprocessing] Input: {X.shape}")
                
                if self.target_column and self.target_column not in X.columns:
                    X_with_target = X.copy()
                    X_with_target[self.target_column] = 0
                    try:
                        transformed = self.preprocessor.transform(X_with_target, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X_with_target)
                    if isinstance(transformed, pd.DataFrame) and self.target_column in transformed.columns:
                        transformed = transformed.drop(columns=[self.target_column])
                else:
                    try:
                        transformed = self.preprocessor.transform(X, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X)
                
                if not isinstance(transformed, pd.DataFrame):
                    try:
                        cols = (getattr(self.preprocessor, "feature_names_out", None) or 
                               getattr(self.preprocessor, "output_columns", None) or
                               [f"f{i}" for i in range(transformed.shape[1])])
                        transformed = pd.DataFrame(transformed, columns=list(cols))
                    except Exception:
                        transformed = pd.DataFrame(transformed)
                
                print(f"    [Preprocessing] Output: {transformed.shape}")
                return transformed

        class FeatureSelectorWrapper(BaseEstimator, TransformerMixin):
            def __init__(self, feature_selector):
                self.feature_selector = feature_selector
                self.selected_features = getattr(feature_selector, 'selected_features', [])
                print(f"    FeatureSelectorWrapper initialized ({len(self.selected_features)} features)")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    [Feature Selection] Input: {X.shape}")
                if hasattr(self.feature_selector, 'transform'):
                    result = self.feature_selector.transform(X)
                else:
                    if isinstance(X, pd.DataFrame):
                        result = X.reindex(columns=self.selected_features, fill_value=0.0)
                    else:
                        result = X
                print(f"    [Feature Selection] Output: {result.shape}")
                return result

        class PCAWrapper(BaseEstimator, TransformerMixin):
            def __init__(self, pca_transformer):
                self.pca_transformer = pca_transformer
                print(f"    PCAWrapper initialized: {type(pca_transformer)}")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    [PCA] Input: {X.shape}")
                from sklearn.pipeline import Pipeline as SKPipeline
                pca_obj = self.pca_transformer
                if isinstance(pca_obj, SKPipeline):
                    for nm, step in reversed(pca_obj.steps):
                        if hasattr(step, "transform"):
                            pca_obj = step
                            break
                X_arr = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)
                X_pca = pca_obj.transform(X_arr)
                if X_pca.ndim == 1:
                    X_pca = X_pca.reshape(-1, 1)
                ncomp = X_pca.shape[1]
                pca_cols = [f"PC{i+1}" for i in range(ncomp)]
                result = pd.DataFrame(X_pca, columns=pca_cols)
                print(f"    [PCA] Output: {result.shape}")
                return result

        class ModelWrapper(BaseEstimator):
            def __init__(self, model):
                self.model = model
                self.is_dict = isinstance(model, dict)
                print(f"    ModelWrapper initialized: {type(model)}")
                if self.is_dict:
                    print(f"    Dict model with {len(model)} targets: {list(model.keys())}")
            
            def fit(self, X, y=None):
                return self
            
            def predict(self, X):
                X_arr = X.values if isinstance(X, pd.DataFrame) else X
                if self.is_dict:
                    predictions = {}
                    for target_name, estimator in self.model.items():
                        predictions[target_name] = estimator.predict(X_arr)
                    if len(predictions) == 1:
                        return list(predictions.values())[0]
                    return np.column_stack(list(predictions.values()))
                else:
                    return self.model.predict(X_arr)

        # ============================================================================
        # MAIN
        # ============================================================================

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--cleaning_metadata', type=str, required=True)  # NEW
            parser.add_argument('--preprocessor', type=str, required=True)
            parser.add_argument('--feature_selector', type=str, default="")
            parser.add_argument('--pca', type=str, default="")
            parser.add_argument('--model_pickle', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--model_type', type=str, required=True)
            parser.add_argument('--unified_pipeline', type=str, required=True)
            parser.add_argument('--pipeline_metadata', type=str, required=True)
            args = parser.parse_args()

            try:
                print("=" * 80)
                print("CONVERTING TO UNIFIED PIPELINE v3.0")
                print("=" * 80)
                print(f"Model Type: {args.model_type}")

                # Load cleaning metadata
                print("[STEP 1/7] Loading cleaning metadata...")
                with open(args.cleaning_metadata, 'r') as f:
                    cleaning_meta = json.load(f)
                print(f"  âœ“ Loaded ({len(cleaning_meta.get('column_names', []))} expected columns)")

                # Load preprocessing metadata
                print("[STEP 2/7] Loading preprocessing metadata...")
                with open(args.preprocess_metadata, 'r') as f:
                    preprocess_meta = json.load(f)
                target_col = preprocess_meta.get('target_columns', ['target'])[0]
                print(f"  âœ“ Target column: {target_col}")

                # Load preprocessor
                print("[STEP 3/7] Loading preprocessor...")
                preprocessor = load_pickle_any(args.preprocessor)
                if preprocessor is None:
                    raise ValueError("Preprocessor required")

                # Load feature selector
                print("[STEP 4/7] Loading feature selector...")
                feature_selector = None
                if args.feature_selector and os.path.exists(args.feature_selector):
                    try:
                        feature_selector = load_pickle_any(args.feature_selector)
                        if feature_selector:
                            print(f"  âœ“ Loaded ({len(getattr(feature_selector, 'selected_features', []))} features)")
                    except Exception as e:
                        print(f"  âš  Could not load: {e}")
                else:
                    print("  âŠ— Not provided")

                # Load PCA
                print("[STEP 5/7] Loading PCA...")
                pca_transformer = None
                if args.pca and os.path.exists(args.pca):
                    if os.path.getsize(args.pca) > 0:
                        try:
                            pca_transformer = load_pickle_any(args.pca)
                            if pca_transformer:
                                print(f"  âœ“ Loaded")
                        except Exception as e:
                            print(f"  âš  Could not load: {e}")
                else:
                    print("  âŠ— Not provided")

                # Load model
                print("[STEP 6/7] Loading model...")
                model = load_pickle_any(args.model_pickle)
                if model is None:
                    raise ValueError("Model required")

                # Build pipeline
                print("[STEP 7/7] Building unified pipeline...")
                print("-" * 80)
                
                pipeline_steps = []
                
                # Step 1: Data Cleaning (NEW!)
                pipeline_steps.append(('data_cleaning', DataCleaningWrapper(cleaning_meta, target_column=target_col)))
                print("  [1] Data Cleaning âœ“ (date dedup, feature extraction, alignment)")
                
                # Step 2: Preprocessing
                pipeline_steps.append(('preprocessor', PreprocessorWrapper(preprocessor, target_column=target_col)))
                print("  [2] Preprocessor âœ“")
                
                # Step 3: Feature Selection (optional)
                if feature_selector:
                    pipeline_steps.append(('feature_selector', FeatureSelectorWrapper(feature_selector)))
                    print("  [3] Feature Selector âœ“")
                else:
                    print("  [3] Feature Selector âŠ—")
                
                # Step 4: PCA (optional)
                if pca_transformer:
                    pipeline_steps.append(('pca', PCAWrapper(pca_transformer)))
                    print("  [4] PCA âœ“")
                else:
                    print("  [4] PCA âŠ—")
                
                # Step 5: Model
                pipeline_steps.append(('model', ModelWrapper(model)))
                print("  [5] Model âœ“")

                print("-" * 80)
                unified_pipeline = Pipeline(pipeline_steps)
                print(f"âœ“ Pipeline created with {len(pipeline_steps)} steps")

                # Save with cloudpickle
                print(f"Saving to: {args.unified_pipeline}")
                ensure_dir_for(args.unified_pipeline)
                
                try:
                    import cloudpickle
                    with open(args.unified_pipeline, 'wb') as f:
                        cloudpickle.dump(unified_pipeline, f)
                    print("âœ“ Saved with cloudpickle")
                except ImportError:
                    joblib.dump(unified_pipeline, args.unified_pipeline, compress=3)
                    print("âœ“ Saved with joblib")
                
                file_size = os.path.getsize(args.unified_pipeline)
                print(f"âœ“ File size: {file_size / 1024 / 1024:.2f} MB")

                # Save metadata
                pipeline_meta = {
                    'created_at': datetime.utcnow().isoformat() + 'Z',
                    'model_type': args.model_type,
                    'target_column': target_col,
                    'pipeline_steps': [name for name, _ in pipeline_steps],
                    'has_data_cleaning': True,  # NEW
                    'has_feature_selector': feature_selector is not None,
                    'has_pca': pca_transformer is not None,
                    'is_dict_model': isinstance(model, dict),
                    'n_targets': len(model) if isinstance(model, dict) else 1,
                    'file_size_mb': round(file_size / 1024 / 1024, 2),
                    'expected_input_columns': list(set([c for c in cleaning_meta.get('column_names', [])] + [target_col])),
                    'cleaning_metadata': cleaning_meta,
                    'preprocess_metadata': preprocess_meta
                }

                ensure_dir_for(args.pipeline_metadata)
                with open(args.pipeline_metadata, 'w') as f:
                    json.dump(pipeline_meta, f, indent=2)

                print("=" * 80)
                print("SUCCESS: UNIFIED PIPELINE CREATED!")
                print("=" * 80)
                print(f"File: {args.unified_pipeline}")
                print(f"Size: {file_size / 1024 / 1024:.2f} MB")
                print(f"Flow: {' â†’ '.join([n for n, _ in pipeline_steps])}")
                print()
                print("ðŸ’¡ USAGE (Raw Data â†’ Predictions):")
                print("   import cloudpickle")
                print("   with open('pipeline.pkl', 'rb') as f:")
                print("       pipeline = cloudpickle.load(f)")
                print("   # Pass completely RAW data (with date, timestamp, etc.)")
                print("   predictions = pipeline.predict(raw_dataframe)")
                print("=" * 80)

            except Exception as e:
                print("=" * 80)
                print("ERROR: CONVERSION FAILED")
                print("=" * 80)
                print(f"Error: {e}")
                traceback.print_exc()
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}  # NEW
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca
      - {inputPath: pca}
      - --model_pickle
      - {inputPath: model_pickle}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_type
      - {inputValue: model_type}
      - --unified_pipeline
      - {outputPath: unified_pipeline}
      - --pipeline_metadata
      - {outputPath: pipeline_metadata}
