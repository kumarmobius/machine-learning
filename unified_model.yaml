name: Convert to Unified Pipeline v2
inputs:
  - {name: preprocessor, type: Data, description: "Fitted Preprocessor pickle file"}
  - {name: feature_selector, type: Data, description: "Fitted FeatureSelector pickle file", optional: true}
  - {name: pca, type: Data, description: "Fitted PCA pickle file", optional: true}
  - {name: model_pickle, type: Model, description: "Trained model pickle file (can be dict for multi-target)"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata JSON file"}
  - {name: model_type, type: String, description: "classification or regression"}
outputs:
  - {name: unified_pipeline, type: Model, description: "Single unified pipeline pickle file (all-in-one)"}
  - {name: pipeline_metadata, type: Data, description: "Pipeline metadata JSON"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, gzip, traceback
        import pandas as pd, numpy as np
        import joblib
        from sklearn.base import BaseEstimator, TransformerMixin
        from sklearn.pipeline import Pipeline
        from datetime import datetime

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def load_pickle_any(path):
            if not os.path.exists(path):
                return None
            
            file_size = os.path.getsize(path)
            if file_size == 0:
                print(f"Warning: {path} is empty (0 bytes)")
                return None
            
            print(f"Loading {path} ({file_size / 1024:.2f} KB)...")
            
            # Try joblib first
            try:
                obj = joblib.load(path)
                print(f"  ✓ Loaded with joblib: {type(obj)}")
                return obj
            except Exception as e:
                print(f"  ✗ joblib failed: {e}")
            
            # Try gzipped cloudpickle
            try:
                with gzip.open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  ✓ Loaded with gzipped cloudpickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ gzipped cloudpickle failed: {e}")
            
            # Try regular cloudpickle
            try:
                with open(path, "rb") as f:
                    import cloudpickle
                    obj = cloudpickle.load(f)
                    print(f"  ✓ Loaded with cloudpickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ cloudpickle failed: {e}")
            
            # Try regular pickle
            try:
                import pickle
                with open(path, "rb") as f:
                    obj = pickle.load(f)
                    print(f"  ✓ Loaded with pickle: {type(obj)}")
                    return obj
            except Exception as e:
                print(f"  ✗ pickle failed: {e}")
            
            raise ValueError(f"Could not load pickle from {path}")

        # ============================================================================
        # WRAPPER CLASSES
        # ============================================================================

        class PreprocessorWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, preprocessor, target_column=None):
                self.preprocessor = preprocessor
                self.target_column = target_column
                print(f"    PreprocessorWrapper initialized (target: {target_column})")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    Preprocessing: input shape {X.shape}")
                
                if self.target_column and self.target_column not in X.columns:
                    X_with_target = X.copy()
                    X_with_target[self.target_column] = 0
                    
                    try:
                        transformed = self.preprocessor.transform(X_with_target, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X_with_target)
                    
                    if isinstance(transformed, pd.DataFrame) and self.target_column in transformed.columns:
                        transformed = transformed.drop(columns=[self.target_column])
                else:
                    try:
                        transformed = self.preprocessor.transform(X, training_mode=False)
                    except TypeError:
                        transformed = self.preprocessor.transform(X)
                
                if not isinstance(transformed, pd.DataFrame):
                    try:
                        cols = (getattr(self.preprocessor, "feature_names_out", None) or 
                               getattr(self.preprocessor, "output_columns", None) or
                               [f"f{i}" for i in range(transformed.shape[1])])
                        transformed = pd.DataFrame(transformed, columns=list(cols))
                    except Exception:
                        transformed = pd.DataFrame(transformed)
                
                print(f"    Preprocessing: output shape {transformed.shape}")
                return transformed

        class FeatureSelectorWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, feature_selector):
                self.feature_selector = feature_selector
                self.selected_features = getattr(feature_selector, 'selected_features', [])
                print(f"    FeatureSelectorWrapper initialized ({len(self.selected_features)} features)")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    Feature Selection: input shape {X.shape}")
                
                if hasattr(self.feature_selector, 'transform'):
                    result = self.feature_selector.transform(X)
                else:
                    if isinstance(X, pd.DataFrame):
                        result = X.reindex(columns=self.selected_features, fill_value=0.0)
                    else:
                        result = X
                
                print(f"    Feature Selection: output shape {result.shape}")
                return result

        class PCAWrapper(BaseEstimator, TransformerMixin):            
            def __init__(self, pca_transformer):
                self.pca_transformer = pca_transformer
                print(f"    PCAWrapper initialized: {type(pca_transformer)}")
            
            def fit(self, X, y=None):
                return self
            
            def transform(self, X):
                print(f"    PCA: input shape {X.shape}")
                
                from sklearn.pipeline import Pipeline as SKPipeline
                pca_obj = self.pca_transformer
                
                if isinstance(pca_obj, SKPipeline):
                    print("    PCA: Extracting from sklearn Pipeline...")
                    for nm, step in reversed(pca_obj.steps):
                        if hasattr(step, "transform"):
                            pca_obj = step
                            print(f"    PCA: Using step '{nm}': {type(step)}")
                            break
                
                X_arr = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)
                X_pca = pca_obj.transform(X_arr)
                
                if X_pca.ndim == 1:
                    X_pca = X_pca.reshape(-1, 1)
                
                ncomp = X_pca.shape[1]
                pca_cols = [f"PC{i+1}" for i in range(ncomp)]
                result = pd.DataFrame(X_pca, columns=pca_cols)
                
                print(f"    PCA: output shape {result.shape}")
                return result

        class ModelWrapper(BaseEstimator):            
            def __init__(self, model):
                self.model = model
                self.is_dict = isinstance(model, dict)
                print(f"    ModelWrapper initialized: {type(model)}")
                if self.is_dict:
                    print(f"    Dict model with {len(model)} targets: {list(model.keys())}")
            
            def fit(self, X, y=None):
                return self
            
            def predict(self, X):
                X_arr = X.values if isinstance(X, pd.DataFrame) else X
                
                if self.is_dict:
                    # Handle dict of models (multi-target)
                    predictions = {}
                    for target_name, estimator in self.model.items():
                        if hasattr(estimator, 'predict'):
                            pred = estimator.predict(X_arr)
                            predictions[target_name] = pred
                        else:
                            raise ValueError(f"Estimator for '{target_name}' has no predict method")
                    
                    # Convert dict to DataFrame or array
                    if len(predictions) == 1:
                        # Single target in dict - return as 1D array
                        result = list(predictions.values())[0]
                    else:
                        # Multiple targets - return as 2D array
                        result = np.column_stack(list(predictions.values()))
                    
                    print(f"    Model Prediction: {len(result)} predictions made for {len(predictions)} target(s)")
                    return result
                else:
                    # Handle single model
                    if hasattr(self.model, 'predict'):
                        predictions = self.model.predict(X_arr)
                        print(f"    Model Prediction: {len(predictions)} predictions made")
                        return predictions
                    else:
                        raise AttributeError(f"Model {type(self.model)} has no predict method")
            
            def predict_proba(self, X):
                X_arr = X.values if isinstance(X, pd.DataFrame) else X
                
                if self.is_dict:
                    # Handle dict of models
                    probas = {}
                    for target_name, estimator in self.model.items():
                        if hasattr(estimator, 'predict_proba'):
                            probas[target_name] = estimator.predict_proba(X_arr)
                    
                    if not probas:
                        raise AttributeError("No models in dict support predict_proba")
                    
                    return probas
                else:
                    # Handle single model
                    if hasattr(self.model, 'predict_proba'):
                        return self.model.predict_proba(X_arr)
                    else:
                        raise AttributeError("Model does not support predict_proba")

        # ============================================================================
        # MAIN
        # ============================================================================

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--preprocessor', type=str, required=True)
            parser.add_argument('--feature_selector', type=str, default="")
            parser.add_argument('--pca', type=str, default="")
            parser.add_argument('--model_pickle', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--model_type', type=str, required=True)
            parser.add_argument('--unified_pipeline', type=str, required=True)
            parser.add_argument('--pipeline_metadata', type=str, required=True)
            args = parser.parse_args()

            try:
                print("=" * 80)
                print("CONVERTING TO UNIFIED PIPELINE v1.2")
                print("=" * 80)
                print(f"Model Type: {args.model_type}")
                print(f"Output: {args.unified_pipeline}")

                # Load metadata
                print("[STEP 1/6] Loading preprocessing metadata...")
                with open(args.preprocess_metadata, 'r') as f:
                    metadata = json.load(f)
                
                target_col = metadata.get('target_columns', ['target'])[0]
                print(f"  Target column: {target_col}")

                # Load preprocessor
                print("[STEP 2/6] Loading preprocessor...")
                preprocessor = load_pickle_any(args.preprocessor)
                if preprocessor is None:
                    raise ValueError("Preprocessor required but not loaded")
                print(f"  ✓ Preprocessor loaded")

                # Load feature selector
                print("[STEP 3/6] Loading feature selector...")
                feature_selector = None
                if args.feature_selector and os.path.exists(args.feature_selector):
                    try:
                        feature_selector = load_pickle_any(args.feature_selector)
                        if feature_selector and hasattr(feature_selector, 'selected_features'):
                            print(f"  ✓ Feature selector loaded ({len(feature_selector.selected_features)} features)")
                    except Exception as e:
                        print(f"  ⚠ Could not load: {e}")
                else:
                    print("  ⊗ Not provided")

                # Load PCA
                print("[STEP 4/6] Loading PCA...")
                pca_transformer = None
                if args.pca and os.path.exists(args.pca):
                    if os.path.getsize(args.pca) > 0:
                        try:
                            pca_transformer = load_pickle_any(args.pca)
                            if pca_transformer:
                                print(f"  ✓ PCA loaded")
                        except Exception as e:
                            print(f"  ⚠ Could not load: {e}")
                else:
                    print("  ⊗ Not provided")

                # Load model
                print("[STEP 5/6] Loading model...")
                model = load_pickle_any(args.model_pickle)
                if model is None:
                    raise ValueError("Model required but not loaded")
                print(f"  ✓ Model loaded")

                # Build pipeline
                print("[STEP 6/6] Building unified pipeline...")
                print("-" * 80)
                
                pipeline_steps = []
                
                pipeline_steps.append(('preprocessor', PreprocessorWrapper(preprocessor, target_column=target_col)))
                print("  [1] Preprocessor ✓")
                
                if feature_selector:
                    pipeline_steps.append(('feature_selector', FeatureSelectorWrapper(feature_selector)))
                    print("  [2] Feature Selector ✓")
                else:
                    print("  [2] Feature Selector ⊗")
                
                if pca_transformer:
                    pipeline_steps.append(('pca', PCAWrapper(pca_transformer)))
                    print("  [3] PCA ✓")
                else:
                    print("  [3] PCA ⊗")
                
                pipeline_steps.append(('model', ModelWrapper(model)))
                print("  [4] Model ✓")

                print("-" * 80)
                unified_pipeline = Pipeline(pipeline_steps)
                print(f"✓ Pipeline created with {len(pipeline_steps)} steps")

                # Save with cloudpickle
                print(f"Saving to: {args.unified_pipeline}")
                ensure_dir_for(args.unified_pipeline)
                
                try:
                    import cloudpickle
                    with open(args.unified_pipeline, 'wb') as f:
                        cloudpickle.dump(unified_pipeline, f)
                    print("✓ Saved with cloudpickle")
                except ImportError:
                    print("⚠ cloudpickle not available, using joblib...")
                    joblib.dump(unified_pipeline, args.unified_pipeline, compress=3)
                    print("✓ Saved with joblib")
                
                file_size = os.path.getsize(args.unified_pipeline)
                print(f"✓ File size: {file_size / 1024 / 1024:.2f} MB")

                # Save metadata
                pipeline_meta = {
                    'created_at': datetime.utcnow().isoformat() + 'Z',
                    'model_type': args.model_type,
                    'target_column': target_col,
                    'pipeline_steps': [name for name, _ in pipeline_steps],
                    'has_feature_selector': feature_selector is not None,
                    'has_pca': pca_transformer is not None,
                    'is_dict_model': isinstance(model, dict),
                    'n_targets': len(model) if isinstance(model, dict) else 1,
                    'file_size_mb': round(file_size / 1024 / 1024, 2),
                    'original_metadata': metadata
                }

                ensure_dir_for(args.pipeline_metadata)
                with open(args.pipeline_metadata, 'w') as f:
                    json.dump(pipeline_meta, f, indent=2)

                print("=" * 80)
                print("SUCCESS: UNIFIED PIPELINE CREATED!")
                print("=" * 80)
                print(f" File: {args.unified_pipeline}")
                print(f" Size: {file_size / 1024 / 1024:.2f} MB")
                print(f"Flow: {' → '.join([n for n, _ in pipeline_steps])}")
                print(" Usage:")
                print("   import cloudpickle")
                print("   with open('model.pkl', 'rb') as f:")
                print("       pipeline = cloudpickle.load(f)")
                print("   predictions = pipeline.predict(raw_data)")
                print("=" * 80)

            except Exception as e:
                print("=" * 80)
                print("ERROR: CONVERSION FAILED")
                print("=" * 80)
                print(f"Error: {e}")
                traceback.print_exc()
                print("=" * 80)
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca
      - {inputPath: pca}
      - --model_pickle
      - {inputPath: model_pickle}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_type
      - {inputValue: model_type}
      - --unified_pipeline
      - {outputPath: unified_pipeline}
      - --pipeline_metadata
      - {outputPath: pipeline_metadata}
