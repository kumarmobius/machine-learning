name: Advanced Preprocess v8.5
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enable_feature_selection, type: String, description: "true/false to enable feature selection", optional: true, default: "true"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: Float, description: "IQR fold for outlier rule", optional: true, default: "1.5"}
  - {name: max_knn_rows, type: Integer, description: "Max rows for KNN imputer else fallback", optional: true, default: "5000"}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: rare_threshold, type: Float, description: "Rare label threshold as fraction or absolute count", optional: true, default: "0.01"}
  - {name: enable_string_similarity, type: String, description: "true/false high-cardinality grouping", optional: true, default: "false"}
  - {name: missing_threshold, type: Float, description: "Drop columns with missing % above this", optional: true, default: "0.40"}
  - {name: unique_threshold, type: Float, description: "Drop columns with unique % above this (ID columns)", optional: true, default: "0.95"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: "20"}
outputs:
  - {name: train_X, type: Data, description: "Processed features parquet"}
  - {name: train_y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "joblib dump of fitted Preprocessor"}
  - {name: feature_selector, type: Data, description: "joblib dump of fitted FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSONf metadata"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime
        import pandas as pd, numpy as np
        from sklearn.impute import SimpleImputer, KNNImputer
        from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
        from sklearn.pipeline import Pipeline
        from sklearn.model_selection import KFold
        from sklearn.experimental import enable_iterative_imputer  # noqa
        from sklearn.impute import IterativeImputer
        from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel, VarianceThreshold
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        import joblib
        import re
        import math
        import category_encoders as ce
        from rapidfuzz import process as rf_process, fuzz as rf_fuzz
        import gzip, cloudpickle
        from sklearn.inspection import permutation_importance
        from sklearn.model_selection import StratifiedKFold
        from sklearn.preprocessing import PolynomialFeatures
        from itertools import combinations
        
        # helpers
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh: return fh.read(n)
            except Exception: return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            if txt[0] in ("{","["): return True
            if "{" in txt or "[" in txt: return True
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path = max(files, key=lambda p: os.path.getsize(p))
                print("Info selected file: " + path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: " + str(path))
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported format: " + str(path))

        def make_hashable_for_dupes(df):
            df = df.copy()
            for c in df.columns:
                try:
                    if df[c].apply(lambda v: isinstance(v,(list,dict,set))).any():
                        df[c] = df[c].map(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(list,dict,set)) else v)
                except Exception:
                    df[c] = df[c].astype(str)
            return df

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',','.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception:
                    try: val=float(number_clean.replace(',', '.'))
                    except Exception: return np.nan
                if mult: val*=MULTIPLIER_MAP.get(mult.lower(),1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            num_search=re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if num_search:
                try: return float(num_search.group(0).replace(',',''))
                except Exception: return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df=df.copy(); report={'converted':[],'skipped':[]}
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                if col in ('__TARGET__TMP__',): continue
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                frac = parsed.notna().sum()/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed; report['converted'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def detect_and_extract_dates(df, enabled=True, exclude_cols=None):
            df = df.copy()
            report = {'date_columns': [], 'skipped': [], 'patterns_found': {}}
            if not enabled:
                return df, report
            
            exclude_cols = set(exclude_cols or [])
            cand = [c for c in df.columns if c not in exclude_cols and 
                    (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
            
            datetime_patterns = [
                (r'^\d{4}[-/]\d{1,2}[-/]\d{1,2}', 'ISO_DATE'),
                (r'^\d{4}[-/]\d{1,2}[-/]\d{1,2}[T\s]\d{1,2}:\d{2}', 'ISO_DATETIME'),
                (r'^\d{1,2}[-/]\d{1,2}[-/]\d{4}', 'US_DATE'),
                (r'^\d{1,2}[-/.]\d{1,2}[-/.]\d{4}', 'EU_DATE'),
                (r'^\w{3,9}\s+\d{1,2},?\s+\d{4}', 'MONTH_NAME'),
                (r'^\d{10}$', 'UNIX_TIMESTAMP'),
                (r'^\d{13}$', 'UNIX_TIMESTAMP_MS'),
            ]
            
            for col in cand:
                ser = df[col].astype(object)
                sample = ser.dropna().astype(str).head(500)
                
                if sample.empty:
                    report['skipped'].append({'col': col, 'parsable_fraction': 0.0})
                    continue
                detected_pattern = None
                for pattern, pattern_name in datetime_patterns:
                    matches = sample.str.match(pattern, na=False).sum()
                    if matches / len(sample) >= 0.5:
                        detected_pattern = pattern_name
                        break
                if detected_pattern in ['UNIX_TIMESTAMP', 'UNIX_TIMESTAMP_MS']:
                    try:
                        numeric_ser = pd.to_numeric(ser, errors='coerce')
                        if detected_pattern == 'UNIX_TIMESTAMP_MS':
                            parsed = pd.to_datetime(numeric_ser, unit='ms', errors='coerce', utc=False)
                        else:
                            parsed = pd.to_datetime(numeric_ser, unit='s', errors='coerce', utc=False)
                        frac = parsed.notna().mean()
                    except:
                        frac = 0.0
                        parsed = None
                else:
                    parsed = pd.to_datetime(sample, errors='coerce', utc=False, infer_datetime_format=True)
                    frac = parsed.notna().mean()
                
                if frac >= 0.6:
                    if detected_pattern in ['UNIX_TIMESTAMP', 'UNIX_TIMESTAMP_MS']:
                        numeric_full = pd.to_numeric(ser, errors='coerce')
                        if detected_pattern == 'UNIX_TIMESTAMP_MS':
                            full = pd.to_datetime(numeric_full, unit='ms', errors='coerce', utc=False)
                        else:
                            full = pd.to_datetime(numeric_full, unit='s', errors='coerce', utc=False)
                    else:
                        full = pd.to_datetime(ser, errors='coerce', utc=False, infer_datetime_format=True)
                    
                    df[col + "_orig"] = df[col]
                    df[col] = full
                    df[col + "_year"] = df[col].dt.year.astype('Int64')
                    df[col + "_month"] = df[col].dt.month.astype('Int64')
                    df[col + "_day"] = df[col].dt.day.astype('Int64')
                    df[col + "_dayofweek"] = df[col].dt.dayofweek.astype('Int64')
                    df[col + "_quarter"] = df[col].dt.quarter.astype('Int64')
                    df[col + "_month_sin"] = np.sin(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_month_cos"] = np.cos(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_day_sin"] = np.sin(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_day_cos"] = np.cos(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5, 6]).astype('Int64')
                    df[col + "_is_month_start"] = df[col].dt.is_month_start.astype('Int64')
                    df[col + "_is_month_end"] = df[col].dt.is_month_end.astype('Int64')
                    df[col + "_is_quarter_start"] = df[col].dt.is_quarter_start.astype('Int64')
                    df[col + "_is_quarter_end"] = df[col].dt.is_quarter_end.astype('Int64')
                    if (df[col].dt.hour != 0).any():
                        df[col + "_hour"] = df[col].dt.hour.astype('Int64')
                        df[col + "_is_business_hours"] = ((df[col].dt.hour >= 9) & 
                                                           (df[col].dt.hour <= 17)).astype('Int64')
                    df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                    
                    report['date_columns'].append({
                        'col': col,
                        'parsable_fraction': frac,
                        'pattern': detected_pattern or 'AUTO_DETECTED'
                    })
                    report['patterns_found'][col] = detected_pattern or 'AUTO_DETECTED'
                    
                    print(f"[INFO] Detected datetime in '{col}' (pattern: {detected_pattern or 'AUTO'}, {frac*100:.1f}% valid)")
                else:
                    report['skipped'].append({'col': col, 'parsable_fraction': frac})

            return df, report
            
        def add_polynomial_features(df, numeric_cols, degree=2, interaction_only=True, max_features=20):
            
            if len(numeric_cols) < 2:
                return df
            
            print(f"[INFO] Creating polynomial features (degree={degree}, interaction_only={interaction_only})")
            variances = df[numeric_cols].var().sort_values(ascending=False)
            top_cols = variances.head(min(5, len(numeric_cols))).index.tolist()
            new_features = {}
            interaction_count = 0
            
            for col1, col2 in combinations(top_cols, 2):
                if interaction_count >= max_features:
                    break
                interaction_name = f"{col1}_x_{col2}"
                new_features[interaction_name] = df[col1] * df[col2]
                interaction_count += 1
                if (df[col2] != 0).all():
                    div_name = f"{col1}_div_{col2}"
                    new_features[div_name] = df[col1] / df[col2]
                    interaction_count += 1
            
            if degree == 2:
                for col in top_cols[:3]:  # Only top 3
                    new_features[f"{col}_squared"] = df[col] ** 2
            for name, values in new_features.items():
                df[name] = values
            
            print(f"[INFO] Created {len(new_features)} polynomial/interaction features")
            return df

        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts=series.value_counts(dropna=False); n=len(series)
            rare = set(counts[counts <= threshold_count].index) if threshold_count is not None else set(counts[counts <= max(1,int(threshold_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals=[v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping={}; used=set()
            for v in vals:
                if v in used: continue
                matches=rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=score_threshold)
                group=[m[0] for m in matches]
                for g in group: mapping[g]=v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0,pd.Series(True,index=df.index)
            q1 = df[numeric_cols].quantile(0.25, numeric_only=True)
            q3 = df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr = q3 - q1
            lower = (q1 - fold * iqr).to_dict()
            upper = (q3 + fold * iqr).to_dict()
            mask = pd.Series(True, index=df.index)
            for c in numeric_cols:
                s = df[c]
                lo = lower.get(c, None); hi = upper.get(c, None)
                cond = pd.Series(True, index=df.index)
                if lo is not None:
                    cond = cond & s.ge(lo)
                if hi is not None:
                    cond = cond & s.le(hi)
                cond = cond.reindex(df.index).fillna(True)
                mask = (mask & cond)
            before=len(df)
            df2 = df.loc[mask].copy()
            removed = before - len(df2)
            df2.reset_index(drop=True, inplace=True)
            return df2, removed, mask

        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0
            q1=df[numeric_cols].quantile(0.25, numeric_only=True); q3=df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr=q3-q1
            lower=(q1 - fold*iqr).to_dict(); upper=(q3 + fold*iqr).to_dict()
            df2=df.copy()
            for c in numeric_cols:
                lo = lower.get(c, None); hi = upper.get(c, None)
                if lo is not None or hi is not None:
                    df2[c]=df2[c].clip(lower=lo, upper=hi)
            return df2,0

        def choose_scaler_for_series(s):
            v=s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0)==0: return StandardScaler()
            skew=float(v.skew()); kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            extreme_frac=float(((v - v.mean()).abs() > 3*v.std(ddof=0)).mean())
            if (v.min()>=0.0 and v.max()<=1.0): return MinMaxScaler()
            if abs(skew)>=1.0: return Pipeline([('power',PowerTransformer(method='yeo-johnson')),('std',StandardScaler())])
            if extreme_frac>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        def choose_categorical_encoder(col, series, target_series=None, train_mode=True, model_type='classification'):
            n = len(series)
            nunique = series.nunique(dropna=True)
            high_card = (nunique > max(50, 0.05 * n))
            if nunique <= 2:
                enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                return 'onehot', enc, {}
            elif nunique <= 10:
                enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                return 'onehot', enc, {}
            elif nunique <= 50:
                if target_series is not None and train_mode and model_type == 'classification':
                    return 'target_kfold', None, {}
                else:
                    return 'count', None, {}
            else:  # high_card == True
                if target_series is not None and train_mode and model_type == 'classification':
                    return 'target_kfold', None, {}
                else:
                    return 'count', None, {}
            
        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=1.0, random_state=42):        
            X_col = df[col].astype(object).fillna('__NA__')
            global_mean = float(y.mean())
            oof = pd.Series(index=df.index, dtype=float)
            try:
                kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                splits = list(kf.split(df, y))
            except:
                from sklearn.model_selection import KFold
                kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                splits = list(kf.split(df))
            for train_idx, val_idx in splits:
                train_means = y.iloc[train_idx].groupby(X_col.iloc[train_idx]).mean()
                oof.iloc[val_idx] = X_col.iloc[val_idx].map(lambda v: train_means.get(v, global_mean))
          
            counts = X_col.value_counts()
            smooth_map = {}
            for cat, cnt in counts.items():
                if cnt > 0:
                    cat_mean = float(y[X_col == cat].mean())
                    alpha = cnt / (cnt + smoothing)
                    smooth_map[cat] = alpha * cat_mean + (1 - alpha) * global_mean
                else:
                    smooth_map[cat] = global_mean
            
            return oof.fillna(global_mean), smooth_map, global_mean

        class Preprocessor:
            def __init__(self):
                self.num_cols=[]; self.cat_cols=[]; self.col_config={}; self.global_metadata={}
                self.date_columns = []
                self.date_patterns = {}

            def fit(self, df, y=None, config=None, target_col=None, model_type='classification'):
                self.global_metadata['config']=config or {}
                self.global_metadata['target_col']=target_col
                self.global_metadata['model_type']=model_type
                nrows=len(df)
                self.original_input_columns = list(df.columns)
                
                # Detect bases from already-extracted date features (keep '_orig' intact!)
                date_suffixes = ['_year', '_month', '_day', '_dayofweek', '_quarter', '_is_weekend', 
                                 '_days_since_epoch', '_month_sin', '_month_cos', '_day_sin', '_day_cos',
                                 '_hour', '_is_business_hours', '_is_month_start', '_is_month_end', 
                                 '_is_quarter_start', '_is_quarter_end']
                
                extracted_bases = set()
                for col in df.columns:
                    for suffix in date_suffixes:
                        if col.endswith(suffix):
                            base = col[:-len(suffix)]  # DON'T strip '_orig' - keep as-is!
                            extracted_bases.add(base)
                            break
                
                # Also check for object/string date columns that haven't been extracted yet
                potential_date_cols = [c for c in df.columns if 
                          (df[c].dtype == 'object' or str(df[c].dtype).startswith('string')) and
                          c != target_col and
                          not any(c.endswith(suf) for suf in date_suffixes)]
                
                # Priority: if we found extracted features, use those bases; otherwise use potential columns
                self.date_columns = list(extracted_bases) if extracted_bases else potential_date_cols
                
                if self.date_columns:
                    print(f"[INFO] Preprocessor will track date columns: {self.date_columns}")
                
                self.num_cols=df.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols=[c for c in df.columns if c not in self.num_cols]
                for c in self.num_cols:
                    s=df[c]; cfg={}
                    missing_frac=s.isna().mean(); cfg['missing_frac']=float(missing_frac)
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['skew']=float(s.dropna().skew()) if s.dropna().shape[0]>2 else 0.0
                    cfg['kurtosis']=float(s.dropna().kurtosis()) if s.dropna().shape[0]>3 else 0.0
                    if missing_frac==0: cfg['imputer']=('none',None)
                    elif missing_frac<0.02:
                        imp=SimpleImputer(strategy='median'); imp.fit(np.array(s).reshape(-1,1)); cfg['imputer']=('simple_median',imp)
                    else:
                        if nrows<=self.global_metadata['config'].get('max_knn_rows',5000) and missing_frac<0.25:
                            cfg['imputer']=('knn',{'n_neighbors':5})
                        else:
                            cfg['imputer']=('iterative',{'max_iter':10,'random_state':0})
                    cfg['scaler_choice']='auto'
                    self.col_config[c]=cfg
                for c in self.cat_cols:
                    s = df[c].astype(object)
                    cfg = {}
                    cfg['n_unique'] = int(s.nunique(dropna=True))
                    cfg['missing_frac'] = float(s.isna().mean())
                    cfg['high_card'] = cfg['n_unique'] > max(50, 0.05*nrows)
                    cfg['rare_threshold_frac'] = self.global_metadata['config'].get('rare_threshold', 0.01)
                    
                    enc_name, enc_obj, _ = choose_categorical_encoder(
                        c, s, 
                        target_series=y, 
                        train_mode=True, 
                        model_type=model_type
                    )
                    
                    cfg['encoder_type'] = enc_name
                    cfg['encoder_obj'] = enc_obj
                    self.col_config[c] = cfg
                if y is not None and model_type == 'classification':
                    for c in self.cat_cols:
                        cfg = self.col_config[c]
                        enc_type = cfg['encoder_type']
                        
                        if enc_type == 'onehot':
                            ohe = cfg.get('encoder_obj')
                            if ohe is not None:
                                print(f"[INFO] Fitting OneHot encoder for column: {c}")
                                resh = df[[c]].astype(str)
                                ohe.fit(resh)
                                cfg['encoder_obj'] = ohe
                                cfg['ohe_columns'] = [f"{c}__{cat}" for cat in ohe.categories_[0]]
                                print(f"[INFO] OneHot encoder fitted for '{c}': {len(ohe.categories_[0])} categories")
                        
                        elif enc_type == 'target_kfold':
                            print(f"[INFO] Fitting K-Fold target encoder for column: {c}")
                            oof_encoding, test_mapping, global_mean = fit_target_encoder_kfold(
                                df, c, y, n_splits=5, smoothing=1.0, random_state=42
                            )
                            cfg['oof_encoding'] = oof_encoding 
                            cfg['target_mapping'] = test_mapping 
                            cfg['target_global_mean'] = float(global_mean)
                            print(f"[INFO] Target encoder fitted for '{c}': {len(test_mapping)} unique categories")
                            
                        elif enc_type == 'count':
                            counts = df[c].astype(object).value_counts().to_dict()
                            cfg['count_map'] = counts
                            print(f"[INFO] Count encoder fitted for '{c}': {len(counts)} unique categories")
                        
                        elif enc_type == 'ordinal':
                            ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                            resh = df[[c]].astype(object)
                            ord_enc.fit(resh)
                            cfg['encoder_obj'] = ord_enc
                            print(f"[INFO] Ordinal encoder fitted for '{c}': {len(ord_enc.categories_[0])} categories")      
                self.global_metadata['num_cols']=self.num_cols; self.global_metadata['cat_cols']=self.cat_cols
                return self

            def transform(self, df, training_mode=False):
                import numpy as np, pandas as pd
                df=df.copy()
                if hasattr(self, 'date_columns') and self.date_columns and not training_mode:
                    for expected_col in self.date_columns:
                        if expected_col not in df.columns:
                            # Check if we have the source column without '_orig' suffix
                            if expected_col.endswith('_orig'):
                                source_name = expected_col.replace('_orig', '')
                                if source_name in df.columns:
                                    print(f"[INFO] Aligning test data: copying '{source_name}' -> '{expected_col}'")
                                    df[expected_col] = df[source_name]                          
                target_col = self.global_metadata.get('target_col')
                protected_cols=set([target_col]) if target_col in df.columns else set()
                cfg_global = self.global_metadata.get('config') or {}
                if hasattr(self, 'date_columns') and self.date_columns:
                    print(f"[INFO] Checking for date columns to extract...")
                    
                    # Only extract dates that are still in object/string format
                    cols_to_extract = []
                    for date_col in self.date_columns:
                        if date_col in df.columns:
                            # Check if it's still an object/string (not already extracted)
                            if df[date_col].dtype == 'object' or str(df[date_col].dtype).startswith('string'):
                                cols_to_extract.append(date_col)
                            else:
                                print(f"[INFO] Skipping '{date_col}' - already processed (dtype: {df[date_col].dtype})")

                    if cols_to_extract:
                        print(f"[INFO] Extracting dates from {len(cols_to_extract)} columns: {cols_to_extract}")
                        exclude_cols = [target_col] if target_col else []
                        df, _ = detect_and_extract_dates(df, enabled=True, exclude_cols=exclude_cols)
                        print(f"[INFO] Date extraction complete. Shape: {df.shape}")
                        
                        # Drop the original string format columns (ending with '_orig')
                        cols_to_drop = [c for c in df.columns if c.endswith('_orig')]
                        if cols_to_drop:
                            print(f"[INFO] Dropping original date string columns: {cols_to_drop}")
                            df = df.drop(columns=cols_to_drop)
                            print(f"[INFO] Shape after dropping original date columns: {df.shape}")
                    else:
                        print(f"[INFO] No date extraction needed - all date columns already processed")
                        
                        # Also drop '_orig' columns in this case (they might already exist)
                        cols_to_drop = [c for c in df.columns if c.endswith('_orig')]
                        if cols_to_drop:
                            print(f"[INFO] Dropping original date string columns: {cols_to_drop}")
                            df = df.drop(columns=cols_to_drop)
                
    
                df.replace(["NaN","nan","None","null","INF","-INF","Inf","-Inf","inf","-inf"], np.nan, inplace=True)
                for c in list(self.num_cols):
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors='coerce')
                temp_df=df.copy()
                for c in self.num_cols:
                    imputer_info=self.col_config.get(c,{}).get('imputer',('none',None))
                    if imputer_info[0] in ('knn','iterative'):
                        med=pd.to_numeric(temp_df[c], errors='coerce').median()
                        temp_df[c]=temp_df[c].fillna(med)

                scaler_objs={}
                for c in self.num_cols:
                    sample=temp_df[c]; scaler=choose_scaler_for_series(sample)
                    try: scaler.fit(sample.values.reshape(-1,1))
                    except Exception: scaler=StandardScaler(); scaler.fit(sample.values.reshape(-1,1))
                    scaler_objs[c]=scaler; self.col_config[c]['scaler_obj']=scaler

                num_matrix=temp_df[self.num_cols].copy()
                scaled_matrix=np.zeros_like(num_matrix.values, dtype=float)
                for i,c in enumerate(self.num_cols):
                    sc=scaler_objs[c]; col_vals=num_matrix[c].values.reshape(-1,1)
                    try: scaled_col=sc.transform(col_vals).reshape(-1)
                    except Exception: scaled_col=StandardScaler().fit_transform(col_vals).reshape(-1)
                    scaled_matrix[:,i]=scaled_col
                scaled_df=pd.DataFrame(scaled_matrix, columns=self.num_cols, index=num_matrix.index)

                need_knn=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='knn']
                if need_knn:
                    base_imp=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    knn_neighbors=base_imp.get('n_neighbors',5) if isinstance(base_imp,dict) else 5
                    knn=KNNImputer(n_neighbors=knn_neighbors)
                    imputed_scaled=knn.fit_transform(scaled_df)
                    imputed_scaled_df=pd.DataFrame(imputed_scaled, columns=self.num_cols, index=scaled_df.index)
                    for c in self.num_cols:
                        sc=scaler_objs[c]
                        try: inv=sc.inverse_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        except Exception: inv=imputed_scaled_df[c].values.reshape(-1)
                        df[c]=inv

                need_iter=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='iterative']
                if need_iter:
                    iter_max_iter=10; base_imp2=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    if isinstance(base_imp2,dict): iter_max_iter=base_imp2.get('max_iter',10)
                    iter_imp=IterativeImputer(max_iter=iter_max_iter, random_state=0)
                    arr=df[self.num_cols].astype(float).replace([np.inf,-np.inf], np.nan).values
                    iter_out=iter_imp.fit_transform(arr)
                    df[self.num_cols]=pd.DataFrame(iter_out, columns=self.num_cols, index=df.index)
                    for c in need_iter: self.col_config[c]['imputer_obj']=iter_imp

                for c in self.num_cols:
                    cfg=self.col_config[c]
                    if cfg['imputer'][0]=='simple_median':
                        imp=cfg['imputer'][1]; df[c]=imp.transform(df[[c]])
                if training_mode:
                    if cfg_global.get('outlier_strategy','drop_iqr')=='drop_iqr':
                        df2, n_removed, kept_mask = drop_outliers_iqr(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                        self.global_metadata['outliers_removed']=int(n_removed)
                        self.global_metadata['kept_mask_after_outlier']=kept_mask
                        df=df2
                    elif cfg_global.get('outlier_strategy')=='winsorize':
                        df,_=winsorize_df(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                        self.global_metadata['kept_mask_after_outlier']=pd.Series(True,index=df.index)

                for c in self.num_cols:
                    sc=self.col_config[c].get('scaler_obj')
                    if sc is None: sc=choose_scaler_for_series(df[c].fillna(df[c].median()))
                    try: transformed=sc.transform(df[[c]].values); df[c]=transformed.reshape(-1)
                    except Exception:
                        try: df[c]=sc.fit_transform(df[[c]].values).reshape(-1)
                        except Exception: df[c]=StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                    self.col_config[c]['scaler_obj']=sc
                if training_mode and cfg_global.get('create_interactions', False):
                    print("[INFO] Creating polynomial/interaction features...")
                    current_num_cols = [c for c in df.columns if c in self.num_cols]
                    if len(current_num_cols) >= 2:
                        df = add_polynomial_features(
                            df, 
                            current_num_cols, 
                            degree=2, 
                            interaction_only=True, 
                            max_features=15
                        )
                        print(f"[INFO] DataFrame shape after interactions: {df.shape}")
                for c in [cc for cc in self.cat_cols if cc in df.columns and cc != target_col]:
                    cfg = self.col_config[c]
                    s = df[c].astype(object)
                    rare_thresh = cfg.get('rare_threshold_frac', 0.01)
                    if rare_thresh < 1.0:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thresh, threshold_count=None)
                    else:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thresh))
                    df[c] = collapsed
                    cfg['rare_values'] = list(rare_set)
                    if cfg_global.get('enable_string_similarity', False) and cfg['n_unique'] > 20:
                        grouped, mapping = string_similarity_group(df[c], score_threshold=90)
                        df[c] = grouped
                        cfg['string_similarity_map'] = mapping
                    
                    enc_type = cfg.get('encoder_type')
                    if enc_type == 'onehot':
                        ohe = cfg.get('encoder_obj')
                        ohe_columns = cfg.get('ohe_columns')
                        
                        if ohe is None or ohe_columns is None:
                            print(f"[WARN] OneHot encoder for '{c}' not pre-fitted, fitting now...")
                            ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                            resh = df[[c]].astype(str)
                            ohe.fit(resh)
                            cfg['encoder_obj'] = ohe
                            ohe_columns = [f"{c}__{cat}" for cat in ohe.categories_[0]]
                            cfg['ohe_columns'] = ohe_columns
                        arr = ohe.transform(df[[c]].astype(str))
                        df_ohe = pd.DataFrame(arr, columns=ohe_columns, index=df.index)
                        df = pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                    elif enc_type == 'target_kfold':
                        if training_mode and 'oof_encoding' in cfg:
                            oof = cfg['oof_encoding']
                            df[c] = oof.reindex(df.index, fill_value=cfg.get('target_global_mean', 0.0)).astype(float)
                            print(f"[INFO] Applied OOF target encoding for '{c}' in TRAINING mode")
                        else:
                            mapping = cfg.get('target_mapping', {})
                            global_mean = cfg.get('target_global_mean', 0.0)
                            df[c] = df[c].map(lambda x: mapping.get(x, global_mean)).astype(float)
                            print(f"[INFO] Applied test target encoding for '{c}' in INFERENCE mode")
                    elif enc_type == 'count':
                        cnt_map = cfg.get('count_map', df[c].value_counts().to_dict())
                        df[c] = df[c].map(lambda x: cnt_map.get(x, 0)).astype(float)
                    elif enc_type == 'ordinal':
                        ord_enc = cfg.get('encoder_obj')
                        if ord_enc is None:
                            print(f"[WARN] Ordinal encoder for '{c}' not pre-fitted, fitting now...")
                            ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                            resh = df[[c]].astype(object)
                            ord_enc.fit(resh)
                            cfg['encoder_obj'] = ord_enc
                        
                        try:
                            df[c] = ord_enc.transform(df[[c]].astype(object)).astype(float)
                        except Exception as e:
                            print(f"[WARN] Ordinal encoding failed for '{c}': {e}, using category codes")
                            # Fallback: use category codes
                            df[c] = df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)

                    
                    # FALLBACK: Category codes
                    else:
                        df[c] = df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)
                for c in list(df.columns):
                    if df[c].dtype=='object':
                        try: df[c]=pd.to_numeric(df[c], errors='coerce')
                        except Exception: pass
                df.replace([np.inf,-np.inf], np.nan, inplace=True)
                return df

            def save(self, path): 
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
                    
            @staticmethod
            def load(path): 
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)

        class FeatureSelector:
            _VAR_THRESH = 1e-5
            _CORR_THRESH = 0.95
            _RELEVANCE = "mi"
            _PI_REPEATS = 5
            _RANDOM_STATE = 42
        
            def __init__(self, task):
                self.task = str(task).lower()
                self.selected_features = []
                self._filter_keep_ = []
                self._relevance_keep_ = []
                self._mi_scores_ = pd.Series(dtype=float)
                self._pi_importance_ = pd.Series(dtype=float)

            def _auto_k(self, p: int, n_samples: int = None) -> int:
                if n_samples is None:
                    base_k = max(10, min(int(math.sqrt(p) * 3.0), p))
                else:
                    if n_samples < 500:
                        base_k = max(10, min(int(math.sqrt(p) * 1.5), int(p * 0.6)))
                    elif n_samples < 5000:
                        base_k = max(10, min(int(math.sqrt(p) * 3), int(p * 0.7)))
                    else:
                        base_k = max(10, min(int(math.sqrt(p) * 4), int(p * 0.8)))
                    
                    max_allowed = max(10, n_samples // 5)
                    base_k = min(base_k, max_allowed)
                
                return base_k
        
            @staticmethod
            def _nzv_keep(X: pd.DataFrame, threshold: float) -> list:
                if X.shape[1] == 0:
                    return []
                vt = VarianceThreshold(threshold=threshold)
                vt.fit(X.values)
                mask = vt.get_support()
                return list(X.columns[mask])
        
            @staticmethod
            def _corr_prune_keep(X: pd.DataFrame, corr_thresh: float) -> list:
                cols = list(X.columns)
                if len(cols) <= 1:
                    return cols
                corr = X.corr(method="spearman").abs()
                keep = set(cols)
                while True:
                    sub = corr.loc[list(keep), list(keep)]
                    np.fill_diagonal(sub.values, 0.0)
                    max_val = sub.values.max()
                    if not np.isfinite(max_val) or max_val < corr_thresh:
                        break
                    idx = np.unravel_index(np.argmax(sub.values), sub.shape)
                    a = sub.index[idx[0]]
                    b = sub.columns[idx[1]]
                    mean_a = sub[a].mean()
                    mean_b = sub[b].mean()
                    drop = a if mean_a >= mean_b else b
                    keep.remove(drop)
                return list(keep)
        
            def _mi_scores(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:
                cols = list(X.columns)
                if self.task == "classification":
                    scores = mutual_info_classif(X.values, y, random_state=self._RANDOM_STATE)
                else:
                    y_num = pd.to_numeric(y, errors="ignore")
                    scores = mutual_info_regression(X.values, y_num, random_state=self._RANDOM_STATE)
                return pd.Series(scores, index=cols).fillna(0.0)
        
            @staticmethod
            def _topk_by_series(scores: pd.Series, k: int) -> list:
                k = max(1, min(k, scores.shape[0]))
                return list(scores.sort_values(ascending=False).head(k).index)
        
            @staticmethod
            def _mrmr_greedy(X: pd.DataFrame, mi: pd.Series, k: int) -> list:
                k = max(1, min(k, X.shape[1]))
                corr = X.corr(method="spearman").abs().fillna(0.0)
                selected = []
                candidates = list(X.columns)
                while len(selected) < k and candidates:
                    best_feat = None
                    best_score = -1e18
                    for c in candidates:
                        rel = float(mi.get(c, 0.0))
                        if not selected:
                            score = rel
                        else:
                            red = float(corr.loc[c, selected].mean())
                            score = rel - red
                        if score > best_score:
                            best_score = score
                            best_feat = c
                    selected.append(best_feat)
                    candidates.remove(best_feat)
                return selected
        
            def _permutation_keep(self, X: pd.DataFrame, y: pd.Series) -> tuple:
                if self.task == "classification":
                    est = ExtraTreesClassifier(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                else:
                    est = ExtraTreesRegressor(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                est.fit(X, y)
                pi = permutation_importance(
                    est, X, y, n_repeats=self._PI_REPEATS, random_state=self._RANDOM_STATE, n_jobs=-1
                )
                imp = pd.Series(pi.importances_mean, index=X.columns).fillna(0.0)
                keep = list(imp[imp > 0.0].index)
                if not keep:
                    keep = [imp.sort_values(ascending=False).index[0]]
                return keep, imp
        
            def fit(self, X: pd.DataFrame, y) -> "FeatureSelector":
                if isinstance(y, (pd.DataFrame, pd.Series)):
                    y_arr = pd.Series(y).values.ravel()
                else:
                    y_arr = np.asarray(y).ravel()
        
                keep1 = self._nzv_keep(X, self._VAR_THRESH)
                X1 = X[keep1]
                keep2 = self._corr_prune_keep(X1, self._CORR_THRESH)
                Xf = X1[keep2]
                self._filter_keep_ = list(Xf.columns)
                mi = self._mi_scores(Xf, y_arr)
                self._mi_scores_ = mi.copy()
                k = self._auto_k(Xf.shape[1], n_samples=len(Xf))
                print(f"[INFO] Adaptive k selection: {k} features (from {Xf.shape[1]} candidates, {len(Xf)} samples)")
                if self._RELEVANCE == "mrmr":
                    keep_rel = self._mrmr_greedy(Xf, mi, k)
                else:
                    keep_rel = self._topk_by_series(mi, k)
                Xr = Xf[keep_rel]
                self._relevance_keep_ = list(Xr.columns)
        
                keep_pi, imp = self._permutation_keep(Xr, y_arr)
                Xb = Xr[keep_pi]
                self._pi_importance_ = imp
                self.selected_features = list(Xb.columns)
                return self
        
            def transform(self, X: pd.DataFrame) -> pd.DataFrame:
                return X.reindex(columns=self.selected_features, fill_value=0.0)
        
            def save(self, path: str) -> None:
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
        
            @staticmethod
            def load(path: str) -> "FeatureSelector":
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)

        def print_head(df, title, nrows=20):
            print("="*80)
            print(title)
            print("="*80)
            try: print(df.head(nrows).to_string())
            except Exception: print(df.head(nrows))

        def print_dtypes(df, title):
            print("="*80)
            print(title)
            print("="*80)
            try: 
                for c in df.columns:
                    print(f"{c}: {df[c].dtype}")
            except Exception:
                print(df.dtypes)

        def print_data_understanding(df, target_col, preview_rows=20):
            print("="*80)
            print("DATA UNDERSTANDING REPORT")
            print("="*80)
            print(f"1. DATASET SHAPE: {df.shape[0]} rows × {df.shape[1]} columns")
            
            print(f"2. COLUMN NAMES AND TYPES:")
            for col in df.columns:
                print(f"   - {col}: {df[col].dtype}")
            
            print(f"3. MISSING VALUES:")
            missing = df.isnull().sum()
            missing_pct = (missing / len(df) * 100).round(2)
            for col in df.columns:
                if missing[col] > 0:
                    print(f"   - {col}: {missing[col]} ({missing_pct[col]}%)")
            
            print(f"4. TARGET VARIABLE ANALYSIS: {target_col}")
            if target_col in df.columns:
                print(f"   - Type: {df[target_col].dtype}")
                print(f"   - Missing: {df[target_col].isnull().sum()} ({(df[target_col].isnull().sum()/len(df)*100):.2f}%)")
                print(f"   - Unique values: {df[target_col].nunique()}")
                print(f"  VALUE COUNTS:")
                vc = df[target_col].value_counts().head(20)
                for val, count in vc.items():
                    pct = (count / len(df) * 100)
                    print(f"      {val}: {count} ({pct:.2f}%)")
            
            print(f"5. NUMERIC COLUMNS SUMMARY:")
            num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if num_cols:
                for col in num_cols[:10]:  # Show first 10
                    print(f"   {col}:")
                    print(f"      Mean: {df[col].mean():.4f}")
                    print(f"      Std: {df[col].std():.4f}")
                    print(f"      Min: {df[col].min():.4f}")
                    print(f"      Max: {df[col].max():.4f}")
                    print(f"      Unique: {df[col].nunique()}")
            
            print(f"6. CATEGORICAL COLUMNS SUMMARY:")
            cat_cols = [c for c in df.columns if c not in num_cols]
            if cat_cols:
                for col in cat_cols[:10]:  # Show first 10
                    print(f"   {col}:")
                    print(f"      Unique values: {df[col].nunique()}")
                    print(f"      Most common: {df[col].value_counts().head(3).to_dict()}")
            
            print(f"7. DATA PREVIEW (First {preview_rows} rows):")
            print(df.head(preview_rows).to_string())
            print("="*80)

        parser=argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--model_type', type=str, default="classification")
        parser.add_argument('--enable_feature_selection', type=str, default="true")
        parser.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        parser.add_argument('--outlier_fold', type=float, default=1.5)
        parser.add_argument('--max_knn_rows', type=int, default=5000)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--rare_threshold', type=float, default=0.01)
        parser.add_argument('--enable_string_similarity', type=str, default="false")
        parser.add_argument('--missing_threshold', type=float, default=0.40)
        parser.add_argument('--unique_threshold', type=float, default=0.95)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args=parser.parse_args()

        try:
            in_path=args.in_file; target_col=args.target_column; model_type=args.model_type.strip().lower()
            enable_fs=str(args.enable_feature_selection).lower() in ("1","true","t","yes","y")
            outlier_strategy=args.outlier_strategy; outlier_fold=float(args.outlier_fold)
            max_knn_rows=int(args.max_knn_rows); detect_thresh=float(args.numeric_detection_threshold)
            rare_threshold=float(args.rare_threshold)
            enable_string_similarity=str(args.enable_string_similarity).lower() in ("1","true","t","yes","y")
            missing_threshold=float(args.missing_threshold)
            unique_threshold=float(args.unique_threshold)
            preview_rows=int(args.preview_rows)
            out_X=args.train_X; out_y=args.train_y; preproc_path=args.preprocessor; fs_path=args.feature_selector; meta_path=args.preprocess_metadata
            if not os.path.exists(in_path):
                print("ERROR input file does not exist: " + str(in_path), file=sys.stderr); sys.exit(1)
            print("Loading: " + in_path)
            df = read_with_pandas(in_path)
            print(f"Loaded shape: {df.shape}")
            print(f"All columns: {list(df.columns)}")
            if target_col not in df.columns:
                print("ERROR target column " + str(target_col) + " not found", file=sys.stderr); sys.exit(1)
            print_data_understanding(df, target_col, preview_rows)
            before=len(df); df_hashable=make_hashable_for_dupes(df); df = df_hashable.drop_duplicates(keep='first')
            dropped_dupes = before - len(df)
            print(f"Removed duplicates: {dropped_dupes} rows dropped")
            y_raw = df[target_col].copy()
            X_raw = df.drop(columns=[target_col]).copy()

            missing_report = {}
            cols_to_drop_missing = []
            for col in X_raw.columns:
                missing_pct = X_raw[col].isnull().sum() / len(X_raw)
                missing_report[col] = float(missing_pct)
                if missing_pct > missing_threshold:
                    cols_to_drop_missing.append(col)
            
            if cols_to_drop_missing:
                print(f"Dropping {len(cols_to_drop_missing)} columns with >{missing_threshold*100}% missing values:")
                for col in cols_to_drop_missing:
                    print(f"   - {col}: {missing_report[col]*100:.2f}% missing")
                X_raw = X_raw.drop(columns=cols_to_drop_missing)
            unique_report = {}
            cols_to_drop_unique = []
            for col in X_raw.columns:
                unique_pct = X_raw[col].nunique() / len(X_raw)
                unique_report[col] = float(unique_pct)
                
                # Only drop high-cardinality columns if they're NOT continuous numeric
                if unique_pct > unique_threshold:
                    # Check if it's a continuous float/numeric column
                    is_numeric = pd.api.types.is_numeric_dtype(X_raw[col])
                    is_float = pd.api.types.is_float_dtype(X_raw[col])
                    
                    # NEW: Try to convert object columns to numeric first
                    col_to_analyze = X_raw[col]
                    conversion_attempted = False
                    
                    if not is_numeric and (X_raw[col].dtype == 'object' or str(X_raw[col].dtype).startswith('string')):
                        # Attempt numeric conversion
                        converted_col = pd.to_numeric(X_raw[col], errors='coerce')
                        non_null_original = X_raw[col].notna().sum()
                        
                        if non_null_original > 0:
                            conversion_success_rate = converted_col.notna().sum() / non_null_original
                            
                            # If conversion is successful for most values (≥80%), treat as numeric
                            if conversion_success_rate >= 0.8:
                                is_numeric = True
                                is_float = pd.api.types.is_float_dtype(converted_col)
                                col_to_analyze = converted_col  # Use converted column for analysis
                                conversion_attempted = True
                                print(f"[INFO] Column '{col}' converted from object to numeric ({conversion_success_rate*100:.1f}% success)")
                    
                    # Check if values are continuous (high variance, not just integer IDs)
                    if is_numeric and len(col_to_analyze.dropna()) > 0:
                        col_std = col_to_analyze.std()
                        col_range = col_to_analyze.max() - col_to_analyze.min()
                        # If it's a float with decent variance/range, it's likely a continuous feature
                        is_continuous = is_float and col_std > 0 and col_range > 0
                    else:
                        is_continuous = False
                    
                    # Only drop if it's NOT a continuous numeric feature
                    if not is_continuous:
                        cols_to_drop_unique.append(col)
                        print(f"[INFO] Flagging '{col}' as ID-like: {unique_pct*100:.2f}% unique, continuous={is_continuous}")
                    else:
                        print(f"[INFO] Keeping '{col}' despite high uniqueness: continuous numeric feature (std={col_std:.4f}, range={col_range:.4f})")
            
            if cols_to_drop_unique:
                print(f"Dropping {len(cols_to_drop_unique)} ID-like columns with >{unique_threshold*100}% unique values:")
                for col in cols_to_drop_unique:
                    print(f"   - {col}: {unique_report[col]*100:.2f}% unique")
                X_raw = X_raw.drop(columns=cols_to_drop_unique)

            print(f"Remaining features after filtering: {X_raw.shape[1]} columns")
            print("Converting object columns in X to numeric where possible")
            X_conv, conv_report = convert_object_columns_advanced(X_raw, detect_threshold=detect_thresh, decimal_comma=False, advanced=True)
            print(f"Conversion report: {len(conv_report.get('converted', []))} columns converted")
            print("Detecting date-like columns in X")
            if meta_path and os.path.exists(meta_path):
                with open(meta_path, "r") as f:
                    meta = json.load(f)
                
                date_report = meta.get("date_report", {})
                if date_report and date_report.get("date_columns"):
                    print(f"[INFO] Applying date extraction (found {len(date_report['date_columns'])} date columns in training)...")
                    X_raw, _ = detect_and_extract_dates(X_raw, enabled=True, exclude_cols=[target_col] if target_col else None)
                    print(f"[INFO] Date extraction applied. New shape: {X_raw.shape}")
            X_dates, date_report = detect_and_extract_dates(X_conv, enabled=True, exclude_cols=None)
            print(f"Date detection report: {len(date_report.get('date_columns', []))} date columns found")
            config={
                'outlier_strategy': outlier_strategy,
                'outlier_fold': outlier_fold,
                'max_knn_rows': max_knn_rows,
                'numeric_detection_threshold': detect_thresh,
                'rare_threshold': rare_threshold,
                'enable_string_similarity': enable_string_similarity,
                'missing_threshold': missing_threshold,
                'unique_threshold': unique_threshold
            }
            y_info={'strategy': None, 'classes_': None}
            label_mapping = None
            if model_type=="regression":
                y_series=pd.to_numeric(y_raw, errors='coerce')
                y_info['strategy']="numeric_coerce"
            else:
                y_num=pd.to_numeric(y_raw, errors='coerce')
                if y_num.notna().mean() > 0.8:
                    y_series = np.floor(y_num + 0.5).astype("Int64")
                else:
                    le=LabelEncoder()
                    y_series = pd.Series(le.fit_transform(y_raw.astype(str)), index=y_raw.index)
                    y_info['strategy']="label_encode"
                    y_info['classes_']=list(map(str, le.classes_))
                    label_mapping = {str(original): int(encoded) for original, encoded in zip(le.classes_, range(len(le.classes_)))}
                    print(f"[INFO] Created label mapping: {label_mapping}")
            if model_type == "classification":
                class_counts = y_series.value_counts().sort_index()
                print("="*80)
                print("CLASS DISTRIBUTION ANALYSIS")
                print("="*80)
                for cls, count in class_counts.items():
                    pct = (count / len(y_series)) * 100
                    print(f"  Class {cls}: {count:,} samples ({pct:.2f}%)")
                min_class = class_counts.min()
                max_class = class_counts.max()
                imbalance_ratio = max_class / min_class if min_class > 0 else float('inf')
                print(f"  Imbalance ratio (max/min): {imbalance_ratio:.2f}:1")
                
                if imbalance_ratio > 5:
                    print("[WARN] Severe class imbalance detected!")
                    print("[WARN] Consider using class_weight='balanced' in training")
                elif imbalance_ratio > 2:
                    print("[INFO] Moderate class imbalance detected")
                else:
                    print("[INFO] Classes are relatively balanced")
                print("="*80)
            keep_mask_y = y_series.notna()
            dropped_y = int((~keep_mask_y).sum())
            if dropped_y>0:
                print(f"Dropped rows with null target: {dropped_y}")
            X_input = X_dates.loc[keep_mask_y].reset_index(drop=True)
            y_series = y_series.loc[keep_mask_y].reset_index(drop=True)

            print(f"Final dataset before preprocessing: {X_input.shape}")
            print_head(X_input, "X head before Preprocessor.fit", preview_rows)
            print_dtypes(X_input, "X dtypes before Preprocessor.fit")
            print("Fitting Preprocessor...")
            pre=Preprocessor()
            pre.fit(X_input, y=y_series if model_type=="classification" else y_series, config=config, target_col=target_col, model_type=model_type)

            print(f"Numeric columns identified: {len(pre.num_cols)}")
            print(f"Categorical columns identified: {len(pre.cat_cols)}")
            print("Transforming data...")
            Xp = pre.transform(X_input, training_mode=True)
            kmask = pre.global_metadata.get('kept_mask_after_outlier', None)
            if kmask is not None and isinstance(kmask, pd.Series):
                kmask = kmask.reindex(X_input.index, fill_value=True)
                Xp = Xp.reset_index(drop=True)
                y_series = y_series.loc[kmask].reset_index(drop=True)
                print(f"Rows after outlier handling: {len(Xp)}")
            print("Optimizing numeric dtypes...")
            for col in Xp.select_dtypes(include=[np.number]).columns:
                try:
                    Xp[col]=pd.to_numeric(Xp[col], downcast='float')
                except Exception:
                    pass

            # Feature selection (optional)
            if enable_fs:
                print("Performing feature selection...")
                fs=FeatureSelector(task=("classification" if model_type=="classification" else "regression"))
                fs.fit(Xp, y_series.values)
                Xs = fs.transform(Xp)
                print(f"Selected {len(fs.selected_features)} features from {Xp.shape[1]}")
                print(f"Selected features: {fs.selected_features}")
            else:
                print("Feature selection disabled, using all features")
                Xs = Xp.copy()
                fs = None
            ensure_dir_for(out_X); ensure_dir_for(out_y); ensure_dir_for(preproc_path); ensure_dir_for(fs_path); ensure_dir_for(meta_path)
            
            Xs.to_parquet(out_X, index=False)
            y_df=pd.DataFrame({target_col: y_series})
            y_df.to_parquet(out_y, index=False)
            
            with gzip.open(preproc_path, "wb") as f:
                cloudpickle.dump(pre, f)
            
            if fs is not None:
                with gzip.open(fs_path, "wb") as f:
                    cloudpickle.dump(fs, f)
            else:
                dummy_fs = type('DummySelector', (), {'selected_features': list(Xs.columns), 'transform': lambda self, X: X})()
                with gzip.open(fs_path, "wb") as f:
                    cloudpickle.dump(dummy_fs, f)

            metadata={
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'model_type': model_type,
                'target_column': target_col,
                'enable_feature_selection': enable_fs,
                'y_strategy': y_info,
                'config': config,
                'dropped_duplicates': dropped_dupes,
                'dropped_missing_cols': cols_to_drop_missing,
                'dropped_unique_cols': cols_to_drop_unique,
                'missing_report': missing_report,
                'unique_report': unique_report,
                'label_mapping': label_mapping,
                'conversion_report': conv_report,
                'date_report': date_report,
                'num_cols': pre.global_metadata.get('num_cols', []),
                'cat_cols': pre.global_metadata.get('cat_cols', []),
                'selected_features': fs.selected_features if fs else list(Xs.columns),
                'final_shape': {'rows': Xs.shape[0], 'cols': Xs.shape[1]}
            }
            with open(meta_path,'w',encoding='utf-8') as fh:
                json.dump(metadata, fh, indent=2, ensure_ascii=False)

            print("="*80)
            print("PREPROCESSING SUMMARY")
            print("="*80)
            print(f"Input shape: {df.shape}")
            print(f"Final X shape: {Xs.shape}")
            print(f"Final y shape: {y_df.shape}")
            print(f"Duplicates removed: {dropped_dupes}")
            print(f"Columns dropped (missing): {len(cols_to_drop_missing)}")
            print(f"Columns dropped (unique ID): {len(cols_to_drop_unique)}")
            print(f"Feature selection enabled: {enable_fs}")
            if enable_fs and fs:
                print(f"Features selected: {len(fs.selected_features)} / {Xp.shape[1]}")
            print(f"Final columns in X: {list(Xs.columns)}")
            print_dtypes(Xs, "Final X dtypes")
            print(f"Saved train_X to: {out_X}")
            print(f"Saved train_y to: {out_y}")
            print(f"Saved preprocessor to: {preproc_path}")
            print(f"Saved feature_selector to: {fs_path}")
            print(f"Saved metadata to: {meta_path}")
            print("="*80)
            print("SUCCESS: Preprocessing and train split complete")
            print("="*80)
        except Exception as exc:
            print("ERROR during preprocessing: " + str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --enable_feature_selection
      - {inputValue: enable_feature_selection}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --missing_threshold
      - {inputValue: missing_threshold}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --preview_rows
      - {inputValue: preview_rows}
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
