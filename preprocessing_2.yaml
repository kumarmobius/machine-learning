name: Advanced Preprocess v8.3.8
description: |
  End-to-end robust preprocessing and feature selection that reads a single dataset containing features and target...
  Rules:
  - Do not preprocess the target column.
  - If model_type is classification and target is categorical or string-like, apply label encoding.
  - If model_type is classification and target is numeric floats, round half-up to whole numbers. Example: 0.5 -> 1, 1.5 -> 2.
  - Outputs are train_X and train_y as separate parquet files. train_X does not contain the target column.
  - Prints all columns, their dtypes, features chosen, and a summary of actions.
  - Preserves the existing logic: robust file reading; duplicate removal; object->numeric parsing including percent/currency and k/m/b suffixes; date detection with calendar features; categorical encoders (onehot/ordinal/target/count) with rare-label collapsing and optional fuzzy grouping; numeric scaling per column via auto Standard/Robust/MinMax or Power+Standard; KNN and Iterative imputers; optional IQR outlier dropping or winsorizing; text TF-IDF + TruncatedSVD; cascaded feature selection: Mutual Information to auto-k followed by model-based selection with ExtraTrees.
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: Float, description: "IQR fold for outlier rule", optional: true, default: "1.5"}
  - {name: max_knn_rows, type: Integer, description: "Max rows for KNN imputer else fallback", optional: true, default: "5000"}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: rare_threshold, type: Float, description: "Rare label threshold as fraction or absolute count", optional: true, default: "0.01"}
  - {name: enable_string_similarity, type: String, description: "true/false high-cardinality grouping", optional: true, default: "false"}
  - {name: tfidf_max_features, type: Integer, description: "Max vocabulary per text column", optional: true, default: "20000"}
  - {name: svd_dims, type: Integer, description: "SVD dims per text column when applied", optional: true, default: "30"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: "20"}
outputs:
  - {name: train_X, type: Data, description: "Processed features parquet"}
  - {name: train_y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "joblib dump of fitted Preprocessor"}
  - {name: feature_selector, type: Data, description: "joblib dump of fitted FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSON metadata"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime

        import pandas as pd, numpy as np
        from sklearn.impute import SimpleImputer, KNNImputer
        from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
        from sklearn.pipeline import Pipeline
        from sklearn.model_selection import KFold
        from sklearn.experimental import enable_iterative_imputer  # noqa
        from sklearn.impute import IterativeImputer
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.decomposition import TruncatedSVD
        from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel,VarianceThreshold
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        import joblib
        import re
        import category_encoders as ce
        from rapidfuzz import process as rf_process, fuzz as rf_fuzz
        import gzip,cloudpickle
        from sklearn.inspection import permutation_importance

        # ---------- helpers ----------
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh: return fh.read(n)
            except Exception: return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            if txt[0] in ("{","["): return True
            if "{" in txt or "[" in txt: return True
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path = max(files, key=lambda p: os.path.getsize(p))
                print("Info selected file: " + path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: " + str(path))
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported format: " + str(path))

        def make_hashable_for_dupes(df):
            df = df.copy()
            for c in df.columns:
                try:
                    if df[c].apply(lambda v: isinstance(v,(list,dict,set))).any():
                        df[c] = df[c].map(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(list,dict,set)) else v)
                except Exception:
                    df[c] = df[c].astype(str)
            return df

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',','.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception:
                    try: val=float(number_clean.replace(',', '.'))
                    except Exception: return np.nan
                if mult: val*=MULTIPLIER_MAP.get(mult.lower(),1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            num_search=re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if num_search:
                try: return float(num_search.group(0).replace(',',''))
                except Exception: return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df=df.copy(); report={'converted':[],'skipped':[]}
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                if col in ('__TARGET__TMP__',): continue
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                frac = parsed.notna().sum()/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed; report['converted'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def detect_and_extract_dates(df, enabled=True, exclude_cols=None):
            df=df.copy(); report={'date_columns':[],'skipped':[]}
            if not enabled: return df, report
            exclude_cols=set(exclude_cols or [])
            cand=[c for c in df.columns if c not in exclude_cols and (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in cand:
                ser=df[col].astype(object); sample=ser.dropna().astype(str).head(500)
                if sample.empty: report['skipped'].append({'col':col,'parsable_fraction':0.0}); continue
                parsed=pd.to_datetime(sample, errors='coerce', utc=False)
                frac=parsed.notna().mean()
                if frac>=0.6:
                    full=pd.to_datetime(ser, errors='coerce', utc=False)
                    df[col+"_orig"]=df[col]; df[col]=full
                    df[col+"_year"]=df[col].dt.year
                    df[col+"_month"]=df[col].dt.month
                    df[col+"_day"]=df[col].dt.day
                    df[col+"_dayofweek"]=df[col].dt.dayofweek
                    df[col+"_is_weekend"]=df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col+"_is_month_start"]=df[col].dt.is_month_start.astype('Int64')
                    df[col+"_is_month_end"]=df[col].dt.is_month_end.astype('Int64')
                    df[col+"_days_since_epoch"]=(df[col]-pd.Timestamp("1970-01-01"))//pd.Timedelta('1D')
                    report['date_columns'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts=series.value_counts(dropna=False); n=len(series)
            rare = set(counts[counts <= threshold_count].index) if threshold_count is not None else set(counts[counts <= max(1,int(threshold_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals=[v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping={}; used=set()
            for v in vals:
                if v in used: continue
                matches=rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=score_threshold)
                group=[m[0] for m in matches]
                for g in group: mapping[g]=v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0,pd.Series(True,index=df.index)
            q1 = df[numeric_cols].quantile(0.25, numeric_only=True)
            q3 = df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr = q3 - q1
            lower = (q1 - fold * iqr).to_dict()
            upper = (q3 + fold * iqr).to_dict()
            mask = pd.Series(True, index=df.index)
            for c in numeric_cols:
                s = df[c]
                lo = lower.get(c, None); hi = upper.get(c, None)
                cond = pd.Series(True, index=df.index)
                if lo is not None:
                    cond = cond & s.ge(lo)
                if hi is not None:
                    cond = cond & s.le(hi)
                cond = cond.reindex(df.index).fillna(True)
                mask = (mask & cond)
            before=len(df)
            df2 = df.loc[mask].copy()
            removed = before - len(df2)
            df2.reset_index(drop=True, inplace=True)
            return df2, removed, mask

        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0
            q1=df[numeric_cols].quantile(0.25, numeric_only=True); q3=df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr=q3-q1
            lower=(q1 - fold*iqr).to_dict(); upper=(q3 + fold*iqr).to_dict()
            df2=df.copy()
            for c in numeric_cols:
                lo = lower.get(c, None); hi = upper.get(c, None)
                if lo is not None or hi is not None:
                    df2[c]=df2[c].clip(lower=lo, upper=hi)
            return df2,0

        def choose_scaler_for_series(s):
            v=s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0)==0: return StandardScaler()
            skew=float(v.skew()); kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            extreme_frac=float(((v - v.mean()).abs() > 3*v.std(ddof=0)).mean())
            if (v.min()>=0.0 and v.max()<=1.0): return MinMaxScaler()
            if abs(skew)>=1.0: return Pipeline([('power',PowerTransformer(method='yeo-johnson')),('std',StandardScaler())])
            if extreme_frac>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        def choose_categorical_encoder(col, series, target_series=None, train_mode=True):
            n=len(series); nunique=series.nunique(dropna=True); high_card=(nunique>max(50,0.05*n))
            if high_card:
                if target_series is not None and train_mode:
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    return 'count', None, {}
            else:
                if nunique<=10:
                    enc=OneHotEncoder(sparse_output=False, handle_unknown='ignore'); return 'onehot', enc, {}
                if target_series is not None and train_mode:
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    enc=OrdinalEncoder(); return 'ordinal', enc, {}

        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=0.3, random_state=42):
            X_col=df[col].astype(object).fillna('__NA__'); global_mean=y.mean()
            oof=pd.Series(index=df.index, dtype=float)
            from sklearn.model_selection import KFold
            kf=KFold(n_splits=min(n_splits,max(2,len(df)//10)), shuffle=True, random_state=random_state)
            for tr_idx, val_idx in kf.split(df):
                means=y.iloc[tr_idx].groupby(X_col.iloc[tr_idx]).mean()
                oof.iloc[val_idx]=X_col.iloc[val_idx].map(lambda v: means.get(v, global_mean))
            counts=X_col.value_counts(); smooth_map={}
            for cat,cnt in counts.items():
                cat_mean=y[X_col==cat].mean() if cnt>0 else global_mean
                alpha=cnt/(cnt+smoothing); smooth_map[cat]=alpha*cat_mean+(1-alpha)*global_mean
            return oof.fillna(global_mean), smooth_map, float(global_mean)

        def _should_textify(ser, min_words=3, min_len=10):
            s = ser.astype(str).fillna("")
            word_cnt_med = s.str.count(r"\\b\w+\\b").median()
            len_med = s.str.len().median()
            numeric_frac = pd.to_numeric(s, errors="coerce").notna().mean()
            uniq_ratio = ser.nunique(dropna=True) / max(len(ser), 1)
            alpha_len = s.str.replace(r"[^A-Za-z]", "", regex=True).str.len()
            alpha_ratio_med = (alpha_len / s.str.len().replace(0, 1)).median()
            tokens = s.str.findall(r"\\b\\w+\\b")
            long_token_cnt_med = tokens.map(lambda ts: sum(len(t) >= 10 for t in ts)).median()
            cond_words_or_len = (word_cnt_med >= min_words) or (len_med >= min_len) or (long_token_cnt_med >= 2)
            cond_alpha = alpha_ratio_med >= 0.5
            cond_not_numeric = numeric_frac < 0.4
            cond_not_id = uniq_ratio < 0.95
            return bool(cond_words_or_len and cond_alpha and cond_not_numeric and cond_not_id)

        class TextFeaturizer:
            def __init__(self, text_cols=None, max_tfidf_features=20000, svd_dims=30):
                self.text_cols = text_cols
                self.max_tfidf_features = int(max_tfidf_features)
                self.svd_dims = int(svd_dims)
                self.vectorizers = {}
                self.svds = {}                # col -> TruncatedSVD or None
                self.use_svd = {}             # col -> bool
                self.tfidf_names = {}         # col -> list of raw TF-IDF column names if no SVD
                self.stats_cols = {}
                self.fitted_cols = []


            def fit(self, X):
                if self.text_cols is None or len(self.text_cols)==0:
                    self.text_cols = [c for c in X.columns if (X[c].dtype=='object' or pd.api.types.is_string_dtype(X[c])) and _should_textify(X[c])]
                for col in self.text_cols:
                    s = X[col].astype(str).fillna("")
                    vec = TfidfVectorizer(max_features=self.max_tfidf_features, ngram_range=(1,2), min_df=2)
                    try:
                        M = vec.fit_transform(s.values)
                    except Exception:
                        continue
                    n_docs, n_terms = M.shape
                    
                    # decide SVD safely
                    if n_terms >= 2 and self.svd_dims > 0:
                        k = min(self.svd_dims, n_terms - 1)   # SVD requires k < n_terms
                        svd = TruncatedSVD(n_components=k, random_state=42).fit(M)
                        self.svds[col] = svd
                        self.use_svd[col] = True
                        self.tfidf_names[col] = None
                    else:
                        # skip SVD; keep raw TF-IDF column names for transform
                        self.svds[col] = None
                        self.use_svd[col] = False
                        self.tfidf_names[col] = [f"{col}__tfidf{i+1}" for i in range(n_terms)]
                    
                    self.vectorizers[col] = vec
                    self.stats_cols[col] = [f"{col}__len", f"{col}__word_count", f"{col}__digit_count", f"{col}__alpha_ratio"]
                    self.fitted_cols.append(col)
                return self

            def transform(self, X):
                X = X.copy()
                out = []
                for col in self.fitted_cols:
                    if col not in X.columns:
                        X[col] = ""
                    s = X[col].astype(str)
                    vec = self.vectorizers[col]; svd = self.svds[col]
                    try:
                        M = vec.transform(s.values)
                        if self.use_svd.get(col, False):
                            Z = svd.transform(M)
                            Z_df = pd.DataFrame(Z, index=X.index,
                                                columns=[f"{col}__svd{i+1}" for i in range(Z.shape[1])])
                        else:
                            # pass raw TF-IDF if SVD was skipped
                            arr = M.toarray()
                            cols = self.tfidf_names.get(col, [f"{col}__tfidf{i+1}" for i in range(arr.shape[1])])
                            Z_df = pd.DataFrame(arr, index=X.index, columns=cols)
                    except Exception:
                        Z_df = pd.DataFrame(index=X.index)
                    stats = pd.DataFrame({
                        f"{col}__len": s.str.len(),
                        f"{col}__word_count": s.str.count(r"\\b\\w+\\b"),
                        f"{col}__digit_count": s.str.count(r"\\d"),
                        f"{col}__alpha_ratio": s.str.replace(r"[^A-Za-z]", "", regex=True).str.len() / s.str.len().replace(0,1)
                    }, index=X.index)
                    out.append(pd.concat([Z_df, stats], axis=1))
                if out:
                    feat = pd.concat(out, axis=1)
                    X = pd.concat([X.drop(columns=[c for c in self.fitted_cols if c in X.columns]), feat], axis=1)
                return X

        DATE_PATTERNS = [
              # ISO date
              (re.compile(r"^\d{4}-\d{2}-\d{2}$"),                     "%Y-%m-%d"),
              (re.compile(r"^\d{4}/\d{2}/\d{2}$"),                     "%Y/%m/%d"),
              (re.compile(r"^\d{4}\.\d{2}\.\d{2}$"),                   "%Y.%m.%d"),
              (re.compile(r"^\d{8}$"),                                 "%Y%m%d"),
          
              # ISO date + time (space or T), seconds optional, fractional optional
              (re.compile(r"^\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}$"),                  "%Y-%m-%d %H:%M"),     # will pre-normalize 'T' to ' ' below
              (re.compile(r"^\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}$"),           "%Y-%m-%d %H:%M:%S"),
              (re.compile(r"^\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}\.\d{1,6}$"),  "%Y-%m-%d %H:%M:%S.%f"),
          
              # ISO + timezone (Z or ±HH:MM or ±HHMM)
              (re.compile(r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$"),             "%Y-%m-%dT%H:%M:%SZ"),
              (re.compile(r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{1,6}Z$"),    "%Y-%m-%dT%H:%M:%S.%fZ"),
              (re.compile(r"^\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}([+-]\d{2}:\d{2}|[+-]\d{4})$"),      "%Y-%m-%d %H:%M:%S%z"),
              (re.compile(r"^\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}\.\d{1,6}([+-]\d{2}:\d{2}|[+-]\d{4})$"), "%Y-%m-%d %H:%M:%S.%f%z"),
          
              # Month name variants
              (re.compile(r"^\d{1,2} [A-Za-z]{3} \d{4}$"),             "%d %b %Y"),            # 11 Nov 2025
              (re.compile(r"^\d{1,2} [A-Za-z]+ \d{4}$"),               "%d %B %Y"),            # 11 November 2025
              (re.compile(r"^[A-Za-z]{3} \d{1,2}, \d{4}$"),            "%b %d, %Y"),           # Nov 11, 2025
              (re.compile(r"^[A-Za-z]+ \d{1,2}, \d{4}$"),              "%B %d, %Y"),           # November 11, 2025
              (re.compile(r"^\d{1,2} [A-Za-z]{3} \d{4} \d{2}:\d{2}:\d{2}$"), "%d %b %Y %H:%M:%S"),
              (re.compile(r"^\d{1,2} [A-Za-z]+ \d{4} \d{2}:\d{2}:\d{2}$"),       "%d %B %Y %H:%M:%S"),
          
              # RFC 2822-like
              (re.compile(r"^[A-Za-z]{3}, \d{1,2} [A-Za-z]{3} \d{4} \d{2}:\d{2}:\d{2} [+-]\d{4}$"),
               "%a, %d %b %Y %H:%M:%S %z"),
              (re.compile(r"^\d{1,2} [A-Za-z]{3} \d{4} \d{2}:\d{2}:\d{2} [+-]\d{4}$"),
               "%d %b %Y %H:%M:%S %z"),
          
              # Two-digit years
              (re.compile(r"^\d{1,2}/\d{1,2}/\d{2}$"), "%m/%d/%y"),    # will disambiguate below if needed
              (re.compile(r"^\d{1,2}-\d{1,2}-\d{2}$"), "%m-%d-%y"),
              (re.compile(r"^\d{2}-\d{2}-\d{2}$"),      "%y-%m-%d"),
          ]

        # Ambiguous day-first vs month-first (same regex but two formats). We'll choose best fit.
        AMBIGUOUS = [
            (re.compile(r"^\d{1,2}/\d{1,2}/\d{4}$"), ("%d/%m/%Y", "%m/%d/%Y")),
            (re.compile(r"^\d{1,2}-\d{1,2}-\d{4}$"), ("%d-%m-%Y", "%m-%d-%Y")),
        ]
        def _matches_frac(sample_str: pd.Series, pat: re.Pattern) -> float:
            return sample_str.str.match(pat, na=False).mean()
        
        def _try_parse_with_format(s: pd.Series, fmt: str) -> pd.Series:
            # pre-normalize T to space for patterns that expect space
            if " %H:%M" in fmt and "T" in s.astype(str).str[10:11].unique().tolist():
                s = s.astype(str).str.replace("T", " ", n=1, regex=False)
            out = pd.to_datetime(s, format=fmt, errors="coerce", utc=("%z" in fmt))
            if "%z" in fmt:
                out = out.dt.tz_localize(None)
            return out
        
        def _choose_ambiguous(sample: pd.Series, fmts: tuple[str, str]) -> str:
            a = _try_parse_with_format(sample, fmts[0])
            b = _try_parse_with_format(sample, fmts[1])
            # score = share parsed + month/day sanity bonus
            def score(ser):
                ok = ser.notna().mean()
                return ok
            sa, sb = score(a), score(b)
            return fmts[0] if sa >= sb else fmts[1]
        
        def _maybe_epoch(series: pd.Series, agree_frac: float = 0.9) -> pd.Series | None:
            s = series.dropna().astype(str)
            if s.empty: return None
            # 10 digits => seconds, 13 => ms, 16 => µs
            if (s.str.fullmatch(r"\d{10}", na=False).mean() >= agree_frac):
                dt = pd.to_datetime(pd.to_numeric(series, errors="coerce"), unit="s", utc=True)
                return dt.dt.tz_localize(None)
            if (s.str.fullmatch(r"\d{13}", na=False).mean() >= agree_frac):
                dt = pd.to_datetime(pd.to_numeric(series, errors="coerce"), unit="ms", utc=True)
                return dt.dt.tz_localize(None)
            if (s.str.fullmatch(r"\d{16}", na=False).mean() >= agree_frac):
                dt = pd.to_datetime(pd.to_numeric(series, errors="coerce"), unit="us", utc=True)
                return dt.dt.tz_localize(None)
            # yyyymmddhhmmss (14) common in logs
            if (s.str.fullmatch(r"\d{14}", na=False).mean() >= agree_frac):
                dt = pd.to_datetime(series, format="%Y%m%d%H%M%S", errors="coerce", utc=True)
                return dt.dt.tz_localize(None)
            return None
        
        # ---- 3) Main detector ----
        def safe_to_datetime(s: pd.Series, agree_frac: float = 0.9) -> pd.Series:
            if not isinstance(s, pd.Series):
                return s
        
            sample = s.dropna().astype(str).head(400)
            if sample.empty:
                return s
        
            # a) Epoch-like
            epoch_parsed = _maybe_epoch(s, agree_frac=agree_frac)
            if epoch_parsed is not None:
                return epoch_parsed
        
            # b) Ambiguous D/M/Y vs M/D/Y
            for pat, fmts in AMBIGUOUS:
                if _matches_frac(sample, pat) >= agree_frac:
                    best_fmt = _choose_ambiguous(sample, fmts)
                    parsed = _try_parse_with_format(s.astype(str), best_fmt)
                    return parsed

            best = None
            best_fmt = None
            for pat, fmt in DATE_PATTERNS:
                frac = _matches_frac(sample, pat)
                if frac >= agree_frac and (best is None or frac > best):
                    best, best_fmt = frac, fmt
        
            if best_fmt is None:
                # d) ISO 8601 generic fallback when most look ISO-ish (avoid noisy fallback)
                isoish = sample.str.match(r"^\d{4}-\d{2}-\d{2}", na=False).mean()
                if isoish >= agree_frac * 0.9:
                    # Let pandas handle mixed ISO without raising the earlier warning.
                    # We normalize 'T' to ' ' to keep it consistent.
                    tmp = s.astype(str).str.replace("T", " ", n=1, regex=False)
                    dt  = pd.to_datetime(tmp, errors="coerce", utc=True, infer_datetime_format=True)
                    return dt.dt.tz_localize(None)
                return s
        
            return _try_parse_with_format(s.astype(str), best_fmt)
        class Preprocessor:
            def __init__(self):
                self.num_cols=[]; self.cat_cols=[]; self.col_config={}; self.global_metadata={}
                self.text = None

            def fit(self, df, y=None, config=None, text_params=None, target_col=None):
                self.global_metadata['config']=config or {}
                self.global_metadata['text_params']=text_params or {}
                self.global_metadata['target_col']=target_col
                nrows=len(df)
                self.num_cols=df.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols=[c for c in df.columns if c not in self.num_cols]
                # clone raw object cols to *_orig for stable text features (do not clone target)
                tcol = target_col or self.global_metadata.get('target_col')
                raw_text_bases = [c for c in df.columns if df[c].dtype == 'object' and c != tcol]
                for c in raw_text_bases:
                    df[c + "_orig"] = df[c].astype(str)
                
                self.global_metadata['text_orig_cols'] = [c + "_orig" for c in raw_text_bases]
                
                self.text = TextFeaturizer(
                    text_cols=self.global_metadata['text_orig_cols'],
                    max_tfidf_features=int(self.global_metadata['text_params'].get('tfidf_max_features', 20000)),
                    svd_dims=int(self.global_metadata['text_params'].get('svd_dims', 30))
                ).fit(df)


                tcol = self.global_metadata.get('target_col')
                protected_text = set(self.global_metadata.get('text_orig_cols', []))
                for c in df.columns:
                    if c == tcol or c in protected_text:
                        continue
                    if df[c].dtype == 'object':
                        df[c] = safe_to_datetime(df[c])
                df = df.copy()
                for c_orig in self.global_metadata.get('text_orig_cols', []):
                    base = c_orig[:-5]  # strip "_orig"
                    if c_orig not in df.columns:
                        if base in df.columns:
                            df[c_orig] = df[base].astype(str)
                        else:
                            # base missing too; create empty placeholder
                            df[c_orig] = ""
                
                if self.text is not None:
                    df_txt = self.text.transform(df)
                else:
                    df_txt = df

                self.num_cols = [c for c in df_txt.columns if pd.api.types.is_numeric_dtype(df_txt[c])]
                self.cat_cols = [c for c in df_txt.columns if c not in self.num_cols]


                for c in self.num_cols:
                    s=df_txt[c]; cfg={}
                    missing_frac=s.isna().mean(); cfg['missing_frac']=float(missing_frac)
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['skew']=float(s.dropna().skew()) if s.dropna().shape[0]>2 else 0.0
                    cfg['kurtosis']=float(s.dropna().kurtosis()) if s.dropna().shape[0]>3 else 0.0
                    if missing_frac==0: cfg['imputer']=('none',None)
                    elif missing_frac<0.02:
                        imp = SimpleImputer(strategy='median')
                        imp.fit(s.to_frame(name=c))     # keep column name at fit time
                        cfg['imputer'] = ('simple_median', imp)
                    else:
                        if nrows<=self.global_metadata['config'].get('max_knn_rows',5000) and missing_frac<0.25:
                            cfg['imputer']=('knn',{'n_neighbors':5})
                        else:
                            cfg['imputer']=('iterative',{'max_iter':10,'random_state':0})
                    cfg['scaler_choice']='auto'
                    self.col_config[c]=cfg
                for c in self.cat_cols:
                    s=df_txt[c].astype(object); cfg={}
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['missing_frac']=float(s.isna().mean())
                    cfg['high_card']=cfg['n_unique']>max(50,0.05*nrows)
                    cfg['rare_threshold_frac']=self.global_metadata['config'].get('rare_threshold',0.01)
                    enc_name,enc_obj,_=choose_categorical_encoder(c,s,(y if y is not None else None),train_mode=(y is not None))
                    cfg['encoder_type']=enc_name; cfg['encoder_obj']=enc_obj
                    self.col_config[c]=cfg
                if y is not None:
                    for c in self.cat_cols:
                        cfg=self.col_config[c]
                        if cfg['encoder_type']=='target':
                            _, mapping, global_mean = fit_target_encoder_kfold(df_txt, c, y, n_splits=5)
                            cfg['target_mapping']=mapping; cfg['target_global_mean']=float(global_mean)
                        elif cfg['encoder_type']=='count':
                            counts=df_txt[c].astype(object).value_counts().to_dict(); cfg['count_map']=counts
                self.global_metadata['num_cols']=self.num_cols; self.global_metadata['cat_cols']=self.cat_cols
                return self

            def transform(self, df, training_mode=False):
                import numpy as np, pandas as pd
            
                df = df.copy()
                target_col = self.global_metadata.get('target_col')
                y_hold = df[target_col] if (target_col is not None and target_col in df.columns) else None
                df.replace(["NaN","nan","None","null","INF","-INF","Inf","-Inf","inf","-inf"], np.nan, inplace=True)
            
                # 1) Safe date parsing on non-target object columns
                for c in df.columns:
                    if c == target_col:
                        continue
                    if df[c].dtype == 'object':
                        df[c] = safe_to_datetime(df[c])
            
                # 2) Apply text featurizer ONCE to non-target columns, then reattach target
                X_no_target = df.drop(columns=[target_col]) if (target_col is not None and target_col in df.columns) else df
                if self.text is not None:
                    X_txt = self.text.transform(X_no_target)
                else:
                    X_txt = X_no_target
                if y_hold is not None:
                    df = pd.concat([X_txt, y_hold], axis=1)
                else:
                    df = X_txt
            
                # 3) Normalize common string tokens to NaN
                df.replace(["NaN","nan","None","null","INF","-INF","Inf","-Inf","inf","-inf"], np.nan, inplace=True)
            
                # 4) Ensure the same schema as at fit-time
                expected_num = list(self.num_cols)
                expected_cat = list(self.cat_cols)
                expected_all = set(expected_num + expected_cat)
                # add missing columns with safe defaults
                for c in expected_all:
                    if c == target_col:
                        continue
                    if c not in df.columns:
                        if c in expected_num:
                            df[c] = np.nan
                        else:
                            df[c] = "__MISSING__"
                # drop extras not known at fit-time (but keep target)
                extra = [c for c in df.columns if c != target_col and c not in expected_all]
                if extra:
                    df.drop(columns=extra, inplace=True)
            
                # 5) Force numeric dtype on learned numeric cols
                for c in expected_num:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors='coerce')
            
                # 6) Pre-fill for KNN/Iterative imputers
                temp_df = df.copy()
                for c in expected_num:
                    imputer_info = self.col_config.get(c, {}).get('imputer', ('none', None))
                    if imputer_info[0] in ('knn', 'iterative'):
                        med = pd.to_numeric(temp_df[c], errors='coerce').median()
                        temp_df[c] = temp_df[c].fillna(med)
            
                # 7) Fit chosen scalers column-wise on temp_df
                scaler_objs = {}
                for c in expected_num:
                    sample = temp_df[c]
                    scaler = choose_scaler_for_series(sample)
                    try:
                        scaler.fit(sample.values.reshape(-1,1))
                    except Exception:
                        from sklearn.preprocessing import StandardScaler
                        scaler = StandardScaler()
                        scaler.fit(sample.values.reshape(-1,1))
                    scaler_objs[c] = scaler
                    self.col_config[c]['scaler_obj'] = scaler
            
                # 8) KNN imputation in scaled space with inverse-transform
                num_matrix = temp_df[expected_num].copy()
                scaled_matrix = np.zeros_like(num_matrix.values, dtype=float)
                for i, c in enumerate(expected_num):
                    sc = scaler_objs[c]
                    col_vals = num_matrix[c].values.reshape(-1,1)
                    try:
                        scaled_col = sc.transform(col_vals).reshape(-1)
                    except Exception:
                        from sklearn.preprocessing import StandardScaler
                        scaled_col = StandardScaler().fit_transform(col_vals).reshape(-1)
                    scaled_matrix[:, i] = scaled_col
                scaled_df = pd.DataFrame(scaled_matrix, columns=expected_num, index=num_matrix.index)
            
                need_knn = [c for c in expected_num if self.col_config[c]['imputer'][0] == 'knn']
                if need_knn:
                    from sklearn.impute import KNNImputer
                    base_imp = self.col_config[expected_num[0]]['imputer'][1] if expected_num else None
                    k = base_imp.get('n_neighbors', 5) if isinstance(base_imp, dict) else 5
                    knn = KNNImputer(n_neighbors=k)
                    imputed_scaled = knn.fit_transform(scaled_df)
                    imputed_scaled_df = pd.DataFrame(imputed_scaled, columns=expected_num, index=scaled_df.index)
                    for c in expected_num:
                        sc = scaler_objs[c]
                        try:
                            inv = sc.inverse_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        except Exception:
                            inv = imputed_scaled_df[c].values.reshape(-1)
                        df[c] = inv
            
                # 9) Iterative imputer (original scale)
                need_iter = [c for c in expected_num if self.col_config[c]['imputer'][0] == 'iterative']
                if need_iter:
                    from sklearn.experimental import enable_iterative_imputer  # noqa: F401
                    from sklearn.impute import IterativeImputer
                    iter_max_iter = 10
                    base_imp2 = self.col_config[expected_num[0]]['imputer'][1] if expected_num else None
                    if isinstance(base_imp2, dict):
                        iter_max_iter = base_imp2.get('max_iter', 10)
                    iter_imp = IterativeImputer(max_iter=iter_max_iter, random_state=0)
                    arr = df[expected_num].astype(float).replace([np.inf, -np.inf], np.nan).values
                    iter_out = iter_imp.fit_transform(arr)
                    df[expected_num] = pd.DataFrame(iter_out, columns=expected_num, index=df.index)
                    for c in need_iter:
                        self.col_config[c]['imputer_obj'] = iter_imp
            
                # 10) Simple median imputer
                for c in expected_num:
                    cfg = self.col_config[c]
                    if cfg['imputer'][0] == 'simple_median':
                        imp = cfg['imputer'][1]
                        df[c] = imp.transform(df[[c]])
            
                # 11) Optional outlier handling during training only
                cfg_global = self.global_metadata.get('config') or {}
                if training_mode:
                    if cfg_global.get('outlier_strategy', 'drop_iqr') == 'drop_iqr':
                        df2, n_removed, kept_mask = drop_outliers_iqr(df, expected_num, fold=cfg_global.get('outlier_fold', 1.5))
                        self.global_metadata['outliers_removed'] = int(n_removed)
                        self.global_metadata['kept_mask_after_outlier'] = kept_mask
                        df = df2
                    elif cfg_global.get('outlier_strategy') == 'winsorize':
                        df, _ = winsorize_df(df, expected_num, fold=cfg_global.get('outlier_fold', 1.5))
                        self.global_metadata['kept_mask_after_outlier'] = pd.Series(True, index=df.index)
            
                # 12) Final scaling on numeric columns
                for c in expected_num:
                    sc = self.col_config[c].get('scaler_obj')
                    if sc is None:
                        sc = choose_scaler_for_series(df[c].fillna(df[c].median()))
                    try:
                        df[c] = sc.transform(df[[c]].values).reshape(-1)
                    except Exception:
                        from sklearn.preprocessing import StandardScaler
                        try:
                            df[c] = sc.fit_transform(df[[c]].values).reshape(-1)
                        except Exception:
                            df[c] = StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                    self.col_config[c]['scaler_obj'] = sc
            
                # 13) Categorical encodings; never touch target
                for c in [cc for cc in expected_cat if cc in df.columns and cc != target_col]:
                    cfg = self.col_config[c]; s = df[c].astype(object)
                    rare_thresh = cfg.get('rare_threshold_frac', 0.01)
                    if rare_thresh < 1.0:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thresh, threshold_count=None)
                    else:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thresh))
                    df[c] = collapsed; cfg['rare_values'] = list(rare_set)
                    if cfg_global.get('enable_string_similarity', False) and cfg['n_unique'] > 20:
                        grouped, mapping = string_similarity_group(df[c], score_threshold=90)
                        df[c] = grouped; cfg['string_similarity_map'] = mapping
                    enc_type = cfg.get('encoder_type')
                    if enc_type == 'onehot':
                        from sklearn.preprocessing import OneHotEncoder
                        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                        resh = df[[c]].astype(str); ohe.fit(resh); arr = ohe.transform(resh)
                        cols = [f"{c}__{cat}" for cat in ohe.categories_[0]]
                        df_ohe = pd.DataFrame(arr, columns=cols, index=df.index)
                        df = pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                        cfg['encoder_obj'] = ohe; cfg['ohe_columns'] = cols
                    elif enc_type == 'ordinal':
                        from sklearn.preprocessing import OrdinalEncoder
                        ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                        resh = df[[c]].astype(object)
                        try:
                            ord_enc.fit(resh); df[c] = ord_enc.transform(resh).astype(float); cfg['encoder_obj'] = ord_enc
                        except Exception:
                            df[c] = df[c].astype('category').cat.codes.astype(float)
                    elif enc_type == 'count':
                        cnt_map = df[c].value_counts().to_dict()
                        df[c] = df[c].map(lambda x: cnt_map.get(x, 0)); cfg['count_map'] = cnt_map
                    elif enc_type == 'target':
                        mapping = cfg.get('target_mapping', {}); global_mean = cfg.get('target_global_mean', 0.0)
                        df[c] = df[c].map(lambda x: mapping.get(x, global_mean))
                    else:
                        df[c] = df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)
            
                # 14) Coerce any leftover objects to numeric if possible
                for c in list(df.columns):
                    if df[c].dtype == 'object' and c != target_col:
                        try:
                            df[c] = pd.to_numeric(df[c], errors='coerce')
                        except Exception:
                            pass
                df.replace([np.inf, -np.inf], np.nan, inplace=True)
                return df


            def save(self, path): joblib.dump(self, path)
            @staticmethod
            def load(path): return joblib.load(path)

        class FeatureSelector:
            # Three-stage selector:
            #   1) Filter pass: near-zero variance, then correlation pruning (|rho| >= 0.95).
            #   2) Relevance pass: MI or greedy mRMR to top k, where k = ceil(2*sqrt(p)).
            #   3) Black-box pass: permutation importance on ExtraTrees; drop <= 0 contributions.
        
            # Public attributes:
            #   - selected_features: list[str]
            
            # fixed knobs to preserve constructor signature
            _VAR_THRESH = 1e-5
            _CORR_THRESH = 0.95
            _RELEVANCE = "mi"          # "mi" or "mrmr"
            _PI_REPEATS = 5
            _RANDOM_STATE = 42
        
            def __init__(self, task):
                self.task = str(task).lower()
                self.selected_features = []
        
                # internals for reproducibility/debug
                self._filter_keep_ = []
                self._relevance_keep_ = []
                self._mi_scores_ = pd.Series(dtype=float)
                self._pi_importance_ = pd.Series(dtype=float)
        
            def _auto_k(self, p: int) -> int:
                return max(5, min(int(math.ceil(math.sqrt(p) * 2.0)), p))
        
            # Filter pass helpers 
            @staticmethod
            def _nzv_keep(X: pd.DataFrame, threshold: float) -> list:
                if X.shape[1] == 0:
                    return []
                vt = VarianceThreshold(threshold=threshold)
                vt.fit(X.values)
                mask = vt.get_support()
                return list(X.columns[mask])
        
            @staticmethod
            def _corr_prune_keep(X: pd.DataFrame, corr_thresh: float) -> list:
                cols = list(X.columns)
                if len(cols) <= 1:
                    return cols
                # use Spearman for monotonic robustness
                corr = X.corr(method="spearman").abs()
                keep = set(cols)
                # greedy: drop the feature with larger mean absolute corr when a pair exceeds threshold
                while True:
                    # find maximum correlation pair among current 'keep'
                    sub = corr.loc[list(keep), list(keep)]
                    np.fill_diagonal(sub.values, 0.0)
                    max_val = sub.values.max()
                    if not np.isfinite(max_val) or max_val < corr_thresh:
                        break
                    # locate the pair
                    idx = np.unravel_index(np.argmax(sub.values), sub.shape)
                    a = sub.index[idx[0]]
                    b = sub.columns[idx[1]]
                    mean_a = sub[a].mean()
                    mean_b = sub[b].mean()
                    drop = a if mean_a >= mean_b else b
                    keep.remove(drop)
                return list(keep)
        
            #  Relevance pass helpers 
            def _mi_scores(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:
                cols = list(X.columns)
                if self.task == "classification":
                    scores = mutual_info_classif(X.values, y, random_state=self._RANDOM_STATE)
                else:
                    # treat as regression
                    y_num = pd.to_numeric(y, errors="ignore")
                    scores = mutual_info_regression(X.values, y_num, random_state=self._RANDOM_STATE)
                return pd.Series(scores, index=cols).fillna(0.0)
        
            @staticmethod
            def _topk_by_series(scores: pd.Series, k: int) -> list:
                k = max(1, min(k, scores.shape[0]))
                return list(scores.sort_values(ascending=False).head(k).index)
        
            @staticmethod
            def _mrmr_greedy(X: pd.DataFrame, mi: pd.Series, k: int) -> list:
                # redundancy by absolute Spearman correlation
                k = max(1, min(k, X.shape[1]))
                corr = X.corr(method="spearman").abs().fillna(0.0)
                selected = []
                candidates = list(X.columns)
                while len(selected) < k and candidates:
                    best_feat = None
                    best_score = -1e18
                    for c in candidates:
                        rel = float(mi.get(c, 0.0))
                        if not selected:
                            score = rel
                        else:
                            red = float(corr.loc[c, selected].mean())
                            score = rel - red
                        if score > best_score:
                            best_score = score
                            best_feat = c
                    selected.append(best_feat)
                    candidates.remove(best_feat)
                return selected
        
            # Black-box pass helper
            def _permutation_keep(self, X: pd.DataFrame, y: pd.Series) -> (list, pd.Series):
                if self.task == "classification":
                    est = ExtraTreesClassifier(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                else:
                    est = ExtraTreesRegressor(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                est.fit(X, y)
                pi = permutation_importance(
                    est, X, y, n_repeats=self._PI_REPEATS, random_state=self._RANDOM_STATE, n_jobs=-1
                )
                imp = pd.Series(pi.importances_mean, index=X.columns).fillna(0.0)
                keep = list(imp[imp > 0.0].index)
                if not keep:
                    # fallback: keep at least one feature with highest PI
                    keep = [imp.sort_values(ascending=False).index[0]]
                return keep, imp
        
            # Public API 
            def fit(self, X: pd.DataFrame, y) -> "FeatureSelector":
                # make sure y is a 1D array-like
                if isinstance(y, (pd.DataFrame, pd.Series)):
                    y_arr = pd.Series(y).values.ravel()
                else:
                    y_arr = np.asarray(y).ravel()
        
                # 1) filter pass
                keep1 = self._nzv_keep(X, self._VAR_THRESH)
                X1 = X[keep1]
                keep2 = self._corr_prune_keep(X1, self._CORR_THRESH)
                Xf = X1[keep2]
                self._filter_keep_ = list(Xf.columns)
        
                # 2) relevance pass
                mi = self._mi_scores(Xf, y_arr)
                self._mi_scores_ = mi.copy()
                k = self._auto_k(Xf.shape[1])
                if self._RELEVANCE == "mrmr":
                    keep_rel = self._mrmr_greedy(Xf, mi, k)
                else:
                    keep_rel = self._topk_by_series(mi, k)
                Xr = Xf[keep_rel]
                self._relevance_keep_ = list(Xr.columns)
        
                # 3) black-box pass (permutation importance)
                keep_pi, imp = self._permutation_keep(Xr, y_arr)
                Xb = Xr[keep_pi]
                self._pi_importance_ = imp
                self.selected_features = list(Xb.columns)
                return self
        
            def transform(self, X: pd.DataFrame) -> pd.DataFrame:
                # Be strict on column order but tolerant to missing/extra columns.
                return X.reindex(columns=self.selected_features, fill_value=0.0)
        
            def save(self, path: str) -> None:
                joblib.dump(self, path)
        
            @staticmethod
            def load(path: str) -> "FeatureSelector":
                return joblib.load(path)

        def print_head(df, title):
            print(title)
            try: print(df.head().to_string())
            except Exception: print(df.head())

        def print_dtypes(df, title):
            print(title)
            try: 
                d={c:str(df[c].dtype) for c in df.columns}
                print(d)
            except Exception:
                print(df.dtypes.astype(str))

        parser=argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--model_type', type=str, default="classification")
        parser.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        parser.add_argument('--outlier_fold', type=float, default=1.5)
        parser.add_argument('--max_knn_rows', type=int, default=5000)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--rare_threshold', type=float, default=0.01)
        parser.add_argument('--enable_string_similarity', type=str, default="false")
        parser.add_argument('--tfidf_max_features', type=int, default=20000)
        parser.add_argument('--svd_dims', type=int, default=30)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args=parser.parse_args()

        try:
            in_path=args.in_file; target_col=args.target_column; model_type=args.model_type.strip().lower()
            outlier_strategy=args.outlier_strategy; outlier_fold=float(args.outlier_fold)
            max_knn_rows=int(args.max_knn_rows); detect_thresh=float(args.numeric_detection_threshold)
            rare_threshold=float(args.rare_threshold)
            enable_string_similarity=str(args.enable_string_similarity).lower() in ("1","true","t","yes","y")
            tfidf_max_features=int(args.tfidf_max_features); svd_dims=int(args.svd_dims)
            preview_rows=int(args.preview_rows)
            out_X=args.train_X; out_y=args.train_y; preproc_path=args.preprocessor; fs_path=args.feature_selector; meta_path=args.preprocess_metadata

            if not os.path.exists(in_path):
                print("ERROR input file does not exist: " + str(in_path), file=sys.stderr); sys.exit(1)
            print("Loading: " + in_path)
            df = read_with_pandas(in_path)
            print("Loaded shape: " + str(df.shape))
            print("All columns: " + str(list(df.columns)))
            print_dtypes(df, "Dtypes before any processing")

            if target_col not in df.columns:
                print("ERROR target column " + str(target_col) + " not found", file=sys.stderr); sys.exit(1)

            # Drop duplicates
            before=len(df); df_hashable=make_hashable_for_dupes(df); df = df_hashable.drop_duplicates(keep='first')
            print("Removed duplicates: " + str(before - len(df)) + " rows dropped")

            # Pull target aside first and never preprocess it with X
            y_raw = df[target_col].copy()
            X_raw = df.drop(columns=[target_col]).copy()

            # Convert object columns in X only
            print("Converting object columns in X to numeric where possible")
            X_conv, conv_report = convert_object_columns_advanced(X_raw, detect_threshold=detect_thresh, decimal_comma=False, advanced=True)
            print("Conversion report preview: " + json.dumps(conv_report, default=str)[:800])

            # Detect dates in X only
            print("Detecting date-like columns in X")
            X_dates, date_report = detect_and_extract_dates(X_conv, enabled=True, exclude_cols=None)
            print("Date parse report preview: " + json.dumps(date_report, default=str)[:800])

            # Build config
            config={
                'outlier_strategy': outlier_strategy,
                'outlier_fold': outlier_fold,
                'max_knn_rows': max_knn_rows,
                'numeric_detection_threshold': detect_thresh,
                'rare_threshold': rare_threshold,
                'enable_string_similarity': enable_string_similarity
            }
            text_params={'tfidf_max_features': tfidf_max_features, 'svd_dims': svd_dims}

            # Prepare y according to task
            y_info={'strategy': None, 'classes_': None}
            if model_type=="regression":
                y_series=pd.to_numeric(y_raw, errors='coerce')
                y_info['strategy']="numeric_coerce"
            else:
                # try numeric rounding half-up
                y_num=pd.to_numeric(y_raw, errors='coerce')
                if y_num.notna().mean() > 0.8:
                    y_series = np.floor(y_num + 0.5).astype("Int64")
                    y_info['strategy']="round_half_up"
                else:
                    # string labels -> label encode
                    le=LabelEncoder()
                    y_series = pd.Series(le.fit_transform(y_raw.astype(str)), index=y_raw.index)
                    y_info['strategy']="label_encode"
                    y_info['classes_']=list(map(str, le.classes_))

            # Drop rows with null y
            keep_mask_y = y_series.notna()
            dropped_y = int((~keep_mask_y).sum())
            if dropped_y>0:
                print("Dropped rows with null target: " + str(dropped_y))
            X_input = X_dates.loc[keep_mask_y].reset_index(drop=True)
            y_series = y_series.loc[keep_mask_y].reset_index(drop=True)

            print_head(X_input, "X head before Preprocessor.fit")
            print_dtypes(X_input, "X dtypes before Preprocessor.fit")

            # Fit preprocessor on X only, pass y for encoders if classification
            pre=Preprocessor()
            pre.fit(X_input, y=y_series if model_type=="classification" else y_series, config=config, text_params=text_params, target_col=target_col)

            print("Plan numeric columns: " + str(pre.num_cols))
            print("Plan categorical columns: " + str(pre.cat_cols))

            # Transform X with training_mode True to apply outlier rule
            Xp = pre.transform(X_input, training_mode=True)

            # Align y to kept_mask if any rows were dropped by outlier removal
            kmask = pre.global_metadata.get('kept_mask_after_outlier', None)
            if kmask is not None and isinstance(kmask, pd.Series):
                # kmask indexes the input passed into transform prior to drop; align to X_input index length
                kmask = kmask.reindex(X_input.index, fill_value=True)
                Xp = Xp.reset_index(drop=True)
                y_series = y_series.loc[kmask].reset_index(drop=True)

            # Ensure numeric dtypes optimal
            print("Optimizing numeric dtypes")
            for col in Xp.select_dtypes(include=[np.number]).columns:
                try:
                    Xp[col]=pd.to_numeric(Xp[col], downcast='float')
                except Exception:
                    pass

            # Feature selection
            print("Fitting feature selector")
            fs=FeatureSelector(task=("classification" if model_type=="classification" else "regression"))
            fs.fit(Xp, y_series.values)
            Xs = fs.transform(Xp)
            print("Selected features: " + str(fs.selected_features))

            # Save outputs
            ensure_dir_for(out_X); ensure_dir_for(out_y); ensure_dir_for(preproc_path); ensure_dir_for(fs_path); ensure_dir_for(meta_path)
            Xs.to_parquet(out_X, index=False)
            y_df=pd.DataFrame({target_col: y_series})
            y_df.to_parquet(out_y, index=False)
            with gzip.open(preproc_path, "wb") as f:
                cloudpickle.dump(pre, f)
            with gzip.open(fs_path, "wb") as f:
                cloudpickle.dump(fs, f)

            metadata={
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'model_type': model_type,
                'target_column': target_col,
                'y_strategy': y_info,
                'config': config,
                'text_params': text_params,
                'conversion_report': conv_report,
                'date_report': date_report,
                'num_cols': pre.global_metadata.get('num_cols', []),
                'cat_cols': pre.global_metadata.get('cat_cols', []),
                'selected_features': fs.selected_features
            }
            with open(meta_path,'w',encoding='utf-8') as fh:
                json.dump(metadata, fh, indent=2, ensure_ascii=False)

            # Final prints
            print("Columns used in final X: " + str(list(Xs.columns)))
            print_dtypes(Xs, "Final X dtypes")
            print("Saved train_X to: " + out_X)
            print("Saved train_y to: " + out_y)
            print("Saved preprocessor to: " + preproc_path)
            print("Saved feature_selector to: " + fs_path)
            print("Saved metadata to: " + meta_path)
            print("SUCCESS Preprocessing and train split complete")
        except Exception as exc:
            print("ERROR during preprocessing: " + str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --tfidf_max_features
      - {inputValue: tfidf_max_features}
      - --svd_dims
      - {inputValue: svd_dims}
      - --preview_rows
      - {inputValue: preview_rows}
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
