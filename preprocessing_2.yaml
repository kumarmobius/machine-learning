name: Load & Preprocess + Auto Feature Select (v2)
description: |
  End-to-end robust preprocessing + automatic feature selection.
  - Smart numeric parsing: currencies, multipliers (k/m/b/t), percents, mixed tokens.
  - Datetime detection + feature extraction.
  - Rare-label collapsing and optional string-similarity grouping.
  - Per-column imputation: none/median/KNN/Iterative chosen by data profile.
  - Smart scaler per numeric: MinMax for [0,1], Robust for heavy tails, Yeo-Johnson+Std for skewed, Std otherwise.
  - Optional outlier handling (fit-time only): drop-IQR or winsorize; transform-time does NOT drop rows.
  - Target handling: auto-cast boolean-like or male/female/other to integers when model_type=classification.
  - Feature selection (two-stage):
      Stage 1: Mutual Information with dynamic k (auto k based on p).
      Stage 2: Sparse model selector (L1 LogisticRegression for classification; LassoCV for regression).
  - Saves: X (selected), y, preprocessor.pkl, feature_selector.pkl, preprocess_metadata.json
inputs:
  - {name: in_file, type: Data, description: "Path to tabular file or directory"}
  - {name: target_column, type: String, description: "Target column name"}
  - {name: model_type, type: String, description: "classification or regression"}
  - {name: outlier_strategy, type: String, optional: true, default: "drop_iqr", description: "'drop_iqr'|'winsorize'|'none' (fit-time only)"}
  - {name: outlier_fold, type: Float, optional: true, default: "1.5", description: "IQR fold for outlier fences"}
  - {name: max_knn_rows, type: Integer, optional: true, default: "5000", description: "Max rows to allow KNN impute"}
  - {name: numeric_detection_threshold, type: Float, optional: true, default: "0.6", description: "Object->numeric if parsable frac >= threshold"}
  - {name: rare_threshold, type: Float, optional: true, default: "0.01", description: "Rare label threshold: fraction or absolute count"}
  - {name: enable_string_similarity, type: String, optional: true, default: "false", description: "true/false for high-cardinality grouping"}
  - {name: preview_rows, type: Integer, optional: true, default: "20"}
outputs:
  - {name: X, type: Data, description: "Parquet of selected features"}
  - {name: y, type: Data, description: "Parquet of target"}
  - {name: preprocessor, type: Data, description: "Pickle of fitted preprocessing object"}
  - {name: feature_selector, type: Data, description: "Pickle of fitted feature selector pipeline"}
  - {name: preprocess_metadata, type: Data, description: "JSON diagnostics and selections"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime
        def pip_install(pkgs):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input", "--quiet"] + pkgs)
        try:
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.linear_model import LogisticRegression, LassoCV
            import joblib
        except Exception:
            pip_install(["pandas","numpy","scikit-learn","joblib","pyarrow","fastparquet","openpyxl","rapidfuzz","category_encoders"])
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.linear_model import LogisticRegression, LassoCV
            import joblib
        try:
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
            import category_encoders as ce
        except Exception:
            rf_process = rf_fuzz = None
            ce = None

        def ensure_dir_for(path):
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_bytes(path, n=8192):
            try:
                with open(path,"rb") as fh: return fh.read(n)
            except: return b""

        def is_json_like(b):
            if not b: return False
            try: t=b.decode("utf-8",errors="ignore").lstrip()
            except: return False
            if not t: return False
            ob=chr(123); br=chr(91); nl=chr(10)
            return t[0] in (ob, br) or (nl+ob) in t or (nl+br) in t

        def load_table(path):
            if os.path.isdir(path):
                cand=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in cand if os.path.isfile(p)]
                if not files: raise ValueError("Input directory empty")
                path = files[0] if len(files)==1 else max(files,key=lambda p: os.path.getsize(p))
                print("Info selected file:", path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: "+path)
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        s=fh.read(8192); fh.seek(0)
                        if is_json_like(s.encode() if isinstance(s,str) else s):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    if not members: raise ValueError("Zip empty")
                    m=max(members,key=lambda n:z.getinfo(n).file_size or 0)
                    with z.open(m) as fh:
                        s=fh.read(8192)
                        if is_json_like(s):
                            with z.open(m) as fh2: return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(m) as fh2: return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path,sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path,lines=True)
                    except: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path,engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except: pass
            try: return pd.read_parquet(path,engine="auto")
            except: pass
            b=sample_bytes(path)
            if is_json_like(b):
                try: return pd.read_json(path,lines=True)
                except: pass
            return pd.read_csv(path)

        CURRENCY = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULT = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(
            r'^\s*(?P<sign>[-+]?)'
            r'(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*'
            r'(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$'
        )
        def parse_alpha_num(x, comma_decimal=False):
            import numpy as _np, pandas as _pd, re as _re
            if _pd.isna(x): return _np.nan
            s=str(x).strip()
            if s=="" or s.lower() in {"nan","none","null","na"}: return _np.nan
            s=_re.sub(CURRENCY,"",s)
            if s.endswith("%"):
                try: return float(s[:-1].replace(",","").replace(" ",""))/100.0
                except: pass
            s=s.replace("\u2212","-").replace("\u2013","-").replace("\u2014","-")
            s=_re.sub(r'[\u00A0\u202F]','',s).strip()
            if comma_decimal: s=s.replace(".","").replace(",",".")
            else: s=s.replace(",","").replace(" ","")
            try: return float(s)
            except: pass
            m=TOKEN_RE.match(s)
            if m:
                num=m.group("number"); mult=m.group("mult"); unit=(m.group("unit") or "").lower()
                try: val=float(num.replace(",","").replace(" ",""))
                except:
                    try: val=float(num.replace(",","."))
                    except: return _np.nan
                if mult: val*=MULT.get(mult.lower(),1.0)
                if "%" in unit: val/=100.0
                return val
            g=re.search(r'[-+]?\d+([.]\d+)?',s)
            if g:
                try: return float(g.group(0))
                except: return _np.nan
            return _np.nan

        def convert_objects_to_numeric(df, thr=0.6):
            df=df.copy()
            obj=[c for c in df.columns if (df[c].dtype=="object" or str(df[c].dtype).startswith("string"))]
            report={"converted":[], "skipped":[]}
            for c in obj:
                ser=df[c].astype(object)
                nn=ser.notna().sum()
                if nn==0: report["skipped"].append(c); continue
                parsed=ser.map(parse_alpha_num)
                frac=parsed.notna().sum()/float(nn)
                if frac>=thr:
                    df[c+"_orig"]=df[c]
                    df[c]=parsed
                    report["converted"].append({"col":c,"parsable_fraction":frac})
                else:
                    report["skipped"].append({"col":c,"parsable_fraction":frac})
            return df, report

        def detect_dates(df):
            df=df.copy(); rep={"date_columns":[], "skipped":[]}
            cand=[c for c in df.columns if (df[c].dtype=="object" or str(df[c].dtype).startswith("string"))]
            for c in cand:
                s=df[c].dropna().astype(str).head(500)
                if s.empty: rep["skipped"].append({"col":c,"parsable_fraction":0.0}); continue
                parsed=pd.to_datetime(s, errors="coerce", utc=False)
                frac=float(parsed.notna().mean())
                if frac>=0.6:
                    full=pd.to_datetime(df[c], errors="coerce", utc=False)
                    df[c+"_orig"]=df[c]; df[c]=full
                    df[c+"_year"]=df[c].dt.year
                    df[c+"_month"]=df[c].dt.month
                    df[c+"_day"]=df[c].dt.day
                    df[c+"_dow"]=df[c].dt.dayofweek
                    df[c+"_is_weekend"]=df[c].dt.dayofweek.isin([5,6]).astype("Int64")
                    df[c+"_is_month_start"]=df[c].dt.is_month_start.astype("Int64")
                    df[c+"_is_month_end"]=df[c].dt.is_month_end.astype("Int64")
                    df[c+"_days_since_epoch"]=(df[c]-pd.Timestamp("1970-01-01"))//pd.Timedelta("1D")
                    rep["date_columns"].append({"col":c,"parsable_fraction":frac})
                else:
                    rep["skipped"].append({"col":c,"parsable_fraction":frac})
            return df, rep

        def collapse_rare(series, thr_frac=0.01, thr_count=None):
            vc=series.value_counts(dropna=False)
            n=len(series)
            if thr_count is not None: rare=set(vc[vc<=thr_count].index)
            else: rare=set(vc[vc<=max(1,int(thr_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), list(rare)

        def choose_scaler(values):
            v=pd.Series(values).dropna()
            if v.empty: return StandardScaler()
            if v.min()>=0 and v.max()<=1: return MinMaxScaler()
            std=float(v.std(ddof=0))
            if std==0: return StandardScaler()
            skew=float(v.skew())
            kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            extreme=float(((v-v.mean()).abs()>3*std).mean())
            if abs(skew)>=1.0: return Pipeline([("power",PowerTransformer(method="yeo-johnson")),("std",StandardScaler())])
            if extreme>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        class Preprocessor:
            def __init__(self, config):
                self.cfg=config
                self.num_cols=[]; self.cat_cols=[]
                self.col_cfg={}
                self.outlier_limits=None

            def _profile(self, X):
                self.num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols = [c for c in X.columns if c not in self.num_cols]

            def fit(self, X, y=None):
                self._profile(X)
                n=len(X)
                for c in self.num_cols:
                    s=X[c]
                    miss=float(s.isna().mean())
                    nc={}
                    if miss==0: nc["imputer"]=("none", None)
                    elif miss<0.02:
                        imp=SimpleImputer(strategy="median"); imp.fit(s.values.reshape(-1,1))
                        nc["imputer"]=("simple_median", imp)
                    else:
                        if n<=self.cfg.get("max_knn_rows",5000) and miss<0.25:
                            nc["imputer"]=("knn", {"n_neighbors":5})
                        else:
                            nc["imputer"]=("iterative", {"max_iter":10, "random_state":0})
                    tmp=s.fillna(s.median())
                    sc=choose_scaler(tmp)
                    try: sc.fit(tmp.values.reshape(-1,1))
                    except: sc=StandardScaler().fit(tmp.values.reshape(-1,1))
                    nc["scaler"]=sc
                    self.col_cfg[c]=nc
                for c in self.cat_cols:
                    s=X[c].astype(object)
                    nunique=int(s.nunique(dropna=True)); high = nunique>max(50, 0.05*len(X))
                    cc={"nunique":nunique,"high_card":high,"rare_threshold":self.cfg.get("rare_threshold",0.01)}
                    if high:
                        if y is not None and self.cfg.get("model_type")=="classification": cc["encoder"]="target"
                        else: cc["encoder"]="count"
                    else:
                        if nunique<=10: cc["encoder"]="onehot"
                        else:
                            if y is not None and self.cfg.get("model_type")=="classification": cc["encoder"]="target"
                            else: cc["encoder"]="ordinal"
                    self.col_cfg[c]=cc
                if self.cfg.get("outlier_strategy")=="winsorize" and self.num_cols:
                    q1=X[self.num_cols].quantile(0.25); q3=X[self.num_cols].quantile(0.75); iqr=q3-q1
                    l=q1 - self.cfg.get("outlier_fold",1.5)*iqr; u=q3 + self.cfg.get("outlier_fold",1.5)*iqr
                    self.outlier_limits={"lower":l.to_dict(), "upper":u.to_dict()}
                return self

            def _impute(self, X):
                X=X.copy()
                need_iter=[c for c in self.num_cols if self.col_cfg[c]["imputer"][0]=="iterative"]
                if need_iter:
                    iter_imp=IterativeImputer(max_iter=self.col_cfg[need_iter[0]]["imputer"][1]["max_iter"], random_state=0)
                    blk=X[self.num_cols].replace([np.inf,-np.inf], np.nan)
                    X[self.num_cols]=iter_imp.fit_transform(blk)
                need_knn=[c for c in self.num_cols if self.col_cfg[c]["imputer"][0]=="knn"]
                if need_knn:
                    knn=KNNImputer(n_neighbors=self.col_cfg[need_knn[0]]["imputer"][1]["n_neighbors"])
                    blk=X[self.num_cols].replace([np.inf,-np.inf], np.nan)
                    X[self.num_cols]=knn.fit_transform(blk)
                for c in self.num_cols:
                    imp=self.col_cfg[c]["imputer"]
                    if imp[0]=="simple_median":
                        X[c]=imp[1].transform(X[[c]])
                return X

            def _outliers_fit_stage(self, X):
                if self.cfg.get("outlier_strategy")=="drop_iqr" and self.num_cols:
                    q1=X[self.num_cols].quantile(0.25); q3=X[self.num_cols].quantile(0.75); iqr=q3-q1
                    l=q1 - self.cfg.get("outlier_fold",1.5)*iqr; u=q3 + self.cfg.get("outlier_fold",1.5)*iqr
                    mask=pd.Series(True,index=X.index)
                    for c in self.num_cols: mask &= X[c].ge(l[c]) & X[c].le(u[c])
                    return X.loc[mask].reset_index(drop=True)
                if self.cfg.get("outlier_strategy")=="winsorize" and self.outlier_limits:
                    for c in self.num_cols:
                        lo=self.outlier_limits["lower"][c]; hi=self.outlier_limits["upper"][c]
                        X[c]=X[c].clip(lower=lo, upper=hi)
                return X

            def _scale(self, X):
                X=X.copy()
                for c in self.num_cols:
                    sc=self.col_cfg[c]["scaler"]
                    try: X[c]=sc.transform(X[[c]]).reshape(-1)
                    except:
                        sc=StandardScaler().fit(X[[c]].fillna(X[c].median()))
                        X[c]=sc.transform(X[[c]]).reshape(-1)
                        self.col_cfg[c]["scaler"]=sc
                return X

            def _encode(self, X, y=None, training=True):
                X=X.copy()
                for c in list(self.cat_cols):
                    if c not in X.columns: continue
                    s=X[c].astype(object)
                    thr=self.col_cfg[c].get("rare_threshold",0.01)
                    if thr<1.0: s, rare = collapse_rare(s, thr_frac=thr)
                    else: s, rare = collapse_rare(s, thr_frac=0.01, thr_count=int(thr))
                    X[c]=s
                    enc=self.col_cfg[c]["encoder"]
                    if enc=="onehot":
                        ohe=OneHotEncoder(sparse_output=False, handle_unknown="ignore")
                        arr=ohe.fit_transform(X[[c]].astype(str)) if training else ohe.transform(X[[c]].astype(str))
                        cols=[f"{c}__{v}" for v in ohe.categories_[0]]
                        X=pd.concat([X.drop(columns=[c]), pd.DataFrame(arr,columns=cols,index=X.index)], axis=1)
                        self.col_cfg[c]["ohe"]=ohe; self.col_cfg[c]["ohe_cols"]=cols
                    elif enc=="ordinal":
                        oe=OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
                        X[c]= (oe.fit_transform(X[[c]]) if training else oe.transform(X[[c]])).astype(float)
                        self.col_cfg[c]["ord"]=oe
                    elif enc=="count":
                        if training:
                            vc=X[c].value_counts().to_dict(); self.col_cfg[c]["count_map"]=vc
                        vc=self.col_cfg[c]["count_map"]
                        X[c]=X[c].map(lambda v: vc.get(v,0)).astype(float)
                    elif enc=="target" and y is not None and training:
                        ser=X[c].astype(object); tgt=pd.Series(y).astype(float)
                        prior=float(tgt.mean())
                        counts=ser.value_counts()
                        means=tgt.groupby(ser).mean()
                        smooth={k:(means.get(k,prior)* (cnt/(cnt+10)) + prior*(10/(cnt+10))) for k,cnt in counts.items()}
                        self.col_cfg[c]["tgt_map"]=smooth; self.col_cfg[c]["tgt_prior"]=prior
                        X[c]=ser.map(lambda v: smooth.get(v, prior)).astype(float)
                    elif enc=="target" and y is None and not training:
                        prior=self.col_cfg[c].get("tgt_prior",0.0); mp=self.col_cfg[c].get("tgt_map",{})
                        X[c]=X[c].astype(object).map(lambda v: mp.get(v, prior)).astype(float)
                    else:
                        X[c]=X[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)
                return X

            def fit_transform(self, X, y=None):
                X=X.replace([np.inf,-np.inf], np.nan)
                X=self._impute(X)
                X=self._outliers_fit_stage(X)
                X=self._scale(X)
                X=self._encode(X, y=y, training=True)
                X=X.replace([np.inf,-np.inf], np.nan)
                for c in X.columns:
                    if X[c].dtype=="object":
                        try: X[c]=pd.to_numeric(X[c], errors="coerce")
                        except: pass
                return X

            def transform(self, X):
                X=X.replace([np.inf,-np.inf], np.nan)
                X=self._impute(X)
                if self.cfg.get("outlier_strategy")=="winsorize" and self.outlier_limits:
                    for c in self.num_cols:
                        lo=self.outlier_limits["lower"][c]; hi=self.outlier_limits["upper"][c]
                        X[c]=X[c].clip(lower=lo, upper=hi)
                X=self._scale(X)
                X=self._encode(X, y=None, training=False)
                X=X.replace([np.inf,-np.inf], np.nan)
                return X

        class FeatureSelector:
            def __init__(self, task):
                self.task=task
                self.mi_selector=None
                self.model_selector=None
                self.selected_features=[]
            def _auto_k(self, p):
                return max(5, min(int(round(math.sqrt(p)*2)), p))
            def fit(self, X, y):
                p=X.shape[1]
                k=self._auto_k(p)
                if self.task=="classification":
                    self.mi_selector = SelectKBest(score_func=mutual_info_classif, k=k).fit(X, y)
                else:
                    self.mi_selector = SelectKBest(score_func=mutual_info_regression, k=k).fit(X, y)
                X1=self.mi_selector.transform(X)
                if self.task=="classification":
                    clf=LogisticRegression(penalty="l1", solver="saga", C=1.0, max_iter=2000)
                    self.model_selector = SelectFromModel(estimator=clf, max_features=X1.shape[1], threshold="median").fit(X1, y)
                else:
                    lasso=LassoCV(cv=5, random_state=0)
                    self.model_selector = SelectFromModel(estimator=lasso, max_features=X1.shape[1], threshold="median").fit(X1, y)
                mask1=self.mi_selector.get_support()
                mask2=self.model_selector.get_support()
                cols=np.array(X.columns)[mask1][mask2]
                self.selected_features=list(cols)
                return self
            def transform(self, X):
                X1=self.mi_selector.transform(X)
                X2=self.model_selector.transform(X1)
                return pd.DataFrame(X2, columns=self.selected_features, index=X.index)

        def print_before(df, target):
            print("BEFORE: head"); print(df.head())
            print("BEFORE: dtypes"); print(df.dtypes.astype(str))
            print("BEFORE: missing"); print(df.isna().sum())

        def print_after(X, selected_cols):
            print("AFTER: head"); 
            try: print(X.head().to_string())
            except: print(X.head())
            print("AFTER: dtypes"); print(X.dtypes.astype(str))
            print("AFTER: missing"); print(X.isna().sum())
            print("SELECTED FEATURES:", selected_cols)

        parser=argparse.ArgumentParser()
        parser.add_argument("--in_file", type=str, required=True)
        parser.add_argument("--target_column", type=str, required=True)
        parser.add_argument("--model_type", type=str, required=True)
        parser.add_argument("--outlier_strategy", type=str, default="drop_iqr")
        parser.add_argument("--outlier_fold", type=float, default=1.5)
        parser.add_argument("--max_knn_rows", type=int, default=5000)
        parser.add_argument("--numeric_detection_threshold", type=float, default=0.6)
        parser.add_argument("--rare_threshold", type=float, default=0.01)
        parser.add_argument("--enable_string_similarity", type=str, default="false")
        parser.add_argument("--preview_rows", type=int, default=20)
        parser.add_argument("--X", type=str, required=True)
        parser.add_argument("--y", type=str, required=True)
        parser.add_argument("--preprocessor", type=str, required=True)
        parser.add_argument("--feature_selector", type=str, required=True)
        parser.add_argument("--preprocess_metadata", type=str, required=True)
        args=parser.parse_args()

        try:
            df=load_table(args.in_file)
            df=df.drop_duplicates(keep="first").reset_index(drop=True)
            if args.target_column not in df.columns:
                print(f"ERROR missing target: {args.target_column}", file=sys.stderr); sys.exit(1)
            print_before(df, args.target_column)

            y=df[args.target_column].copy()
            model_type=args.model_type.strip().lower()
            if model_type=="classification":
                mapping_bool={"true":1,"false":0,"yes":1,"no":0,"y":1,"n":0,"t":1,"f":0,"1":1,"0":0}
                def labelize(v):
                    if pd.isna(v): return np.nan
                    s=str(v).strip().lower()
                    if s in mapping_bool: return mapping_bool[s]
                    if s in {"male","m"}: return 1
                    if s in {"female","f"}: return 0
                    if s in {"other","others","o"}: return 2
                    try: return int(float(s))
                    except: return s
                y_lab = y.map(labelize)
                if not pd.api.types.is_numeric_dtype(y_lab):
                    y_codes, _ = pd.factorize(y_lab, sort=True)
                    y = pd.Series(y_codes, index=y.index).astype(int)
                else:
                    y = y_lab.astype(int)
            else:
                y = pd.to_numeric(y, errors="coerce")

            mask = y.notna()
            df = df.loc[mask].reset_index(drop=True)
            y = y.loc[mask].reset_index(drop=True)

            df, conv_rep = convert_objects_to_numeric(df, thr=args.numeric_detection_threshold)
            df, date_rep = detect_dates(df)

            X = df.drop(columns=[args.target_column]).copy()

            cfg={
                "model_type": model_type,
                "outlier_strategy": args.outlier_strategy,
                "outlier_fold": float(args.outlier_fold),
                "max_knn_rows": int(args.max_knn_rows),
                "rare_threshold": float(args.rare_threshold),
            }
            pre = Preprocessor(cfg)
            Xp = pre.fit_transform(X, y=y if model_type=="classification" else None)

            Xp = Xp.replace([np.inf,-np.inf], np.nan)
            for c in Xp.columns:
                if Xp[c].dtype=="object":
                    try: Xp[c]=pd.to_numeric(Xp[c], errors="coerce")
                    except: pass
            Xp = Xp.fillna(0.0)

            fs = FeatureSelector("classification" if model_type=="classification" else "regression")
            fs.fit(Xp, y if model_type=="classification" else pd.to_numeric(y, errors="coerce").fillna(y.mean()))
            Xs = fs.transform(Xp)

            print_after(Xs, fs.selected_features)

            ensure_dir_for(args.X); ensure_dir_for(args.y); ensure_dir_for(args.preprocessor); ensure_dir_for(args.feature_selector); ensure_dir_for(args.preprocess_metadata)
            Xs.to_parquet(args.X, index=False)
            y.to_parquet(args.y, index=False)
            joblib.dump(pre, args.preprocessor)
            joblib.dump(fs, args.feature_selector)

            meta={
                "timestamp": datetime.utcnow().isoformat()+"Z",
                "model_type": model_type,
                "conversion_report": conv_rep,
                "date_report": date_rep,
                "selected_features": fs.selected_features,
            }
            with open(args.preprocess_metadata, "w", encoding="utf-8") as fh:
                json.dump(meta, fh, indent=2, ensure_ascii=False)
            print("SUCCESS saved outputs.")
        except Exception as e:
            print("ERROR during preprocessing:", e, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --preview_rows
      - {inputValue: preview_rows}
      - --X
      - {outputPath: X}
      - --y
      - {outputPath: y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
