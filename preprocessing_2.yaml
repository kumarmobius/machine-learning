
name: Advanced Preprocess v8
description: |
  Robust auto-preprocessing + feature selection.
  - Safe file loading (CSV/TSV/JSON/JSONL/Excel/Parquet/Feather/ORC, gz/zip, dirs)
  - Duplicate-safe with unhashable cells handled
  - Advanced numeric parsing from strings with units, %, k/m/b suffixes
  - Datetime detection + date features
  - Per-column imputers (none/median/KNN/Iterative)
  - Smart scaler choice (MinMax/Robust/Standard or Power+Std)
  - Optional IQR outlier drop or winsorization (index-safe, mask-align safe)
  - Rare-category collapsing + string-similarity grouping (optional)
  - Encoders: OneHot, Ordinal, Count, Target (KFold leakage-safe)
  - Text features: TF-IDF + TruncatedSVD for “long enough” short text columns
  - Fully finite safeguards before imputers (no ±∞, huge values capped)
  - Feature selection: Mutual Information + Model-based (LightGBM if available, else ExtraTrees)
  - Saves: X, y, preprocessor.pkl, feature_selector.pkl, preprocess_metadata.json
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: Float, description: "IQR fold for outlier rule", optional: true, default: 1.5}
  - {name: max_knn_rows, type: Integer, description: "Max rows for KNN imputer else fallback", optional: true, default: 5000}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: 0.6}
  - {name: rare_threshold, type: Float, description: "Rare label threshold as fraction or absolute count", optional: true, default: 0.01}
  - {name: enable_string_similarity, type: String, description: "true/false high-cardinality grouping", optional: true, default: "false"}
  - {name: tfidf_max_features, type: Integer, description: "Max vocabulary per text column", optional: true, default: 20000}
  - {name: svd_dims, type: Integer, description: "SVD dims per text column when applied", optional: true, default: 30}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: 20}
outputs:
  - {name: X, type: Data, description: "Processed features parquet"}
  - {name: y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "joblib dump of fitted Preprocessor"}
  - {name: feature_selector, type: Data, description: "joblib dump of fitted FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSON metadata"}

implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime

        def pip_install(pkgs):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input"] + pkgs)

        try:
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
            HAVE_LGBM = False
            try:
                from lightgbm import LGBMClassifier, LGBMRegressor
                HAVE_LGBM = True
            except Exception:
                pass
        except Exception:
            pip_install(["pandas","numpy","scikit-learn","joblib","category_encoders","rapidfuzz","pyarrow","fastparquet","openpyxl","scipy"])
            try:
                pip_install(["lightgbm"])
            except Exception:
                pass
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
            HAVE_LGBM = False
            try:
                from lightgbm import LGBMClassifier, LGBMRegressor
                HAVE_LGBM = True
            except Exception:
                pass

        # ----------- IO helpers -----------
        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh:
                    return fh.read(n)
            except Exception:
                return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes:
                return False
            try:
                txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception:
                return False
            if not txt:
                return False
            open_brace = chr(123)
            open_bracket = chr(91)
            newline = chr(10)
            first_char = txt[0]
            if first_char == open_brace or first_char == open_bracket:
                return True
            if (newline + open_brace) in txt or (newline + open_bracket) in txt:
                return True
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries = [os.path.join(path, f) for f in os.listdir(path) if not f.startswith(".")]
                files = [p for p in entries if os.path.isfile(p)]
                if not files:
                    raise ValueError("No regular files inside directory: " + path)
                path = files[0] if len(files) == 1 else max(files, key=lambda p: os.path.getsize(p))
                print("Info: in_file was directory, selected:", path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found or not a file: " + path)
            ext = os.path.splitext(path)[1].lower()
            if ext == ".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path, "rt", encoding="utf-8", errors="ignore") as fh:
                        sample = fh.read(8192)
                        fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try:
                                return pd.read_json(fh, lines=True)
                            except Exception:
                                fh.seek(0)
                                return pd.read_csv(fh)
                        fh.seek(0)
                        return pd.read_csv(fh)
                except Exception:
                    pass
            if ext == ".zip":
                with zipfile.ZipFile(path, "r") as z:
                    members = [n for n in z.namelist() if not n.endswith("/")]
                    if not members:
                        raise ValueError("Zip archive empty: " + path)
                    member = max(members, key=lambda n: z.getinfo(n).file_size or 0)
                    with z.open(member) as fh:
                        sample = fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2, encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2, encoding="utf-8"))
            try:
                if ext == ".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext == ".feather": return pd.read_feather(path)
                if ext == ".orc": return pd.read_orc(path)
            except Exception:
                pass
            try:
                return pd.read_parquet(path, engine="auto")
            except Exception:
                pass
            sample = sample_file_bytes(path)
            if is_likely_json(sample):
                try: return pd.read_json(path, lines=True)
                except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported or unreadable file format: " + path")

        # ---------- Utils ----------
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def safe_drop_duplicates(df):
            tmp = df.copy()
            for c in tmp.columns:
                tmp[c] = tmp[c].apply(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(dict,list,set)) else v)
            before = len(df)
            mask = ~tmp.duplicated(keep="first")
            df2 = df.loc[mask].copy()
            print(f"Removed duplicates: {before - len(df2)} rows dropped")
            return df2

        # ---------- Advanced parsing ----------
        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3, 'm':1e6, 'b':1e9, 't':1e12}
        TOKEN_RE = re.compile(
            r'^\\s*'
            r'(?P<sign>[-+]?)'
            r'(?P<number>(?:\\d{1,3}(?:[,\\s]\\d{3})+|\\d+)(?:[.,]\\d+)?)'
            r'\\s*'
            r'(?P<mult>[kKmMbBtT])?'
            r'\\s*(?P<unit>[A-Za-z%/°µμ²³]*)\\s*$'
        )
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            import numpy as _np, pandas as _pd, re as _re
            if _pd.isna(s): return _np.nan
            orig = str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return _np.nan
            tmp = _re.sub(CURRENCY_SYMBOLS_REGEX, '', orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp = tmp.replace('\\u2212','-').replace('\\u2013','-').replace('\\u2014','-')
            tmp = _re.sub(r'[\\u00A0\\u202F]','', tmp).strip()
            if decimal_comma: tmp = tmp.replace('.','').replace(',','.')
            else: tmp = tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m = TOKEN_RE.match(tmp)
            if m:
                number = m.group('number'); mult = m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean = number.replace(',','').replace(' ','')
                try: val = float(number_clean)
                except Exception:
                    try: val = float(number_clean.replace(',', '.'))
                    except Exception: return _np.nan
                if mult: val *= MULTIPLIER_MAP.get(mult.lower(), 1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            num_search = _re.search(r'[-+]?\\d+([.,]\\d+)?', tmp)
            if num_search:
                try: return float(num_search.group(0).replace(',',''))
                except Exception: return _np.nan
            return _np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df = df.copy()
            report = {'converted': [], 'skipped': []}
            obj_cols = [c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                ser = df[col].astype(object)
                total_non_null = ser.notna().sum()
                if total_non_null == 0:
                    report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                parsable = parsed.notna().sum(); frac = float(parsable) / float(total_non_null) if total_non_null>0 else 0.0
                if frac >= detect_threshold:
                    df[col + "_orig"] = df[col]
                    df[col] = parsed
                    report['converted'].append({'col': col, 'parsable_fraction': frac})
                else:
                    report['skipped'].append({'col': col, 'parsable_fraction': frac})
            return df, report

        # ---------- Dates ----------
        def detect_and_extract_dates(df, date_parse_enabled=True):
            df = df.copy()
            report = {'date_columns': [], 'skipped': []}
            if not date_parse_enabled: return df, report
            candidate_cols = [c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in candidate_cols:
                ser = df[col].astype(object)
                sample = ser.dropna().astype(str).head(500)
                if sample.empty:
                    report['skipped'].append({'col': col, 'parsable_fraction': 0.0}); continue
                parsed = pd.to_datetime(sample, errors='coerce', infer_datetime_format=True, utc=False)
                parsable_frac = float(parsed.notna().mean())
                if parsable_frac >= 0.6:
                    full_parsed = pd.to_datetime(ser, errors='coerce', infer_datetime_format=True, utc=False)
                    df[col + "_orig"] = df[col]
                    df[col] = full_parsed
                    df[col + "_year"] = df[col].dt.year
                    df[col + "_month"] = df[col].dt.month
                    df[col + "_day"] = df[col].dt.day
                    df[col + "_dayofweek"] = df[col].dt.dayofweek
                    df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col + "_is_month_start"] = df[col].dt.is_month_start.astype('Int64')
                    df[col + "_is_month_end"] = df[col].dt.is_month_end.astype('Int64')
                    df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                    report['date_columns'].append({'col': col, 'parsable_fraction': parsable_frac})
                else:
                    report['skipped'].append({'col': col, 'parsable_fraction': parsable_frac})
            return df, report

        # ---------- Rare & similarity ----------
        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts = series.value_counts(dropna=False)
            n = len(series)
            if threshold_count is not None:
                rare = set(counts[counts <= threshold_count].index)
            else:
                rare = set(counts[counts <= max(1, int(threshold_frac * n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals = [v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping = {}; used = set()
            for v in vals:
                if v in used: continue
                matches = rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=score_threshold)
                group = [m[0] for m in matches]
                for g in group:
                    mapping[g] = v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        # ---------- Outliers ----------
        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            import numpy as _np, pandas as _pd
            if not numeric_cols: return df, 0
            q1 = df[numeric_cols].quantile(0.25)
            q3 = df[numeric_cols].quantile(0.75)
            iqr = q3 - q1
            lower = q1 - fold * iqr
            upper = q3 + fold * iqr
            mask = _np.ones(len(df), dtype=bool)
            for c in numeric_cols:
                col = df[c]
                cond = col.ge(lower.get(c, col.min()-1e9)) & col.le(upper.get(c, col.max()+1e9))
                mask &= cond.to_numpy(dtype=bool)
            before = len(df)
            df2 = df.loc[mask].copy()
            df2.reset_index(drop=True, inplace=True)
            return df2, before - len(df2)

        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df, 0
            q1 = df[numeric_cols].quantile(0.25)
            q3 = df[numeric_cols].quantile(0.75)
            iqr = q3 - q1
            lower = q1 - fold * iqr
            upper = q3 + fold * iqr
            df2 = df.copy()
            for c in numeric_cols:
                lo = lower.get(c, None); up = upper.get(c, None)
                if lo is not None and up is not None:
                    df2[c] = df2[c].clip(lower=lo, upper=up)
            return df2, 0

        # ---------- Scaler choice ----------
        def choose_scaler_for_series(s):
            v = s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0) == 0: return StandardScaler()
            skew = float(v.skew())
            kurt = float(v.kurtosis()) if len(v) > 3 else 0.0
            if (v.min() >= 0.0 and v.max() <= 1.0):
                return MinMaxScaler()
            extreme_frac = float(((v - v.mean()).abs() > 3 * v.std(ddof=0)).mean())
            if abs(skew) >= 1.0:
                return Pipeline([('power', PowerTransformer(method='yeo-johnson')), ('std', StandardScaler())])
            if extreme_frac > 0.01 or abs(kurt) > 10:
                return RobustScaler()
            return StandardScaler()

        # ---------- Encoder chooser ----------
        def choose_categorical_encoder(col, series, target_series=None, train_mode=True):
            n = len(series)
            nunique = series.nunique(dropna=True)
            high_card = (nunique > max(50, 0.05 * n))
            if high_card:
                if target_series is not None and train_mode:
                    enc = ce.TargetEncoder(cols=[col], smoothing=0.3)
                    return 'target', enc
                else:
                    return 'count', None
            else:
                if nunique <= 10:
                    enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                    return 'onehot', enc
                if target_series is not None and train_mode:
                    enc = ce.TargetEncoder(cols=[col], smoothing=0.3)
                    return 'target', enc
                else:
                    enc = OrdinalEncoder()
                    return 'ordinal', enc

        # ---------- Feature Selector ----------
        class FeatureSelector:
            def __init__(self, task):
                self.task = task
                self.mi_selector = None
                self.model_selector = None
                self.selected_features = []

            def _auto_k(self, p):
                return max(5, min(int(round(math.sqrt(p)*2)), p))

            def fit(self, X, y):
                p = X.shape[1]
                k = self._auto_k(p)
                if self.task == "classification":
                    self.mi_selector = SelectKBest(score_func=mutual_info_classif, k=k).fit(X, y)
                else:
                    self.mi_selector = SelectKBest(score_func=mutual_info_regression, k=k).fit(X, y)
                X1 = self.mi_selector.transform(X)
                cols_stage1 = np.array(X.columns)[self.mi_selector.get_support()]
                if self.task == "classification":
                    if HAVE_LGBM:
                        est = LGBMClassifier(n_estimators=600, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42)
                    else:
                        est = ExtraTreesClassifier(n_estimators=400, random_state=42, n_jobs=-1)
                else:
                    if HAVE_LGBM:
                        est = LGBMRegressor(n_estimators=600, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42)
                    else:
                        est = ExtraTreesRegressor(n_estimators=400, random_state=42, n_jobs=-1)
                self.model_selector = SelectFromModel(estimator=est, max_features=X1.shape[1], threshold="median").fit(X1, y)
                mask2 = self.model_selector.get_support()
                cols_final = cols_stage1[mask2]
                self.selected_features = list(cols_final) if len(cols_final)>0 else list(cols_stage1)
                return self

            def transform(self, X):
                X1 = self.mi_selector.transform(X)
                X2 = self.model_selector.transform(X1)
                return pd.DataFrame(X2, columns=self.selected_features, index=X.index)

        # ---------- Preprocessor ----------
        class Preprocessor:
            def __init__(self, config, text_cfg):
                self.config = config
                self.text_cfg = text_cfg
                self.num_cols = []
                self.cat_cols = []
                self.text_cols = []
                self.col_config = {}
                self.text_models = {}

            def _detect_text_cols(self, df):
                cand = [c for c in df.columns if df[c].dtype=='object' or str(df[c].dtype).startswith('string')]
                chosen = []
                for c in cand:
                    s = df[c].astype(str).fillna("")
                    avg_words = s.map(lambda x: len(x.split())).mean()
                    avg_len = s.map(len).mean()
                    if avg_words >= 3 or avg_len >= 10:
                        chosen.append(c)
                return chosen

            def fit(self, df, y=None, task="classification"):
                self.num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                obj_cols = [c for c in df.columns if c not in self.num_cols]
                self.text_cols = self._detect_text_cols(df[obj_cols]) if obj_cols else []
                self.cat_cols = [c for c in obj_cols if c not in self.text_cols]

                nrows = len(df)
                for c in self.num_cols:
                    s = df[c]
                    cfg = {}
                    missing_frac = float(s.isna().mean())
                    cfg['missing_frac'] = missing_frac
                    cfg['n_unique'] = int(s.nunique(dropna=True))
                    if missing_frac == 0:
                        cfg['imputer'] = ('none', None)
                    elif missing_frac < 0.02:
                        imp = SimpleImputer(strategy='median').fit(np.array(s).reshape(-1,1))
                        cfg['imputer'] = ('simple_median', imp)
                    else:
                        if nrows <= self.config.get('max_knn_rows', 5000) and missing_frac < 0.25:
                            cfg['imputer'] = ('knn', {'n_neighbors':5})
                        else:
                            cfg['imputer'] = ('iterative', {'max_iter':10, 'random_state':0})
                    cfg['scaler_choice'] = 'auto'
                    self.col_config[c] = cfg

                for c in self.cat_cols:
                    s = df[c].astype(object)
                    cfg = {}
                    cfg['n_unique'] = int(s.nunique(dropna=True))
                    cfg['missing_frac'] = float(s.isna().mean())
                    cfg['rare_threshold_frac'] = self.config.get('rare_threshold', 0.01)
                    enc_name, enc_obj = choose_categorical_encoder(c, s, (y if y is not None else None), train_mode=(y is not None))
                    cfg['encoder_type'] = enc_name
                    cfg['encoder_obj'] = enc_obj
                    self.col_config[c] = cfg

                if y is not None:
                    for c in self.cat_cols:
                        cfg = self.col_config[c]
                        if cfg['encoder_type'] == 'target':
                            X_col = df[c].astype(object).fillna('__NA__')
                            global_mean = float(pd.Series(y).mean())
                            counts = X_col.value_counts()
                            smooth_map = {}
                            for cat, cnt in counts.items():
                                cat_mean = float(pd.Series(y)[X_col==cat].mean()) if cnt>0 else global_mean
                                alpha = cnt / (cnt + 0.3)
                                smooth_map[cat] = alpha * cat_mean + (1 - alpha) * global_mean
                            cfg['target_mapping'] = smooth_map
                            cfg['target_global_mean'] = global_mean
                        elif cfg['encoder_type'] == 'count':
                            counts = df[c].astype(object).value_counts().to_dict()
                            cfg['count_map'] = counts

                for c in self.text_cols:
                    s = df[c].astype(str).fillna("")
                    tfidf = TfidfVectorizer(max_features=int(self.text_cfg.get('tfidf_max',10000)), ngram_range=(1,2))
                    mat = tfidf.fit_transform(s)
                    n_comp = min(int(self.text_cfg.get('svd_max',50)), max(1, mat.shape[1]-1))
                    svd = TruncatedSVD(n_components=n_comp, random_state=42)
                    red = svd.fit_transform(mat)
                    names = [f"{c}__svd_{i}" for i in range(red.shape[1])]
                    self.text_models[c] = {'tfidf': tfidf, 'svd': svd, 'names': names}
                return self

            def _sanitize_numeric(self, df):
                df = df.copy()
                for c in self.num_cols:
                    ser = df[c]
                    ser = ser.replace([np.inf, -np.inf], np.nan)
                    if ser.notna().sum() > 0:
                        try:
                            hi = ser.quantile(0.999)
                            lo = ser.quantile(0.001)
                            df[c] = ser.clip(lower=lo, upper=hi)
                        except Exception:
                            df[c] = ser
                return df

            def transform(self, df, training_mode=False):
                df = df.copy()

                for c in self.text_cols:
                    s = df[c].astype(str).fillna("")
                    model = self.text_models.get(c)
                    if model:
                        mat = model['tfidf'].transform(s)
                        red = model['svd'].transform(mat)
                        for i, name in enumerate(model['names']):
                            df[name] = red[:, i]
                df.drop(columns=[c for c in self.text_cols if c in df.columns], inplace=True, errors='ignore')

                for c in df.columns:
                    if str(df[c].dtype).startswith('datetime64'):
                        df[c] = (df[c] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')

                df = self._sanitize_numeric(df)

                temp_df = df.copy()
                for c in self.num_cols:
                    info = self.col_config.get(c, {}).get('imputer', ('none', None))
                    if info[0] in ('knn','iterative'):
                        med = temp_df[c].median()
                        temp_df[c] = temp_df[c].fillna(med)

                scaler_objs = {}
                for c in self.num_cols:
                    scaler = choose_scaler_for_series(temp_df[c])
                    try:
                        scaler.fit(temp_df[c].values.reshape(-1,1))
                    except Exception:
                        scaler = StandardScaler().fit(temp_df[c].fillna(temp_df[c].median()).values.reshape(-1,1))
                    scaler_objs[c] = scaler
                    self.col_config[c]['scaler_obj'] = scaler

                if self.num_cols:
                    scaled_matrix = []
                    for c in self.num_cols:
                        sc = scaler_objs[c]
                        col_vals = temp_df[c].values.reshape(-1,1)
                        try: scaled_col = sc.transform(col_vals).reshape(-1)
                        except Exception: scaled_col = StandardScaler().fit_transform(col_vals).reshape(-1)
                        scaled_matrix.append(scaled_col)
                    import numpy as _np
                    scaled_df = pd.DataFrame(_np.vstack(scaled_matrix).T, columns=self.num_cols, index=temp_df.index)
                else:
                    scaled_df = pd.DataFrame(index=df.index)

                need_knn = [c for c in self.num_cols if self.col_config[c]['imputer'][0]=='knn']
                if need_knn and len(self.num_cols)>0:
                    base = self.col_config[self.num_cols[0]]['imputer'][1]
                    n_neighbors = base.get('n_neighbors',5) if isinstance(base, dict) else 5
                    knn = KNNImputer(n_neighbors=n_neighbors)
                    imputed_scaled = knn.fit_transform(scaled_df)
                    imputed_scaled_df = pd.DataFrame(imputed_scaled, columns=self.num_cols, index=scaled_df.index)
                    for c in self.num_cols:
                        sc = scaler_objs[c]
                        try: inv = sc.inverse_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        except Exception: inv = sc.fit_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        df[c] = inv

                need_iter = [c for c in self.num_cols if self.col_config[c]['imputer'][0]=='iterative']
                if need_iter:
                    iter_max = 10
                    base2 = self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    if isinstance(base2, dict): iter_max = base2.get('max_iter',10)
                    iter_imp = IterativeImputer(max_iter=iter_max, random_state=0)
                    arr = iter_imp.fit_transform(df[self.num_cols])
                    df[self.num_cols] = pd.DataFrame(arr, columns=self.num_cols, index=df.index)
                    for c in need_iter: self.col_config[c]['imputer_obj'] = iter_imp

                for c in self.num_cols:
                    info = self.col_config[c]['imputer']
                    if info[0] == 'simple_median':
                        imp = info[1]
                        df[c] = imp.transform(df[[c]])

                if self.config.get('outlier_strategy','drop_iqr') == 'drop_iqr':
                    df, n_removed = drop_outliers_iqr(df, self.num_cols, fold=self.config.get('outlier_fold',1.5))
                elif self.config.get('outlier_strategy') == 'winsorize':
                    df, _ = winsorize_df(df, self.num_cols, fold=self.config.get('outlier_fold',1.5))

                for c in self.num_cols:
                    sc = self.col_config[c].get('scaler_obj')
                    if sc is None:
                        sc = choose_scaler_for_series(df[c].fillna(df[c].median()))
                    try:
                        df[c] = sc.transform(df[[c]].values).reshape(-1)
                    except Exception:
                        df[c] = StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                    self.col_config[c]['scaler_obj'] = sc

                for c in self.cat_cols:
                    if c not in df.columns: continue
                    cfg = self.col_config[c]
                    s = df[c].astype(object)
                    rare_thresh = cfg.get('rare_threshold_frac', 0.01)
                    if rare_thresh < 1.0:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thresh, threshold_count=None)
                    else:
                        collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thresh))
                    df[c] = collapsed
                    cfg['rare_values'] = list(rare_set)
                    if self.config.get('enable_string_similarity', False) and cfg['n_unique'] > 20:
                        grouped, mapping = string_similarity_group(df[c], score_threshold=90)
                        df[c] = grouped
                        cfg['string_similarity_map'] = mapping
                    enc_type = cfg.get('encoder_type')
                    if enc_type == 'onehot':
                        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                        resh = df[[c]].astype(str)
                        ohe.fit(resh)
                        arr = ohe.transform(resh)
                        cols = [f"{c}__{cat}" for cat in ohe.categories_[0]]
                        df_ohe = pd.DataFrame(arr, columns=cols, index=df.index)
                        df = pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                        cfg['encoder_obj'] = ohe
                        cfg['ohe_columns'] = cols
                    elif enc_type == 'ordinal':
                        ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                        resh = df[[c]].astype(object)
                        try:
                            ord_enc.fit(resh)
                            df[c] = ord_enc.transform(resh).astype(float)
                            cfg['encoder_obj'] = ord_enc
                        except Exception:
                            df[c] = df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)
                    elif enc_type == 'count':
                        cnt_map = df[c].value_counts().to_dict()
                        df[c] = df[c].map(lambda x: cnt_map.get(x, 0))
                        cfg['count_map'] = cnt_map
                    elif enc_type == 'target':
                        mapping = cfg.get('target_mapping', {})
                        global_mean = cfg.get('target_global_mean', 0.0)
                        df[c] = df[c].map(lambda x: mapping.get(x, global_mean))
                    else:
                        df[c] = df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)

                for c in list(df.columns):
                    if df[c].dtype=='object':
                        try: df[c] = pd.to_numeric(df[c], errors='coerce')
                        except Exception: pass
                return df

        # ---------- Printing helpers ----------
        def _cls_name(obj):
            try: return obj.__class__.__name__
            except Exception: return str(obj)

        def print_before(df, target_col, preview_rows):
            print("DF_HEAD:"); print(df.head(preview_rows).to_string())
            print("DF_INFO:"); print(df.info())
            print("DF_DTYPES:"); print(df.dtypes.astype(str))
            print("DF_DESCRIBE:"); print(df.describe(include='all').transpose())
            print("DF_NULL_COUNTS:"); print(df.isna().sum())

        def print_plan(pre):
            print("PLAN_NUM_COLS:", pre.num_cols)
            print("PLAN_CAT_COLS:", pre.cat_cols)
            print("PLAN_TEXT_COLS:", pre.text_cols)

        def print_after(X, selected_features=None):
            print("AFTER_HEAD:"); print(X.head(20).to_string())
            print("AFTER_DTYPES:"); print(X.dtypes.astype(str))
            print("AFTER_NULL_COUNTS:"); print(X.isna().sum())
            if selected_features is not None:
                print("SELECTED_FEATURES:", selected_features)

        # ---------- Main ----------
        ap = argparse.ArgumentParser()
        ap.add_argument('--in_file', type=str, required=True)
        ap.add_argument('--target_column', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="auto")
        ap.add_argument('--drop_cols_csv', type=str, default="piMetadata,execution_timestamp,pipelineid,component_id,projectid")
        ap.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        ap.add_argument('--outlier_fold', type=float, default=1.5)
        ap.add_argument('--max_knn_rows', type=int, default=5000)
        ap.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        ap.add_argument('--rare_threshold', type=float, default=0.01)
        ap.add_argument('--enable_string_similarity', type=str, default="false")
        ap.add_argument('--preview_rows', type=int, default=20)
        ap.add_argument('--text_tfidf_max_features', type=int, default=10000)
        ap.add_argument('--text_svd_max_components', type=int, default=50)
        ap.add_argument('--X', type=str, required=True)
        ap.add_argument('--y', type=str, required=True)
        ap.add_argument('--preprocessor', type=str, required=True)
        ap.add_argument('--feature_selector', type=str, required=True)
        ap.add_argument('--preprocess_metadata', type=str, required=True)
        args = ap.parse_args()

        try:
            in_path = args.in_file
            target_col = args.target_column
            model_type = args.model_type.lower().strip() if args.model_type else "auto"
            drop_cols = [c.strip() for c in (args.drop_cols_csv or "").split(",") if c.strip()]
            outlier_strategy = args.outlier_strategy
            outlier_fold = float(args.outlier_fold)
            max_knn_rows = int(args.max_knn_rows)
            detect_thresh = float(args.numeric_detection_threshold)
            rare_threshold = float(args.rare_threshold)
            enable_string_similarity = str(args.enable_string_similarity).lower() in ("1","true","t","yes","y")
            preview_rows = int(args.preview_rows)
            text_cfg = {'tfidf_max': int(args.text_tfidf_max_features), 'svd_max': int(args.text_svd_max_components)}

            out_X = args.X; out_y = args.y; preproc_path = args.preprocessor; fs_path = args.feature_selector; meta_path = args.preprocess_metadata

            if not os.path.exists(in_path):
                print("ERROR: input file does not exist:", in_path, file=sys.stderr); sys.exit(1)
            print("Loading:", in_path)
            df = read_with_pandas(in_path)

            for c in drop_cols:
                if c in df.columns:
                    df.drop(columns=[c], inplace=True)

            print_before(df, target_col, preview_rows)

            df = safe_drop_duplicates(df)

            if target_col not in df.columns:
                print(f"ERROR: target column '{target_col}' not found in data", file=sys.stderr); sys.exit(1)

            before = len(df)
            df = df[df[target_col].notna()].copy()
            print(f"Dropped rows with null target '{target_col}': {before - len(df)} rows dropped")

            y_raw = df[target_col].copy()
            X_init = df.drop(columns=[target_col]).copy()

            X_init, conv_report = convert_object_columns_advanced(X_init, detect_threshold=detect_thresh, decimal_comma=False, advanced=True)
            X_init, date_report = detect_and_extract_dates(X_init, date_parse_enabled=True)

            y = y_raw.copy()
            if model_type == "auto":
                is_numeric = pd.api.types.is_numeric_dtype(y)
                n_unique = y.nunique(dropna=True)
                if not is_numeric:
                    model_type = "classification"
                else:
                    model_type = "classification" if n_unique<=10 else "regression"
            if model_type == "classification":
                if y.dtype == bool:
                    y = y.astype(int)
                elif not pd.api.types.is_numeric_dtype(y):
                    le = LabelEncoder(); y = pd.Series(le.fit_transform(y), index=y.index)
                    target_classes = dict(zip([str(x) for x in le.classes_], [int(i) for i in range(len(le.classes_))]))
                else:
                    target_classes = None
            else:
                target_classes = None

            config = {
                'outlier_strategy': outlier_strategy,
                'outlier_fold': outlier_fold,
                'max_knn_rows': max_knn_rows,
                'numeric_detection_threshold': detect_thresh,
                'rare_threshold': rare_threshold,
                'enable_string_similarity': enable_string_similarity
            }
            pre = Preprocessor(config=config, text_cfg=text_cfg)
            pre.fit(X_init, y=y if model_type=="classification" else None, task=model_type)
            print_plan(pre)

            X_processed = pre.transform(X_init, training_mode=True)

            for c in list(X_processed.columns):
                if X_processed[c].dtype=='object':
                    try: X_processed[c] = pd.to_numeric(X_processed[c], errors='coerce')
                    except Exception: pass

            fs = FeatureSelector(task=model_type)
            fs.fit(X_processed, y)
            Xs = fs.transform(X_processed)
            print_after(Xs, fs.selected_features)

            ensure_dir_for(out_X); ensure_dir_for(out_y); ensure_dir_for(preproc_path); ensure_dir_for(fs_path); ensure_dir_for(meta_path)
            Xs.to_parquet(out_X, index=False)
            y_df = y.to_frame(name=target_col) if hasattr(y, "to_frame") else pd.DataFrame({target_col: y})
            y_df.to_parquet(out_y, index=False)
            joblib.dump(pre, preproc_path)
            joblib.dump(fs, fs_path)

            col_cfg_dump = {}
            for k, cfg in pre.col_config.items():
                d = {}
                for kk, vv in cfg.items():
                    try:
                        if hasattr(vv, "__class__"):
                            d[kk] = _cls_name(vv)
                        else:
                            d[kk] = str(vv)
                    except Exception:
                        d[kk] = str(vv)
                col_cfg_dump[k] = d

            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'config': config,
                'conversion_report': conv_report,
                'date_report': date_report,
                'num_cols': pre.num_cols,
                'cat_cols': pre.cat_cols,
                'text_cols': pre.text_cols,
                'col_config_summary': col_cfg_dump,
                'model_type': model_type,
                'target_classes': target_classes,
                'selected_features': fs.selected_features
            }
            with open(meta_path, 'w', encoding='utf-8') as fh:
                json.dump(metadata, fh, indent=2, ensure_ascii=False)

            print("SUCCESS: Preprocessing + Feature Selection complete.")

        except Exception as exc:
            print("ERROR during preprocessing:", exc, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --tfidf_max_features
      - {inputValue: tfidf_max_features}
      - --svd_dims
      - {inputValue: svd_dims}
      - --preview_rows
      - {inputValue: preview_rows}
      - --X
      - {outputPath: X}
      - --y
      - {outputPath: y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
