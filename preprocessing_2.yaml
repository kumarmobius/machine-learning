name: Advanced Preprocess v10
description: |
  End-to-end robust preprocessor and feature selector.
  - Loads CSV/TSV/JSON/JSONL/Excel/Parquet/Feather/ORC or directory of files.
  - Drops user-specified junk columns early.
  - Coerces dict/list to JSON strings to avoid unhashable errors.
  - Removes duplicates safely.
  - Converts alphanumeric numbers, units, percents to numeric.
  - Detects date-like strings, extracts calendar features.
  - Smart imputers per column (none/median/KNN/Iterative).
  - Smart scalers per column (MinMax/Standard/Robust/Power).
  - Rare-label collapsing and encoder selection per categorical (OneHot/Target/Count/Ordinal).
  - Optional string-similarity grouping for high-cardinality categoricals.
  - Optional outlier handling (drop IQR or winsorize) during fit only.
  - Text columns: TF-IDF + TruncatedSVD when long-enough text is detected.
  - Target normalization: maps boolean-like, yes/no, male/female, etc.
  - Task detection (auto) or user-provided model_type (classification|regression).
  - Two-stage feature selection: Mutual Information -> Model-based (LightGBM or RF fallback).
  - Saves X, y (parquet), preprocessor.pkl, feature_selector.pkl, metadata.json.
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification|regression|auto", optional: true, default: "auto"}
  - {name: drop_cols_csv, type: String, description: "Columns to drop early (comma-separated)", optional: true, default: "piMetadata,execution_timestamp,pipelineid,component_id,projectid"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: String, description: "IQR fold for outlier rule", optional: true, default: "1.5"}
  - {name: max_knn_rows, type: String, description: "Max rows for KNN imputer else fallback", optional: true, default: "5000"}
  - {name: numeric_detection_threshold, type: String, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: rare_threshold, type: String, description: "Rare label threshold as fraction or absolute count", optional: true, default: "0.01"}
  - {name: enable_string_similarity, type: String, description: "true/false for high-card grouping", optional: true, default: "false"}
  - {name: text_tfidf_max_features, type: String, description: "Max vocabulary per text column", optional: true, default: "10000"}
  - {name: text_svd_max_components, type: String, description: "SVD dims per text column when applied", optional: true, default: "50"}
  - {name: preview_rows, type: String, description: "Rows to include in preview prints", optional: true, default: "20"}
outputs:
  - {name: X, type: Data, description: "Processed features parquet"}
  - {name: y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "Pickled Preprocessor"}
  - {name: feature_selector, type: Data, description: "Pickled FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSON metadata of decisions and diagnostics"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime
        from collections import Counter
        def pip_install(pkgs):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input"] + pkgs)
        try:
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
            _lgbm_ok=True
            try:
                from lightgbm import LGBMClassifier, LGBMRegressor  # type: ignore
            except Exception:
                _lgbm_ok=False
        except Exception:
            pip_install(["pandas","numpy","scikit-learn","joblib","category_encoders","rapidfuzz","pyarrow","fastparquet","openpyxl","scipy"])
            try:
                pip_install(["lightgbm"])
            except Exception:
                pass
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
            _lgbm_ok=True
            try:
                from lightgbm import LGBMClassifier, LGBMRegressor  # type: ignore
            except Exception:
                _lgbm_ok=False

        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh: return fh.read(n)
            except Exception: return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            return txt[0:1] in ("{","[") or ("\n{" in txt) or ("\n[" in txt)

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path=max(files, key=lambda p: os.path.getsize(p))
                print("Info: directory input. Selected:", path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input not found or not a file: "+str(path))
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2, encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2, encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except Exception: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            sample=sample_file_bytes(path)
            if is_likely_json(sample):
                try: return pd.read_json(path, lines=True)
                except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported or unreadable file format: "+path)

        def coerce_unhashable_objects(df):
            def _coerce(x):
                if isinstance(x, (dict, list, tuple)):
                    try: return json.dumps(x, sort_keys=True, ensure_ascii=False)
                    except Exception: return str(x)
                return x
            obj_cols=[c for c in df.columns if df[c].dtype=='object']
            for c in obj_cols:
                try:
                    pd.factorize(df[c], sort=True)
                except TypeError:
                    df[c]=df[c].map(_coerce)
            return df

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)'
                              r'(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)'
                              r'\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',', '.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception:
                    try: val=float(number_clean.replace(',', '.'))
                    except Exception: return np.nan
                if mult: val *= MULTIPLIER_MAP.get(mult.lower(),1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            ms=re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if ms:
                try: return float(ms.group(0).replace(',',''))
                except Exception: return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df=df.copy(); report={'converted':[], 'skipped':[]}
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                parsable=parsed.notna().sum(); frac=parsable/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed; report['converted'].append({'col':col,'parsable_fraction':float(frac)})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':float(frac)})
            return df, report

        def detect_and_extract_dates(df):
            df=df.copy(); report={'date_columns':[], 'skipped':[]}
            cand=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in cand:
                ser=df[col].astype(object); sample=ser.dropna().astype(str).head(500)
                if sample.empty: report['skipped'].append({'col':col,'parsable_fraction':0.0}); continue
                parsed=pd.to_datetime(sample, errors='coerce', utc=False)
                frac=float(parsed.notna().mean())
                if frac>=0.6:
                    full=pd.to_datetime(ser, errors='coerce', utc=False); df[col+"_orig"]=df[col]; df[col]=full
                    df[col+"_year"]=df[col].dt.year
                    df[col+"_month"]=df[col].dt.month
                    df[col+"_day"]=df[col].dt.day
                    df[col+"_dow"]=df[col].dt.dayofweek
                    df[col+"_is_weekend"]=df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col+"_is_month_start"]=df[col].dt.is_month_start.astype('Int64')
                    df[col+"_is_month_end"]=df[col].dt.is_month_end.astype('Int64')
                    df[col+"_days_since_epoch"]=(df[col]-pd.Timestamp("1970-01-01"))//pd.Timedelta('1D')
                    report['date_columns'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts=series.value_counts(dropna=False); n=len(series)
            if threshold_count is not None:
                rare=set(counts[counts<=threshold_count].index)
            else:
                rare=set(counts[counts<=max(1,int(threshold_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals=[v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping={}; used=set()
            for v in vals:
                if v in used: continue
                matches=rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=score_threshold)
                group=[m[0] for m in matches]
                for g in group: mapping[g]=v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0
            q1=df[numeric_cols].quantile(0.25, numeric_only=True); q3=df[numeric_cols].quantile(0.75, numeric_only=True); iqr=q3-q1
            lower=(q1 - fold*iqr).reindex(numeric_cols); upper=(q3 + fold*iqr).reindex(numeric_cols)
            mask=pd.Series(True, index=df.index)
            for c in numeric_cols:
                col=df[c]
                lc=lower.get(c, np.nan); uc=upper.get(c, np.nan)
                ok=pd.Series(True, index=df.index)
                if pd.notna(lc): ok = ok & col.ge(lc)
                if pd.notna(uc): ok = ok & col.le(uc)
                mask = mask & ok.fillna(True)
            before=len(df); df2=df.loc[mask.values].copy().reset_index(drop=True)
            return df2, before-len(df2)

        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0
            q1=df[numeric_cols].quantile(0.25, numeric_only=True); q3=df[numeric_cols].quantile(0.75, numeric_only=True); iqr=q3-q1
            lower=(q1 - fold*iqr).reindex(numeric_cols); upper=(q3 + fold*iqr).reindex(numeric_cols)
            df2=df.copy()
            for c in numeric_cols:
                col=df2[c]
                lc=lower.get(c, np.nan); uc=upper.get(c, np.nan)
                if pd.notna(lc): col=col.clip(lower=lc)
                if pd.notna(uc): col=col.clip(upper=uc)
                df2[c]=col
            return df2,0

        def choose_scaler_for_series(s):
            v=s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0)==0: return StandardScaler()
            skew=float(v.skew()); kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            if v.min()>=0.0 and v.max()<=1.0: return MinMaxScaler()
            if abs(skew)>=1.0: return Pipeline([('power',PowerTransformer(method='yeo-johnson')),('std',StandardScaler())])
            if float(((v - v.mean()).abs()>3*v.std(ddof=0)).mean())>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        def choose_categorical_encoder(col, series, target_series=None, train_mode=True):
            n=len(series); nunique=series.nunique(dropna=True)
            high_card=(nunique>max(50, 0.05*n))
            if high_card:
                if target_series is not None and train_mode: return 'target', ce.TargetEncoder(cols=[col], smoothing=0.3), {}
                else: return 'count', None, {}
            else:
                if nunique<=10: return 'onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), {}
                if target_series is not None and train_mode: return 'target', ce.TargetEncoder(cols=[col], smoothing=0.3), {}
                else: return 'ordinal', OrdinalEncoder(), {}

        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=0.3, random_state=42):
            X_col=df[col].astype(object).fillna('__NA__'); global_mean=float(pd.Series(y).mean())
            oof=pd.Series(index=df.index, dtype=float)
            kf=KFold(n_splits=min(max(2, len(df)//10), n_splits), shuffle=True, random_state=random_state)
            for tr_idx, val_idx in kf.split(df):
                means=pd.Series(y).iloc[tr_idx].groupby(X_col.iloc[tr_idx]).mean()
                oof.iloc[val_idx]=X_col.iloc[val_idx].map(lambda v: means.get(v, global_mean))
            counts=X_col.value_counts()
            smooth_map={}
            for cat,cnt in counts.items():
                cat_mean=float(pd.Series(y)[X_col==cat].mean()) if cnt>0 else global_mean
                alpha=cnt/(cnt+smoothing); smooth_map[cat]=alpha*cat_mean + (1-alpha)*global_mean
            return oof.fillna(global_mean), smooth_map, global_mean

        class Preprocessor:
            def __init__(self, config):
                self.config=config
                self.num_cols=[]; self.cat_cols=[]
                self.col_config={}
                self.text_columns_info={}
            def _log_plan(self):
                print("PLAN: numeric:", self.num_cols)
                print("PLAN: categorical:", self.cat_cols)
                for c in self.num_cols:
                    cfg=self.col_config.get(c,{}); imp=cfg.get('imputer',('none',None))
                    sc=cfg.get('scaler_obj'); print(f"[NUM] {c} | missing={cfg.get('missing_frac')} | imputer={imp[0]} | scaler={type(sc).__name__ if sc else 'auto'}")
                for c in self.cat_cols:
                    cfg=self.col_config.get(c,{})
                    print(f"[CAT] {c} | nuniq={cfg.get('n_unique')} | high_card={cfg.get('high_card')} | encoder={cfg.get('encoder_type')} | rare_thr={cfg.get('rare_threshold_frac')}")

            def _detect_text_columns(self, df):
                text_cols=[]
                obj_cols=[c for c in df.columns if df[c].dtype=='object']
                for c in obj_cols:
                    s=df[c].astype(str)
                    avg_words=s.map(lambda x: len(x.split())).mean()
                    max_token_len=s.map(lambda x: max((len(t) for t in x.split()), default=0)).max()
                    if avg_words>2 or max_token_len>=10:
                        text_cols.append(c)
                return text_cols

            def fit(self, df, y=None):
                text_cols=self._detect_text_columns(df)
                tfidf_max=int(self.config.get('text_tfidf_max_features',10000))
                svd_dims=int(self.config.get('text_svd_max_components',50))
                used_cols=set()
                for c in text_cols:
                    try:
                        vec=TfidfVectorizer(max_features=tfidf_max, ngram_range=(1,2), min_df=2)
                        tfidf=vec.fit_transform(df[c].astype(str).fillna(""))
                        if svd_dims>0 and tfidf.shape[1]>svd_dims:
                            from sklearn.decomposition import TruncatedSVD
                            svd=TruncatedSVD(n_components=min(svd_dims, tfidf.shape[1]-1), random_state=42)
                            red=svd.fit_transform(tfidf)
                            out_cols=[f"{c}__svd_{i}" for i in range(red.shape[1])]
                            self.text_columns_info[c]=(vec, svd, out_cols)
                        else:
                            out_cols=[f"{c}__tfidf_{i}" for i in range(tfidf.shape[1])]
                            self.text_columns_info[c]=(vec, None, out_cols)
                        used_cols.add(c)
                    except Exception as e:
                        print(f"Warn: text vectorization skipped for {c} due to {e}")
                df2=df.drop(columns=list(used_cols), errors='ignore').copy()

                self.num_cols=df2.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols=[c for c in df2.columns if c not in self.num_cols]
                nrows=len(df2)
                for c in self.num_cols:
                    s=df2[c]; cfg={}
                    miss=float(s.isna().mean()); cfg['missing_frac']=miss
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['skew']=float(s.dropna().skew()) if s.dropna().shape[0]>2 else 0.0
                    cfg['kurtosis']=float(s.dropna().kurtosis()) if s.dropna().shape[0]>3 else 0.0
                    if miss==0: cfg['imputer']=('none',None)
                    elif miss<0.02:
                        imp=SimpleImputer(strategy='median'); imp.fit(np.array(s).reshape(-1,1)); cfg['imputer']=('simple_median', imp)
                    else:
                        if nrows<=int(self.config.get('max_knn_rows',5000)) and miss<0.25:
                            cfg['imputer']=('knn', {'n_neighbors':5})
                        else:
                            cfg['imputer']=('iterative', {'max_iter':10, 'random_state':0})
                    cfg['scaler_choice']='auto'; self.col_config[c]=cfg
                for c in self.cat_cols:
                    s=df2[c].astype(object); cfg={}
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['missing_frac']=float(s.isna().mean())
                    cfg['high_card']=cfg['n_unique']>max(50,0.05*nrows)
                    cfg['rare_threshold_frac']=float(self.config.get('rare_threshold',0.01))
                    enc_name, enc_obj, _ = choose_categorical_encoder(c, s, (y if y is not None else None), train_mode=(y is not None))
                    cfg['encoder_type']=enc_name; cfg['encoder_obj']=enc_obj; self.col_config[c]=cfg
                if y is not None:
                    for c in self.cat_cols:
                        cfg=self.col_config[c]
                        if cfg['encoder_type']=='target':
                            _, mapping, global_mean = fit_target_encoder_kfold(df2, c, y, n_splits=5)
                            cfg['target_mapping']=mapping; cfg['target_global_mean']=float(global_mean)
                        elif cfg['encoder_type']=='count':
                            counts=df2[c].astype(object).value_counts().to_dict(); cfg['count_map']=counts
                self._log_plan()
                return self

            def transform(self, df, training_mode=False):
                df=df.copy()
                for c,(vec, svd, out_cols) in self.text_columns_info.items():
                    X=vec.transform(df[c].astype(str).fillna(""))
                    if svd is not None:
                        Xs=svd.transform(X)
                    else:
                        Xs=X.toarray()
                    Xs=pd.DataFrame(Xs, columns=out_cols, index=df.index)
                    df=pd.concat([df.drop(columns=[c], errors='ignore'), Xs], axis=1)

                num_cols=[c for c in self.num_cols if c in df.columns]
                cat_cols=[c for c in self.cat_cols if c in df.columns]

                temp_df=df.copy(); scaler_objs={}
                for c in num_cols:
                    imputer_info=self.col_config.get(c,{}).get('imputer',('none',None))
                    if imputer_info[0] in ('knn','iterative'):
                        med=temp_df[c].median(); temp_df[c]=temp_df[c].fillna(med)
                for c in num_cols:
                    sample=temp_df[c]; sc=choose_scaler_for_series(sample)
                    try: sc.fit(sample.values.reshape(-1,1))
                    except Exception: sc=StandardScaler(); sc.fit(sample.values.reshape(-1,1))
                    scaler_objs[c]=sc; self.col_config[c]['scaler_obj']=sc

                if num_cols:
                    scaled_df=np.zeros((len(temp_df), len(num_cols)), dtype=float)
                    for i,c in enumerate(num_cols):
                        sc=scaler_objs[c]
                        col_vals=temp_df[c].values.reshape(-1,1)
                        try: scaled_col=sc.transform(col_vals).reshape(-1)
                        except Exception: scaled_col=StandardScaler().fit_transform(col_vals).reshape(-1)
                        scaled_df[:,i]=scaled_col
                    scaled_df=pd.DataFrame(scaled_df, columns=num_cols, index=temp_df.index)
                    need_knn=[c for c in num_cols if self.col_config[c]['imputer'][0]=='knn']
                    if need_knn:
                        base_imp=self.col_config[num_cols[0]]['imputer'][1] if num_cols else None
                        n_neighbors=5
                        if isinstance(base_imp, dict): n_neighbors=base_imp.get('n_neighbors',5)
                        knn=KNNImputer(n_neighbors=n_neighbors)
                        imputed_knn=knn.fit_transform(scaled_df)
                        imputed_knn=pd.DataFrame(imputed_knn, columns=num_cols, index=scaled_df.index)
                        for c in num_cols:
                            sc=scaler_objs[c]
                            try: inv=sc.inverse_transform(imputed_knn[c].values.reshape(-1,1)).reshape(-1)
                            except Exception: inv=StandardScaler().fit_transform(imputed_knn[c].values.reshape(-1,1)).reshape(-1)
                            df[c]=inv
                    need_iter=[c for c in num_cols if self.col_config[c]['imputer'][0]=='iterative']
                    if need_iter:
                        base_imp2=self.col_config[num_cols[0]]['imputer'][1] if num_cols else None
                        max_iter=10
                        if isinstance(base_imp2, dict): max_iter=base_imp2.get('max_iter',10)
                        iter_imp=IterativeImputer(max_iter=max_iter, random_state=0)
                        arr=iter_imp.fit_transform(df[num_cols])
                        df[num_cols]=pd.DataFrame(arr, columns=num_cols, index=df.index)
                        for c in need_iter: self.col_config[c]['imputer_obj']=iter_imp

                    for c in num_cols:
                        info=self.col_config[c]; if info['imputer'][0]=='simple_median':
                            imp=info['imputer'][1]; df[c]=imp.transform(df[[c]])

                    if training_mode:
                        strat=self.config.get('outlier_strategy','drop_iqr')
                        if strat=='drop_iqr':
                            df, n_removed = drop_outliers_iqr(df, num_cols, fold=float(self.config.get('outlier_fold',1.5)))
                            print(f"Outliers dropped: {n_removed}")
                        elif strat=='winsorize':
                            df, _ = winsorize_df(df, num_cols, fold=float(self.config.get('outlier_fold',1.5)))

                    for c in num_cols:
                        sc=self.col_config[c].get('scaler_obj') or choose_scaler_for_series(df[c].fillna(df[c].median()))
                        try: df[c]=sc.transform(df[[c]].values).reshape(-1)
                        except Exception:
                            try: df[c]=sc.fit_transform(df[[c]].values).reshape(-1)
                            except Exception: df[c]=StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                        self.col_config[c]['scaler_obj']=sc

                for c in cat_cols:
                    cfg=self.col_config[c]; s=df[c].astype(object)
                    rare_thr=float(cfg.get('rare_threshold_frac',0.01))
                    if rare_thr<1.0: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thr, threshold_count=None)
                    else: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thr))
                    df[c]=collapsed; cfg['rare_values']=list(rare_set)
                    if str(self.config.get('enable_string_similarity',"false")).lower() in ("1","true","t","yes","y") and cfg['n_unique']>20:
                        grouped, mapping=string_similarity_group(df[c], score_threshold=90); df[c]=grouped; cfg['string_similarity_map']=mapping
                    enc_type=cfg.get('encoder_type')
                    if enc_type=='onehot':
                        ohe=OneHotEncoder(sparse_output=False, handle_unknown='ignore'); resh=df[[c]].astype(str)
                        ohe.fit(resh); arr=ohe.transform(resh); cols=[f"{c}__{cat}" for cat in ohe.categories_[0]]
                        df_ohe=pd.DataFrame(arr, columns=cols, index=df.index); df=pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                        cfg['encoder_obj']=ohe; cfg['ohe_columns']=cols
                    elif enc_type=='ordinal':
                        ord_enc=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1); resh=df[[c]].astype(object)
                        try: ord_enc.fit(resh); df[c]=ord_enc.transform(resh).astype(float); cfg['encoder_obj']=ord_enc
                        except Exception: df[c]=df[c].astype('category').cat.codes.astype(float)
                    elif enc_type=='count':
                        cnt_map=df[c].value_counts().to_dict(); df[c]=df[c].map(lambda x: cnt_map.get(x,0)); cfg['count_map']=cnt_map
                    elif enc_type=='target':
                        mapping=cfg.get('target_mapping',{}); g=cfg.get('target_global_mean',0.0)
                        df[c]=df[c].map(lambda x: mapping.get(x,g))
                    else:
                        df[c]=df[c].astype('category').cat.codes.replace({-1: np.nan}).astype(float)
                return df

        class FeatureSelector:
            def __init__(self, task, k_override=None):
                self.task=task; self.k_override=k_override
                self.mi_selector=None; self.model_selector=None; self.selected_features=[]
            def _auto_k(self, p):
                if self.k_override is not None: return max(1, min(int(self.k_override), p))
                return max(5, min(int(round(math.sqrt(p)*2)), p))
            def fit(self, X, y):
                p=X.shape[1]; k=self._auto_k(p)
                if self.task=="classification":
                    self.mi_selector=SelectKBest(score_func=mutual_info_classif, k=k).fit(X, y)
                else:
                    self.mi_selector=SelectKBest(score_func=mutual_info_regression, k=k).fit(X, y)
                X1=self.mi_selector.transform(X)
                if self.task=="classification":
                    if _lgbm_ok:
                        est=LGBMClassifier(n_estimators=600, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42)
                    else:
                        est=RandomForestClassifier(n_estimators=400, max_depth=None, n_jobs=-1, random_state=42)
                else:
                    if _lgbm_ok:
                        est=LGBMRegressor(n_estimators=600, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42)
                    else:
                        est=RandomForestRegressor(n_estimators=400, max_depth=None, n_jobs=-1, random_state=42)
                self.model_selector=SelectFromModel(estimator=est, threshold="median", max_features=X1.shape[1]).fit(X1, y)
                mask1=self.mi_selector.get_support(); mask2=self.model_selector.get_support()
                cols=np.array(X.columns)[mask1][mask2]
                self.selected_features=list(cols); return self
            def transform(self, X):
                X1=self.mi_selector.transform(X); X2=self.model_selector.transform(X1)
                return pd.DataFrame(X2, columns=self.selected_features, index=X.index)

        _truthy=set(["true","yes","y","1","t"]) ; _falsy=set(["false","no","n","0","f"])
        def normalize_target_series(y):
            ser=pd.Series(y).copy()
            lower=ser.astype(str).str.lower()
            if set(lower.unique()) <= (_truthy | _falsy):
                ser=lower.map(lambda v: 1 if v in _truthy else 0)
                task="classification"
                return ser.astype(int), task, {"mapping":"bool->01"}
            mapping={"male":1,"m":1,"female":0,"f":0,"others":2,"other":2,"non-binary":2,"nb":2}
            if lower.isin(mapping.keys()).mean()>0.8:
                ser=lower.map(lambda v: mapping.get(v,2)).astype(int)
                task="classification"
                return ser, task, {"mapping":"sex->labels"}
            nunique=ser.nunique(dropna=True)
            if nunique>1 and nunique<=10:
                codes, uniques = pd.factorize(ser, sort=True)
                task="classification"
                return pd.Series(codes), task, {"mapping":"factorize", "classes":[str(u) for u in uniques]}
            try:
                ser_num=pd.to_numeric(ser, errors='coerce')
                if ser_num.notna().mean()>0.9:
                    task="regression"
                    return ser_num, task, {"mapping":"numeric"}
            except Exception:
                pass
            codes, uniques = pd.factorize(ser, sort=True)
            task="classification"
            return pd.Series(codes), task, {"mapping":"factorize_fallback", "classes":[str(u) for u in uniques]}

        def print_before(df, target_col):
            print("BEFORE: head"); 
            try: print(df.head().to_string())
            except Exception: print(df.head())
            print("BEFORE: dtypes"); print(df.dtypes.astype(str))
            print("BEFORE: nulls"); print(df.isna().sum())

        def print_plan(pre):
            print("PLAN: per-column choices printed above via pre._log_plan()")

        def print_after(Xp, pre):
            print("AFTER: head"); 
            try: print(Xp.head().to_string())
            except Exception: print(Xp.head())
            print("AFTER: dtypes"); print(Xp.dtypes.astype(str))
            print("AFTER: nulls"); print(Xp.isna().sum())

        ap=argparse.ArgumentParser()
        ap.add_argument('--in_file', type=str, required=True)
        ap.add_argument('--target_column', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="auto")
        ap.add_argument('--drop_cols_csv', type=str, default="piMetadata,execution_timestamp,pipelineid,component_id,projectid")
        ap.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        ap.add_argument('--outlier_fold', type=str, default="1.5")
        ap.add_argument('--max_knn_rows', type=str, default="5000")
        ap.add_argument('--numeric_detection_threshold', type=str, default="0.6")
        ap.add_argument('--rare_threshold', type=str, default="0.01")
        ap.add_argument('--enable_string_similarity', type=str, default="false")
        ap.add_argument('--preview_rows', type=str, default="20")
        ap.add_argument('--text_tfidf_max_features', type=str, default="10000")
        ap.add_argument('--text_svd_max_components', type=str, default="50")
        ap.add_argument('--X', type=str, required=True)
        ap.add_argument('--y', type=str, required=True)
        ap.add_argument('--preprocessor', type=str, required=True)
        ap.add_argument('--feature_selector', type=str, required=True)
        ap.add_argument('--preprocess_metadata', type=str, required=True)
        args=ap.parse_args()

        try:
            cfg={
                'outlier_strategy': str(args.outlier_strategy),
                'outlier_fold': float(args.outlier_fold),
                'max_knn_rows': int(args.max_knn_rows),
                'numeric_detection_threshold': float(args.numeric_detection_threshold),
                'rare_threshold': float(args.rare_threshold),
                'enable_string_similarity': str(args.enable_string_similarity).lower() in ("1","true","t","yes","y"),
                'text_tfidf_max_features': int(args.text_tfidf_max_features),
                'text_svd_max_components': int(args.text_svd_max_components),
            }
            in_path=args.in_file; target_col=args.target_column
            if not os.path.exists(in_path): print("ERROR: input file not found:", in_path, file=sys.stderr); sys.exit(1)
            print("Loading:", in_path)
            df=read_with_pandas(in_path)
            print("Loaded shape:", df.shape)

            drop_cols=[c.strip() for c in str(args.drop_cols_csv).split(",") if c.strip()]
            df=df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')

            df=coerce_unhashable_objects(df)

            before=len(df); df=df.drop_duplicates(keep='first'); print(f"Removed duplicates: {before-len(df)} rows")

            if target_col not in df.columns:
                print(f"ERROR: target column '{target_col}' not in data", file=sys.stderr); sys.exit(1)

            print_before(df, target_col)

            y_raw=df[target_col]
            y_norm, inferred_task, y_info = normalize_target_series(y_raw)
            task=args.model_type.strip().lower()
            if task not in ("classification","regression"):
                task=inferred_task
            print("Task:", task, "| target_info:", y_info)
            df=df.drop(columns=[target_col]).copy()

            df, conv_report = convert_object_columns_advanced(df, detect_threshold=float(cfg['numeric_detection_threshold']), decimal_comma=False, advanced=True)
            print("Conversion report (truncated):", json.dumps(conv_report, default=str)[:1000])

            df, date_report = detect_and_extract_dates(df)
            print("Date report (truncated):", json.dumps(date_report, default=str)[:1000])

            pre=Preprocessor(config=cfg)
            pre.fit(df, y=y_norm if task in ('classification','regression') else None)

            print_plan(pre)

            Xp=pre.transform(df, training_mode=True)

            for c in list(Xp.columns):
                if Xp[c].dtype=='object':
                    try: Xp[c]=pd.to_numeric(Xp[c], errors='coerce')
                    except Exception: pass

            print_after(Xp, pre)

            fs=FeatureSelector(task=task, k_override=None).fit(Xp, y_norm)
            Xs=fs.transform(Xp)
            print("SELECTED FEATURES:", fs.selected_features)

            ensure_dir_for(args.X); ensure_dir_for(args.y); ensure_dir_for(args.preprocessor); ensure_dir_for(args.feature_selector); ensure_dir_for(args.preprocess_metadata)
            Xs.to_parquet(args.X, index=False)
            pd.DataFrame({target_col: y_norm}).to_parquet(args.y, index=False)
            joblib.dump(pre, args.preprocessor)
            joblib.dump(fs, args.feature_selector)

            metadata={
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'config': cfg,
                'conversion_report': conv_report,
                'date_report': date_report,
                'task': task,
                'selected_features': fs.selected_features,
                'num_cols': pre.num_cols,
                'cat_cols': pre.cat_cols
            }
            with open(args.preprocess_metadata, 'w', encoding='utf-8') as fh: json.dump(metadata, fh, indent=2, ensure_ascii=False)

            print("SUCCESS: wrote:")
            print(" X ->", args.X)
            print(" y ->", args.y)
            print(" preprocessor ->", args.preprocessor)
            print(" feature_selector ->", args.feature_selector)
            print(" metadata ->", args.preprocess_metadata)
        except Exception as exc:
            print("ERROR during preprocessing:", exc, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --drop_cols_csv
      - {inputValue: drop_cols_csv}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --text_tfidf_max_features
      - {inputValue: text_tfidf_max_features}
      - --text_svd_max_components
      - {inputValue: text_svd_max_components}
      - --preview_rows
      - {inputValue: preview_rows}
      - --X
      - {outputPath: X}
      - --y
      - {outputPath: y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
