name: Advanced Preprocess v8.3.0
description: |
  End-to-end robust preprocessing and feature selection that reads a single dataset containing features and target...
  Rules:
  - Drop columns with >40% missing values immediately after load
  - Drop unique identifier columns (>95% unique values)
  - Do not preprocess the target column
  - If model_type is classification and target is categorical or string-like, apply label encoding
  - If model_type is classification and target is numeric floats, round half-up to whole numbers
  - Outputs are train_X and train_y as separate parquet files. train_X does not contain the target column
  - Prints comprehensive data understanding statistics
  - Feature selection is optional via enable_feature_selection parameter
  - Preserves robust preprocessing logic with advanced categorical encoders
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enable_feature_selection, type: String, description: "true/false to enable feature selection", optional: true, default: "true"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: Float, description: "IQR fold for outlier rule", optional: true, default: "1.5"}
  - {name: max_knn_rows, type: Integer, description: "Max rows for KNN imputer else fallback", optional: true, default: "5000"}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: rare_threshold, type: Float, description: "Rare label threshold as fraction or absolute count", optional: true, default: "0.01"}
  - {name: enable_string_similarity, type: String, description: "true/false high-cardinality grouping", optional: true, default: "false"}
  - {name: missing_threshold, type: Float, description: "Drop columns with missing % above this", optional: true, default: "0.40"}
  - {name: unique_threshold, type: Float, description: "Drop columns with unique % above this (ID columns)", optional: true, default: "0.95"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: "20"}
outputs:
  - {name: train_X, type: Data, description: "Processed features parquet"}
  - {name: train_y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "joblib dump of fitted Preprocessor"}
  - {name: feature_selector, type: Data, description: "joblib dump of fitted FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSON metadata"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime
        import pandas as pd, numpy as np
        from sklearn.impute import SimpleImputer, KNNImputer
        from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
        from sklearn.pipeline import Pipeline
        from sklearn.model_selection import KFold
        from sklearn.experimental import enable_iterative_imputer  # noqa
        from sklearn.impute import IterativeImputer
        from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel, VarianceThreshold
        from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
        import joblib
        import re
        import category_encoders as ce
        from rapidfuzz import process as rf_process, fuzz as rf_fuzz
        import gzip, cloudpickle
        from sklearn.inspection import permutation_importance
        
        # ---------- helpers ----------
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh: return fh.read(n)
            except Exception: return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            if txt[0] in ("{","["): return True
            if "{" in txt or "[" in txt: return True
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path = max(files, key=lambda p: os.path.getsize(p))
                print("Info selected file: " + path)
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: " + str(path))
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported format: " + str(path))

        def make_hashable_for_dupes(df):
            df = df.copy()
            for c in df.columns:
                try:
                    if df[c].apply(lambda v: isinstance(v,(list,dict,set))).any():
                        df[c] = df[c].map(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(list,dict,set)) else v)
                except Exception:
                    df[c] = df[c].astype(str)
            return df

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',','.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception:
                    try: val=float(number_clean.replace(',', '.'))
                    except Exception: return np.nan
                if mult: val*=MULTIPLIER_MAP.get(mult.lower(),1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            num_search=re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if num_search:
                try: return float(num_search.group(0).replace(',',''))
                except Exception: return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df=df.copy(); report={'converted':[],'skipped':[]}
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                if col in ('__TARGET__TMP__',): continue
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                frac = parsed.notna().sum()/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed; report['converted'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def detect_and_extract_dates(df, enabled=True, exclude_cols=None):
            df=df.copy(); report={'date_columns':[],'skipped':[]}
            if not enabled: return df, report
            exclude_cols=set(exclude_cols or [])
            cand=[c for c in df.columns if c not in exclude_cols and (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in cand:
                ser=df[col].astype(object); sample=ser.dropna().astype(str).head(500)
                if sample.empty: report['skipped'].append({'col':col,'parsable_fraction':0.0}); continue
                parsed=pd.to_datetime(sample, errors='coerce', utc=False)
                frac=parsed.notna().mean()
                if frac>=0.6:
                    full=pd.to_datetime(ser, errors='coerce', utc=False)
                    df[col+"_orig"]=df[col]; df[col]=full
                    df[col+"_year"]=df[col].dt.year
                    df[col+"_month"]=df[col].dt.month
                    df[col+"_day"]=df[col].dt.day
                    df[col+"_dayofweek"]=df[col].dt.dayofweek
                    df[col+"_is_weekend"]=df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col+"_is_month_start"]=df[col].dt.is_month_start.astype('Int64')
                    df[col+"_is_month_end"]=df[col].dt.is_month_end.astype('Int64')
                    df[col+"_days_since_epoch"]=(df[col]-pd.Timestamp("1970-01-01"))//pd.Timedelta('1D')
                    report['date_columns'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts=series.value_counts(dropna=False); n=len(series)
            rare = set(counts[counts <= threshold_count].index) if threshold_count is not None else set(counts[counts <= max(1,int(threshold_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals=[v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping={}; used=set()
            for v in vals:
                if v in used: continue
                matches=rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=score_threshold)
                group=[m[0] for m in matches]
                for g in group: mapping[g]=v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0,pd.Series(True,index=df.index)
            q1 = df[numeric_cols].quantile(0.25, numeric_only=True)
            q3 = df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr = q3 - q1
            lower = (q1 - fold * iqr).to_dict()
            upper = (q3 + fold * iqr).to_dict()
            mask = pd.Series(True, index=df.index)
            for c in numeric_cols:
                s = df[c]
                lo = lower.get(c, None); hi = upper.get(c, None)
                cond = pd.Series(True, index=df.index)
                if lo is not None:
                    cond = cond & s.ge(lo)
                if hi is not None:
                    cond = cond & s.le(hi)
                cond = cond.reindex(df.index).fillna(True)
                mask = (mask & cond)
            before=len(df)
            df2 = df.loc[mask].copy()
            removed = before - len(df2)
            df2.reset_index(drop=True, inplace=True)
            return df2, removed, mask

        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols: return df,0
            q1=df[numeric_cols].quantile(0.25, numeric_only=True); q3=df[numeric_cols].quantile(0.75, numeric_only=True)
            iqr=q3-q1
            lower=(q1 - fold*iqr).to_dict(); upper=(q3 + fold*iqr).to_dict()
            df2=df.copy()
            for c in numeric_cols:
                lo = lower.get(c, None); hi = upper.get(c, None)
                if lo is not None or hi is not None:
                    df2[c]=df2[c].clip(lower=lo, upper=hi)
            return df2,0

        def choose_scaler_for_series(s):
            v=s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0)==0: return StandardScaler()
            skew=float(v.skew()); kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            extreme_frac=float(((v - v.mean()).abs() > 3*v.std(ddof=0)).mean())
            if (v.min()>=0.0 and v.max()<=1.0): return MinMaxScaler()
            if abs(skew)>=1.0: return Pipeline([('power',PowerTransformer(method='yeo-johnson')),('std',StandardScaler())])
            if extreme_frac>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        def choose_categorical_encoder(col, series, target_series=None, train_mode=True, model_type='classification'):
            n=len(series); nunique=series.nunique(dropna=True); high_card=(nunique>max(50,0.05*n))
            
            # Use advanced encoders for both high and low cardinality
            if high_card:
                if target_series is not None and train_mode and model_type == 'classification':
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    return 'count', None, {}
            else:
                if nunique<=10:
                    enc=OneHotEncoder(sparse_output=False, handle_unknown='ignore'); return 'onehot', enc, {}
                elif target_series is not None and train_mode and model_type == 'classification':
                    # Use target encoding for medium cardinality in classification
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    # Use count encoding as fallback
                    return 'count', None, {}

        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=0.3, random_state=42):
            X_col=df[col].astype(object).fillna('__NA__'); global_mean=y.mean()
            oof=pd.Series(index=df.index, dtype=float)
            from sklearn.model_selection import KFold
            kf=KFold(n_splits=min(n_splits,max(2,len(df)//10)), shuffle=True, random_state=random_state)
            for tr_idx, val_idx in kf.split(df):
                means=y.iloc[tr_idx].groupby(X_col.iloc[tr_idx]).mean()
                oof.iloc[val_idx]=X_col.iloc[val_idx].map(lambda v: means.get(v, global_mean))
            counts=X_col.value_counts(); smooth_map={}
            for cat,cnt in counts.items():
                cat_mean=y[X_col==cat].mean() if cnt>0 else global_mean
                alpha=cnt/(cnt+smoothing); smooth_map[cat]=alpha*cat_mean+(1-alpha)*global_mean
            return oof.fillna(global_mean), smooth_map, float(global_mean)

        class Preprocessor:
            def __init__(self):
                self.num_cols=[]; self.cat_cols=[]; self.col_config={}; self.global_metadata={}

            def fit(self, df, y=None, config=None, target_col=None, model_type='classification'):
                self.global_metadata['config']=config or {}
                self.global_metadata['target_col']=target_col
                self.global_metadata['model_type']=model_type
                nrows=len(df)
                
                # Identify numeric and categorical columns
                self.num_cols=df.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols=[c for c in df.columns if c not in self.num_cols]

                # Configure numeric columns
                for c in self.num_cols:
                    s=df[c]; cfg={}
                    missing_frac=s.isna().mean(); cfg['missing_frac']=float(missing_frac)
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['skew']=float(s.dropna().skew()) if s.dropna().shape[0]>2 else 0.0
                    cfg['kurtosis']=float(s.dropna().kurtosis()) if s.dropna().shape[0]>3 else 0.0
                    if missing_frac==0: cfg['imputer']=('none',None)
                    elif missing_frac<0.02:
                        imp=SimpleImputer(strategy='median'); imp.fit(np.array(s).reshape(-1,1)); cfg['imputer']=('simple_median',imp)
                    else:
                        if nrows<=self.global_metadata['config'].get('max_knn_rows',5000) and missing_frac<0.25:
                            cfg['imputer']=('knn',{'n_neighbors':5})
                        else:
                            cfg['imputer']=('iterative',{'max_iter':10,'random_state':0})
                    cfg['scaler_choice']='auto'
                    self.col_config[c]=cfg
                    
                # Configure categorical columns
                for c in self.cat_cols:
                    s=df[c].astype(object); cfg={}
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['missing_frac']=float(s.isna().mean())
                    cfg['high_card']=cfg['n_unique']>max(50,0.05*nrows)
                    cfg['rare_threshold_frac']=self.global_metadata['config'].get('rare_threshold',0.01)
                    enc_name,enc_obj,_=choose_categorical_encoder(c,s,(y if y is not None else None),train_mode=(y is not None), model_type=model_type)
                    cfg['encoder_type']=enc_name; cfg['encoder_obj']=enc_obj
                    self.col_config[c]=cfg
                    
                # Fit target encoders if classification
                if y is not None and model_type == 'classification':
                    for c in self.cat_cols:
                        cfg=self.col_config[c]
                        if cfg['encoder_type']=='target':
                            _, mapping, global_mean = fit_target_encoder_kfold(df, c, y, n_splits=5)
                            cfg['target_mapping']=mapping; cfg['target_global_mean']=float(global_mean)
                        elif cfg['encoder_type']=='count':
                            counts=df[c].astype(object).value_counts().to_dict(); cfg['count_map']=counts
                            
                self.global_metadata['num_cols']=self.num_cols; self.global_metadata['cat_cols']=self.cat_cols
                return self

            def transform(self, df, training_mode=False):
                import numpy as np, pandas as pd
                df=df.copy()

                # Never touch the target column
                target_col = self.global_metadata.get('target_col')
                protected_cols=set([target_col]) if target_col in df.columns else set()

                # normalize string tokens to NaN
                df.replace(["NaN","nan","None","null","INF","-INF","Inf","-Inf","inf","-inf"], np.nan, inplace=True)

                # force numeric on learned numeric cols
                for c in list(self.num_cols):
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors='coerce')

                temp_df=df.copy()

                for c in self.num_cols:
                    imputer_info=self.col_config.get(c,{}).get('imputer',('none',None))
                    if imputer_info[0] in ('knn','iterative'):
                        med=pd.to_numeric(temp_df[c], errors='coerce').median()
                        temp_df[c]=temp_df[c].fillna(med)

                scaler_objs={}
                for c in self.num_cols:
                    sample=temp_df[c]; scaler=choose_scaler_for_series(sample)
                    try: scaler.fit(sample.values.reshape(-1,1))
                    except Exception: scaler=StandardScaler(); scaler.fit(sample.values.reshape(-1,1))
                    scaler_objs[c]=scaler; self.col_config[c]['scaler_obj']=scaler

                num_matrix=temp_df[self.num_cols].copy()
                scaled_matrix=np.zeros_like(num_matrix.values, dtype=float)
                for i,c in enumerate(self.num_cols):
                    sc=scaler_objs[c]; col_vals=num_matrix[c].values.reshape(-1,1)
                    try: scaled_col=sc.transform(col_vals).reshape(-1)
                    except Exception: scaled_col=StandardScaler().fit_transform(col_vals).reshape(-1)
                    scaled_matrix[:,i]=scaled_col
                scaled_df=pd.DataFrame(scaled_matrix, columns=self.num_cols, index=num_matrix.index)

                need_knn=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='knn']
                if need_knn:
                    base_imp=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    knn_neighbors=base_imp.get('n_neighbors',5) if isinstance(base_imp,dict) else 5
                    knn=KNNImputer(n_neighbors=knn_neighbors)
                    imputed_scaled=knn.fit_transform(scaled_df)
                    imputed_scaled_df=pd.DataFrame(imputed_scaled, columns=self.num_cols, index=scaled_df.index)
                    for c in self.num_cols:
                        sc=scaler_objs[c]
                        try: inv=sc.inverse_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        except Exception: inv=imputed_scaled_df[c].values.reshape(-1)
                        df[c]=inv

                need_iter=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='iterative']
                if need_iter:
                    iter_max_iter=10; base_imp2=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    if isinstance(base_imp2,dict): iter_max_iter=base_imp2.get('max_iter',10)
                    iter_imp=IterativeImputer(max_iter=iter_max_iter, random_state=0)
                    arr=df[self.num_cols].astype(float).replace([np.inf,-np.inf], np.nan).values
                    iter_out=iter_imp.fit_transform(arr)
                    df[self.num_cols]=pd.DataFrame(iter_out, columns=self.num_cols, index=df.index)
                    for c in need_iter: self.col_config[c]['imputer_obj']=iter_imp

                for c in self.num_cols:
                    cfg=self.col_config[c]
                    if cfg['imputer'][0]=='simple_median':
                        imp=cfg['imputer'][1]; df[c]=imp.transform(df[[c]])

                cfg_global=self.global_metadata.get('config') or {}
                if training_mode:
                    if cfg_global.get('outlier_strategy','drop_iqr')=='drop_iqr':
                        df2, n_removed, kept_mask = drop_outliers_iqr(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                        self.global_metadata['outliers_removed']=int(n_removed)
                        self.global_metadata['kept_mask_after_outlier']=kept_mask
                        df=df2
                    elif cfg_global.get('outlier_strategy')=='winsorize':
                        df,_=winsorize_df(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                        self.global_metadata['kept_mask_after_outlier']=pd.Series(True,index=df.index)

                for c in self.num_cols:
                    sc=self.col_config[c].get('scaler_obj')
                    if sc is None: sc=choose_scaler_for_series(df[c].fillna(df[c].median()))
                    try: transformed=sc.transform(df[[c]].values); df[c]=transformed.reshape(-1)
                    except Exception:
                        try: df[c]=sc.fit_transform(df[[c]].values).reshape(-1)
                        except Exception: df[c]=StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                    self.col_config[c]['scaler_obj']=sc

                # categorical encodings; never touch target
                for c in [cc for cc in self.cat_cols if cc in df.columns and cc != target_col]:
                    cfg=self.col_config[c]; s=df[c].astype(object)
                    rare_thresh=cfg.get('rare_threshold_frac',0.01)
                    if rare_thresh<1.0: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thresh, threshold_count=None)
                    else: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thresh))
                    df[c]=collapsed; cfg['rare_values']=list(rare_set)
                    if cfg_global.get('enable_string_similarity', False) and cfg['n_unique']>20:
                        grouped, mapping = string_similarity_group(df[c], score_threshold=90); df[c]=grouped; cfg['string_similarity_map']=mapping
                    enc_type=cfg.get('encoder_type')
                    if enc_type=='onehot':
                        ohe=OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                        resh=df[[c]].astype(str); ohe.fit(resh); arr=ohe.transform(resh)
                        cols=[f"{c}__{cat}" for cat in ohe.categories_[0]]
                        df_ohe=pd.DataFrame(arr, columns=cols, index=df.index)
                        df=pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                        cfg['encoder_obj']=ohe; cfg['ohe_columns']=cols
                    elif enc_type=='ordinal':
                        ord_enc=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                        resh=df[[c]].astype(object)
                        try: ord_enc.fit(resh); df[c]=ord_enc.transform(resh).astype(float); cfg['encoder_obj']=ord_enc
                        except Exception: df[c]=df[c].astype('category').cat.codes.astype(float)
                    elif enc_type=='count':
                        cnt_map=cfg.get('count_map', df[c].value_counts().to_dict())
                        df[c]=df[c].map(lambda x: cnt_map.get(x,0))
                    elif enc_type=='target':
                        mapping=cfg.get('target_mapping',{}); global_mean=cfg.get('target_global_mean',0.0)
                        df[c]=df[c].map(lambda x: mapping.get(x, global_mean))
                    else:
                        df[c]=df[c].astype('category').cat.codes.replace({-1:np.nan}).astype(float)

                for c in list(df.columns):
                    if df[c].dtype=='object':
                        try: df[c]=pd.to_numeric(df[c], errors='coerce')
                        except Exception: pass
                df.replace([np.inf,-np.inf], np.nan, inplace=True)
                return df

            def save(self, path): 
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
                    
            @staticmethod
            def load(path): 
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)

        class FeatureSelector:
            _VAR_THRESH = 1e-5
            _CORR_THRESH = 0.95
            _RELEVANCE = "mi"
            _PI_REPEATS = 5
            _RANDOM_STATE = 42
        
            def __init__(self, task):
                self.task = str(task).lower()
                self.selected_features = []
                self._filter_keep_ = []
                self._relevance_keep_ = []
                self._mi_scores_ = pd.Series(dtype=float)
                self._pi_importance_ = pd.Series(dtype=float)
        
            def _auto_k(self, p: int) -> int:
                return max(5, min(int(math.ceil(math.sqrt(p) * 2.0)), p))
        
            @staticmethod
            def _nzv_keep(X: pd.DataFrame, threshold: float) -> list:
                if X.shape[1] == 0:
                    return []
                vt = VarianceThreshold(threshold=threshold)
                vt.fit(X.values)
                mask = vt.get_support()
                return list(X.columns[mask])
        
            @staticmethod
            def _corr_prune_keep(X: pd.DataFrame, corr_thresh: float) -> list:
                cols = list(X.columns)
                if len(cols) <= 1:
                    return cols
                corr = X.corr(method="spearman").abs()
                keep = set(cols)
                while True:
                    sub = corr.loc[list(keep), list(keep)]
                    np.fill_diagonal(sub.values, 0.0)
                    max_val = sub.values.max()
                    if not np.isfinite(max_val) or max_val < corr_thresh:
                        break
                    idx = np.unravel_index(np.argmax(sub.values), sub.shape)
                    a = sub.index[idx[0]]
                    b = sub.columns[idx[1]]
                    mean_a = sub[a].mean()
                    mean_b = sub[b].mean()
                    drop = a if mean_a >= mean_b else b
                    keep.remove(drop)
                return list(keep)
        
            def _mi_scores(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:
                cols = list(X.columns)
                if self.task == "classification":
                    scores = mutual_info_classif(X.values, y, random_state=self._RANDOM_STATE)
                else:
                    y_num = pd.to_numeric(y, errors="ignore")
                    scores = mutual_info_regression(X.values, y_num, random_state=self._RANDOM_STATE)
                return pd.Series(scores, index=cols).fillna(0.0)
        
            @staticmethod
            def _topk_by_series(scores: pd.Series, k: int) -> list:
                k = max(1, min(k, scores.shape[0]))
                return list(scores.sort_values(ascending=False).head(k).index)
        
            @staticmethod
            def _mrmr_greedy(X: pd.DataFrame, mi: pd.Series, k: int) -> list:
                k = max(1, min(k, X.shape[1]))
                corr = X.corr(method="spearman").abs().fillna(0.0)
                selected = []
                candidates = list(X.columns)
                while len(selected) < k and candidates:
                    best_feat = None
                    best_score = -1e18
                    for c in candidates:
                        rel = float(mi.get(c, 0.0))
                        if not selected:
                            score = rel
                        else:
                            red = float(corr.loc[c, selected].mean())
                            score = rel - red
                        if score > best_score:
                            best_score = score
                            best_feat = c
                    selected.append(best_feat)
                    candidates.remove(best_feat)
                return selected
        
            def _permutation_keep(self, X: pd.DataFrame, y: pd.Series) -> tuple:
                if self.task == "classification":
                    est = ExtraTreesClassifier(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                else:
                    est = ExtraTreesRegressor(
                        n_estimators=400, max_features="sqrt", random_state=self._RANDOM_STATE, n_jobs=-1
                    )
                est.fit(X, y)
                pi = permutation_importance(
                    est, X, y, n_repeats=self._PI_REPEATS, random_state=self._RANDOM_STATE, n_jobs=-1
                )
                imp = pd.Series(pi.importances_mean, index=X.columns).fillna(0.0)
                keep = list(imp[imp > 0.0].index)
                if not keep:
                    keep = [imp.sort_values(ascending=False).index[0]]
                return keep, imp
        
            def fit(self, X: pd.DataFrame, y) -> "FeatureSelector":
                if isinstance(y, (pd.DataFrame, pd.Series)):
                    y_arr = pd.Series(y).values.ravel()
                else:
                    y_arr = np.asarray(y).ravel()
        
                keep1 = self._nzv_keep(X, self._VAR_THRESH)
                X1 = X[keep1]
                keep2 = self._corr_prune_keep(X1, self._CORR_THRESH)
                Xf = X1[keep2]
                self._filter_keep_ = list(Xf.columns)
        
                mi = self._mi_scores(Xf, y_arr)
                self._mi_scores_ = mi.copy()
                k = self._auto_k(Xf.shape[1])
                if self._RELEVANCE == "mrmr":
                    keep_rel = self._mrmr_greedy(Xf, mi, k)
                else:
                    keep_rel = self._topk_by_series(mi, k)
                Xr = Xf[keep_rel]
                self._relevance_keep_ = list(Xr.columns)
        
                keep_pi, imp = self._permutation_keep(Xr, y_arr)
                Xb = Xr[keep_pi]
                self._pi_importance_ = imp
                self.selected_features = list(Xb.columns)
                return self
        
            def transform(self, X: pd.DataFrame) -> pd.DataFrame:
                return X.reindex(columns=self.selected_features, fill_value=0.0)
        
            def save(self, path: str) -> None:
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
        
            @staticmethod
            def load(path: str) -> "FeatureSelector":
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)

        def print_head(df, title, nrows=20):
            print("="*80)
            print(title)
            print("="*80)
            try: print(df.head(nrows).to_string())
            except Exception: print(df.head(nrows))

        def print_dtypes(df, title):
            print("="*80)
            print(title)
            print("="*80)
            try: 
                for c in df.columns:
                    print(f"{c}: {df[c].dtype}")
            except Exception:
                print(df.dtypes)

        def print_data_understanding(df, target_col, preview_rows=20):
            print("="*80)
            print("DATA UNDERSTANDING REPORT")
            print("="*80)
            print(f"1. DATASET SHAPE: {df.shape[0]} rows × {df.shape[1]} columns")
            
            print(f"2. COLUMN NAMES AND TYPES:")
            for col in df.columns:
                print(f"   - {col}: {df[col].dtype}")
            
            print(f"3. MISSING VALUES:")
            missing = df.isnull().sum()
            missing_pct = (missing / len(df) * 100).round(2)
            for col in df.columns:
                if missing[col] > 0:
                    print(f"   - {col}: {missing[col]} ({missing_pct[col]}%)")
            
            print(f"4. TARGET VARIABLE ANALYSIS: {target_col}")
            if target_col in df.columns:
                print(f"   - Type: {df[target_col].dtype}")
                print(f"   - Missing: {df[target_col].isnull().sum()} ({(df[target_col].isnull().sum()/len(df)*100):.2f}%)")
                print(f"   - Unique values: {df[target_col].nunique()}")
                print(f"  VALUE COUNTS:")
                vc = df[target_col].value_counts().head(20)
                for val, count in vc.items():
                    pct = (count / len(df) * 100)
                    print(f"      {val}: {count} ({pct:.2f}%)")
            
            print(f"5. NUMERIC COLUMNS SUMMARY:")
            num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if num_cols:
                for col in num_cols[:10]:  # Show first 10
                    print(f"   {col}:")
                    print(f"      Mean: {df[col].mean():.4f}")
                    print(f"      Std: {df[col].std():.4f}")
                    print(f"      Min: {df[col].min():.4f}")
                    print(f"      Max: {df[col].max():.4f}")
                    print(f"      Unique: {df[col].nunique()}")
            
            print(f"6. CATEGORICAL COLUMNS SUMMARY:")
            cat_cols = [c for c in df.columns if c not in num_cols]
            if cat_cols:
                for col in cat_cols[:10]:  # Show first 10
                    print(f"   {col}:")
                    print(f"      Unique values: {df[col].nunique()}")
                    print(f"      Most common: {df[col].value_counts().head(3).to_dict()}")
            
            print(f"7. DATA PREVIEW (First {preview_rows} rows):")
            print(df.head(preview_rows).to_string())
            print("="*80)

        parser=argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--model_type', type=str, default="classification")
        parser.add_argument('--enable_feature_selection', type=str, default="true")
        parser.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        parser.add_argument('--outlier_fold', type=float, default=1.5)
        parser.add_argument('--max_knn_rows', type=int, default=5000)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--rare_threshold', type=float, default=0.01)
        parser.add_argument('--enable_string_similarity', type=str, default="false")
        parser.add_argument('--missing_threshold', type=float, default=0.40)
        parser.add_argument('--unique_threshold', type=float, default=0.95)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args=parser.parse_args()

        try:
            in_path=args.in_file; target_col=args.target_column; model_type=args.model_type.strip().lower()
            enable_fs=str(args.enable_feature_selection).lower() in ("1","true","t","yes","y")
            outlier_strategy=args.outlier_strategy; outlier_fold=float(args.outlier_fold)
            max_knn_rows=int(args.max_knn_rows); detect_thresh=float(args.numeric_detection_threshold)
            rare_threshold=float(args.rare_threshold)
            enable_string_similarity=str(args.enable_string_similarity).lower() in ("1","true","t","yes","y")
            missing_threshold=float(args.missing_threshold)
            unique_threshold=float(args.unique_threshold)
            preview_rows=int(args.preview_rows)
            out_X=args.train_X; out_y=args.train_y; preproc_path=args.preprocessor; fs_path=args.feature_selector; meta_path=args.preprocess_metadata

            if not os.path.exists(in_path):
                print("ERROR input file does not exist: " + str(in_path), file=sys.stderr); sys.exit(1)
            print("Loading: " + in_path)
            df = read_with_pandas(in_path)
            print(f"Loaded shape: {df.shape}")
            print(f"All columns: {list(df.columns)}")

            if target_col not in df.columns:
                print("ERROR target column " + str(target_col) + " not found", file=sys.stderr); sys.exit(1)

            # Print comprehensive data understanding
            print_data_understanding(df, target_col, preview_rows)

            # Drop duplicates
            before=len(df); df_hashable=make_hashable_for_dupes(df); df = df_hashable.drop_duplicates(keep='first')
            dropped_dupes = before - len(df)
            print(f"Removed duplicates: {dropped_dupes} rows dropped")

            # Pull target aside first
            y_raw = df[target_col].copy()
            X_raw = df.drop(columns=[target_col]).copy()

            # Drop columns with high missing percentage (excluding target)
            missing_report = {}
            cols_to_drop_missing = []
            for col in X_raw.columns:
                missing_pct = X_raw[col].isnull().sum() / len(X_raw)
                missing_report[col] = float(missing_pct)
                if missing_pct > missing_threshold:
                    cols_to_drop_missing.append(col)
            
            if cols_to_drop_missing:
                print(f"Dropping {len(cols_to_drop_missing)} columns with >{missing_threshold*100}% missing values:")
                for col in cols_to_drop_missing:
                    print(f"   - {col}: {missing_report[col]*100:.2f}% missing")
                X_raw = X_raw.drop(columns=cols_to_drop_missing)

            # Drop unique identifier columns (high cardinality)
            unique_report = {}
            cols_to_drop_unique = []
            for col in X_raw.columns:
                unique_pct = X_raw[col].nunique() / len(X_raw)
                unique_report[col] = float(unique_pct)
                if unique_pct > unique_threshold:
                    cols_to_drop_unique.append(col)
            
            if cols_to_drop_unique:
                print(f"Dropping {len(cols_to_drop_unique)} ID-like columns with >{unique_threshold*100}% unique values:")
                for col in cols_to_drop_unique:
                    print(f"   - {col}: {unique_report[col]*100:.2f}% unique")
                X_raw = X_raw.drop(columns=cols_to_drop_unique)

            print(f"Remaining features after filtering: {X_raw.shape[1]} columns")

            # Convert object columns in X only
            print("Converting object columns in X to numeric where possible")
            X_conv, conv_report = convert_object_columns_advanced(X_raw, detect_threshold=detect_thresh, decimal_comma=False, advanced=True)
            print(f"Conversion report: {len(conv_report.get('converted', []))} columns converted")

            # Detect dates in X only
            print("Detecting date-like columns in X")
            X_dates, date_report = detect_and_extract_dates(X_conv, enabled=True, exclude_cols=None)
            print(f"Date detection report: {len(date_report.get('date_columns', []))} date columns found")

            # Build config
            config={
                'outlier_strategy': outlier_strategy,
                'outlier_fold': outlier_fold,
                'max_knn_rows': max_knn_rows,
                'numeric_detection_threshold': detect_thresh,
                'rare_threshold': rare_threshold,
                'enable_string_similarity': enable_string_similarity,
                'missing_threshold': missing_threshold,
                'unique_threshold': unique_threshold
            }

            # Prepare y according to task
            y_info={'strategy': None, 'classes_': None}
            if model_type=="regression":
                y_series=pd.to_numeric(y_raw, errors='coerce')
                y_info['strategy']="numeric_coerce"
            else:
                # try numeric rounding half-up
                y_num=pd.to_numeric(y_raw, errors='coerce')
                if y_num.notna().mean() > 0.8:
                    y_series = np.floor(y_num + 0.5).astype("Int64")
                    y_info['strategy']="round_half_up"
                else:
                    # string labels -> label encode
                    le=LabelEncoder()
                    y_series = pd.Series(le.fit_transform(y_raw.astype(str)), index=y_raw.index)
                    y_info['strategy']="label_encode"
                    y_info['classes_']=list(map(str, le.classes_))

            # Drop rows with null y
            keep_mask_y = y_series.notna()
            dropped_y = int((~keep_mask_y).sum())
            if dropped_y>0:
                print(f"Dropped rows with null target: {dropped_y}")
            X_input = X_dates.loc[keep_mask_y].reset_index(drop=True)
            y_series = y_series.loc[keep_mask_y].reset_index(drop=True)

            print(f"Final dataset before preprocessing: {X_input.shape}")
            print_head(X_input, "X head before Preprocessor.fit", preview_rows)
            print_dtypes(X_input, "X dtypes before Preprocessor.fit")

            # Fit preprocessor on X only, pass y for encoders
            print("Fitting Preprocessor...")
            pre=Preprocessor()
            pre.fit(X_input, y=y_series if model_type=="classification" else y_series, config=config, target_col=target_col, model_type=model_type)

            print(f"Numeric columns identified: {len(pre.num_cols)}")
            print(f"Categorical columns identified: {len(pre.cat_cols)}")

            # Transform X with training_mode True to apply outlier rule
            print("Transforming data...")
            Xp = pre.transform(X_input, training_mode=True)

            # Align y to kept_mask if any rows were dropped by outlier removal
            kmask = pre.global_metadata.get('kept_mask_after_outlier', None)
            if kmask is not None and isinstance(kmask, pd.Series):
                kmask = kmask.reindex(X_input.index, fill_value=True)
                Xp = Xp.reset_index(drop=True)
                y_series = y_series.loc[kmask].reset_index(drop=True)
                print(f"Rows after outlier handling: {len(Xp)}")

            # Ensure numeric dtypes optimal
            print("Optimizing numeric dtypes...")
            for col in Xp.select_dtypes(include=[np.number]).columns:
                try:
                    Xp[col]=pd.to_numeric(Xp[col], downcast='float')
                except Exception:
                    pass

            # Feature selection (optional)
            if enable_fs:
                print("Performing feature selection...")
                fs=FeatureSelector(task=("classification" if model_type=="classification" else "regression"))
                fs.fit(Xp, y_series.values)
                Xs = fs.transform(Xp)
                print(f"Selected {len(fs.selected_features)} features from {Xp.shape[1]}")
                print(f"Selected features: {fs.selected_features}")
            else:
                print("Feature selection disabled, using all features")
                Xs = Xp.copy()
                fs = None

            # Save outputs
            ensure_dir_for(out_X); ensure_dir_for(out_y); ensure_dir_for(preproc_path); ensure_dir_for(fs_path); ensure_dir_for(meta_path)
            
            Xs.to_parquet(out_X, index=False)
            y_df=pd.DataFrame({target_col: y_series})
            y_df.to_parquet(out_y, index=False)
            
            with gzip.open(preproc_path, "wb") as f:
                cloudpickle.dump(pre, f)
            
            if fs is not None:
                with gzip.open(fs_path, "wb") as f:
                    cloudpickle.dump(fs, f)
            else:
                # Save a dummy selector if feature selection was disabled
                dummy_fs = type('DummySelector', (), {'selected_features': list(Xs.columns), 'transform': lambda self, X: X})()
                with gzip.open(fs_path, "wb") as f:
                    cloudpickle.dump(dummy_fs, f)

            metadata={
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'model_type': model_type,
                'target_column': target_col,
                'enable_feature_selection': enable_fs,
                'y_strategy': y_info,
                'config': config,
                'dropped_duplicates': dropped_dupes,
                'dropped_missing_cols': cols_to_drop_missing,
                'dropped_unique_cols': cols_to_drop_unique,
                'missing_report': missing_report,
                'unique_report': unique_report,
                'conversion_report': conv_report,
                'date_report': date_report,
                'num_cols': pre.global_metadata.get('num_cols', []),
                'cat_cols': pre.global_metadata.get('cat_cols', []),
                'selected_features': fs.selected_features if fs else list(Xs.columns),
                'final_shape': {'rows': Xs.shape[0], 'cols': Xs.shape[1]}
            }
            with open(meta_path,'w',encoding='utf-8') as fh:
                json.dump(metadata, fh, indent=2, ensure_ascii=False)

            # Final prints
            print("="*80)
            print("PREPROCESSING SUMMARY")
            print("="*80)
            print(f"Input shape: {df.shape}")
            print(f"Final X shape: {Xs.shape}")
            print(f"Final y shape: {y_df.shape}")
            print(f"Duplicates removed: {dropped_dupes}")
            print(f"Columns dropped (missing): {len(cols_to_drop_missing)}")
            print(f"Columns dropped (unique ID): {len(cols_to_drop_unique)}")
            print(f"Feature selection enabled: {enable_fs}")
            if enable_fs and fs:
                print(f"Features selected: {len(fs.selected_features)} / {Xp.shape[1]}")
            print(f"Final columns in X: {list(Xs.columns)}")
            print_dtypes(Xs, "Final X dtypes")
            print(f"Saved train_X to: {out_X}")
            print(f"Saved train_y to: {out_y}")
            print(f"Saved preprocessor to: {preproc_path}")
            print(f"Saved feature_selector to: {fs_path}")
            print(f"Saved metadata to: {meta_path}")
            print("="*80)
            print("SUCCESS: Preprocessing and train split complete")
            print("="*80)
        except Exception as exc:
            print("ERROR during preprocessing: " + str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --enable_feature_selection
      - {inputValue: enable_feature_selection}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --missing_threshold
      - {inputValue: missing_threshold}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --preview_rows
      - {inputValue: preview_rows}
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
