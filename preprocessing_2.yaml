name: Advanced Preprocess v7.2
description: |
  End-to-end robust preprocessing and feature selection component.
  - Loads CSV/TSV/JSON/JSONL/Excel/Parquet/Feather/ORC.
  - Drops duplicates safely even with dict/list columns.
  - Drops known non-modeling columns: piMetadata, execution_timestamp, pipelineid, component_id, projectid (if present).
  - Converts object numerics/percents/currencies and k/m/b suffixes to float.
  - Detects dates, extracts calendar features.
  - Encodes categoricals with OneHot/Ordinal/Target/Count heuristics.
  - Scales numerics with per-column auto Standard/Robust/MinMax or Power+Standard.
  - Optional string-similarity grouping for high-cardinality categoricals.
  - Optional outlier handling (IQR drop or winsorize) only in training mode.
  - Text columns: add cheap stats and, when suitable, TF-IDF + TruncatedSVD features per column.
  - Saves fitted Preprocessor and FeatureSelector for reuse.
  - Feature selection: Mutual Information (auto-k) + Model-based selector (ExtraTrees) cascaded.
  - Works for classification or regression; auto label-encodes y for classification.
inputs:
  - {name: in_file, type: Data, description: "Path to the dataset file or directory"}
  - {name: target_column, type: String, description: "Name of the target column"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: outlier_strategy, type: String, description: "drop_iqr | winsorize | none (applies only during fit)", optional: true, default: "drop_iqr"}
  - {name: outlier_fold, type: Float, description: "IQR fold for outlier rule", optional: true, default: "1.5"}
  - {name: max_knn_rows, type: Integer, description: "Max rows for KNN imputer else fallback", optional: true, default: "5000"}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: rare_threshold, type: Float, description: "Rare label threshold as fraction or absolute count", optional: true, default: "0.01"}
  - {name: enable_string_similarity, type: String, description: "true/false high-cardinality grouping", optional: true, default: "false"}
  - {name: tfidf_max_features, type: Integer, description: "Max vocabulary per text column", optional: true, default: "20000"}
  - {name: svd_dims, type: Integer, description: "SVD dims per text column when applied", optional: true, default: "30"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: "20"}
outputs:
  - {name: X, type: Data, description: "Processed features parquet"}
  - {name: y, type: Data, description: "Target parquet"}
  - {name: preprocessor, type: Data, description: "joblib dump of fitted Preprocessor"}
  - {name: feature_selector, type: Data, description: "joblib dump of fitted FeatureSelector"}
  - {name: preprocess_metadata, type: Data, description: "JSON metadata"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, re, io, gzip, zipfile, math
        from datetime import datetime
        from collections import defaultdict
        def pip_install(pkgs):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input"] + pkgs)
        try:
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz
        except Exception:
            pip_install(["pandas","numpy","scikit-learn","joblib","category_encoders","rapidfuzz","pyarrow","fastparquet","openpyxl"])
            import pandas as pd, numpy as np
            from sklearn.impute import SimpleImputer, KNNImputer
            from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder
            from sklearn.pipeline import Pipeline
            from sklearn.model_selection import KFold
            from sklearn.experimental import enable_iterative_imputer  # noqa
            from sklearn.impute import IterativeImputer
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression, SelectFromModel
            from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
            import joblib
            import category_encoders as ce
            from rapidfuzz import process as rf_process, fuzz as rf_fuzz

        # ---------------- IO helpers ----------------
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def sample_file_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh: return fh.read(n)
            except Exception: return b""

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            ob=chr(123); obk=chr(91); nl=chr(10)
            if txt[0] in (ob,obk): return True
            if (nl+ob) in txt or (nl+obk) in txt: return True
            return False

        def read_with_pandas(path):
            if os.path.isdir(path):
                entries=[os.path.join(path,f) for f in os.listdir(path) if not f.startswith(".")]
                files=[p for p in entries if os.path.isfile(p)]
                if not files: raise ValueError("No files in dir: "+path)
                path = max(files, key=lambda p: os.path.getsize(p))
                print(f"Info selected file: {path}")
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: "+path)
            ext=os.path.splitext(path)[1].lower()
            if ext==".gz" or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                try:
                    with gzip.open(path,"rt",encoding="utf-8",errors="ignore") as fh:
                        sample=fh.read(8192); fh.seek(0)
                        if is_likely_json(sample.encode() if isinstance(sample,str) else sample):
                            fh.seek(0)
                            try: return pd.read_json(fh, lines=True)
                            except Exception: fh.seek(0); return pd.read_csv(fh)
                        fh.seek(0); return pd.read_csv(fh)
                except Exception: pass
            if ext==".zip":
                with zipfile.ZipFile(path,"r") as z:
                    members=[n for n in z.namelist() if not n.endswith("/")]
                    member=max(members, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample=fh.read(8192)
                        if is_likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2,encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2,encoding="utf-8"))
            try:
                if ext==".csv": return pd.read_csv(path)
                if ext in (".tsv",".tab"): return pd.read_csv(path, sep="\t")
                if ext in (".json",".ndjson",".jsonl"):
                    try: return pd.read_json(path, lines=True)
                    except ValueError: return pd.read_json(path)
                if ext in (".xls",".xlsx"): return pd.read_excel(path)
                if ext in (".parquet",".pq"): return pd.read_parquet(path, engine="auto")
                if ext==".feather": return pd.read_feather(path)
                if ext==".orc": return pd.read_orc(path)
            except Exception: pass
            try: return pd.read_parquet(path, engine="auto")
            except Exception: pass
            sample=sample_file_bytes(path)
            if is_likely_json(sample):
                try: return pd.read_json(path, lines=True)
                except Exception: pass
            try: return pd.read_csv(path)
            except Exception: pass
            raise ValueError("Unsupported format: "+path)

        # ------------- Safe duplicate removal -------------
        def make_hashable_for_dupes(df):
            df = df.copy()
            for c in df.columns:
                try:
                    if df[c].apply(lambda v: isinstance(v,(list,dict,set))).any():
                        df[c] = df[c].map(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(list,dict,set)) else v)
                except Exception:
                    df[c] = df[c].astype(str)
            return df

        # -------------- Numeric parsing -------------------
        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',','.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult'); unit=(m.group('unit') or '').lower()
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception:
                    try: val=float(number_clean.replace(',', '.'))
                    except Exception: return np.nan
                if mult: val*=MULTIPLIER_MAP.get(mult.lower(),1.0)
                if unit and '%' in unit: val = val/100.0
                return val
            num_search=re.search(r'[-+]?\d+([.,]\d+)?', tmp)
            if num_search:
                try: return float(num_search.group(0).replace(',',''))
                except Exception: return np.nan
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, decimal_comma=False, advanced=True):
            df=df.copy(); report={'converted':[],'skipped':[]}
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: report['skipped'].append(col); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x, decimal_comma=decimal_comma)) if advanced else pd.to_numeric(ser, errors='coerce')
                frac = parsed.notna().sum()/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed; report['converted'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        # -------------- Dates ------------------------------
        def detect_and_extract_dates(df, enabled=True):
            df=df.copy(); report={'date_columns':[],'skipped':[]}
            if not enabled: return df, report
            cand=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in cand:
                ser=df[col].astype(object); sample=ser.dropna().astype(str).head(500)
                if sample.empty: report['skipped'].append({'col':col,'parsable_fraction':0.0}); continue
                parsed=pd.to_datetime(sample, errors='coerce', utc=False)
                frac=parsed.notna().mean()
                if frac>=0.6:
                    full=pd.to_datetime(ser, errors='coerce', utc=False)
                    df[col+"_orig"]=df[col]; df[col]=full
                    df[col+"_year"]=df[col].dt.year
                    df[col+"_month"]=df[col].dt.month
                    df[col+"_day"]=df[col].dt.day
                    df[col+"_dayofweek"]=df[col].dt.dayofweek
                    df[col+"_is_weekend"]=df[col].dt.dayofweek.isin([5,6]).astype('Int64')
                    df[col+"_is_month_start"]=df[col].dt.is_month_start.astype('Int64')
                    df[col+"_is_month_end"]=df[col].dt.is_month_end.astype('Int64')
                    df[col+"_days_since_epoch"]=(df[col]-pd.Timestamp("1970-01-01"))//pd.Timedelta('1D')
                    report['date_columns'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        # -------------- Rare labels and similarity --------
        def collapse_rare_labels(series, threshold_frac=0.01, threshold_count=None):
            counts=series.value_counts(dropna=False); n=len(series)
            rare = set(counts[counts <= threshold_count].index) if threshold_count is not None else set(counts[counts <= max(1,int(threshold_frac*n))].index)
            return series.map(lambda x: "__RARE__" if x in rare else x), rare

        def string_similarity_group(series, score_threshold=90):
            vals=[v for v in pd.Series(series.dropna().unique()).astype(str)]
            mapping={}; used=set()
            for v in vals:
                if v in used: continue
                matches=rf_process.extract(v, vals, scorer=rf_fuzz.token_sort_ratio, score_cutoff=90)
                group=[m[0] for m in matches]
                for g in group: mapping[g]=v; used.add(g)
            return pd.Series(series).astype(object).map(lambda x: mapping.get(str(x), x)), mapping

        # -------------- Outliers --------------------------
        def drop_outliers_iqr(df, numeric_cols, fold=1.5):
            if not numeric_cols:
                return df, 0
            # numeric snapshot to avoid object/string issues
            num = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            q1 = num.quantile(0.25)
            q3 = num.quantile(0.75)
            iqr = q3 - q1
            lower = (q1 - fold * iqr).to_dict()
            upper = (q3 + fold * iqr).to_dict()
        
            n = len(df)
            mask = np.ones(n, dtype=bool)
            for c in numeric_cols:
                if c not in num.columns:
                    continue
                col = num[c]
                lo = lower.get(c, None); hi = upper.get(c, None)
                cond = pd.Series(True, index=df.index)
                if lo is not None:
                    cond &= col.ge(lo)
                if hi is not None:
                    cond &= col.le(hi)
                mask &= cond.fillna(True).to_numpy()
        
            keep_idx = np.where(mask)[0]
            df2 = df.iloc[keep_idx].reset_index(drop=True)
            return df2, n - len(df2)
        def winsorize_df(df, numeric_cols, fold=1.5):
            if not numeric_cols:
                return df, 0
            num = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            q1 = num.quantile(0.25)
            q3 = num.quantile(0.75)
            iqr = q3 - q1
            lower = (q1 - fold * iqr).to_dict()
            upper = (q3 + fold * iqr).to_dict()
        
            df2 = df.copy()
            for c in numeric_cols:
                if c not in num.columns:
                    continue
                lo = lower.get(c, None); hi = upper.get(c, None)
                col = pd.to_numeric(df2[c], errors='coerce')
                df2[c] = col.clip(lower=lo, upper=hi)
            return df2, 0

        # -------------- Scalers ---------------------------
        def choose_scaler_for_series(s):
            v=s.dropna()
            if v.empty: return StandardScaler()
            if v.std(ddof=0)==0: return StandardScaler()
            skew=float(v.skew()); kurt=float(v.kurtosis()) if len(v)>3 else 0.0
            extreme_frac=float(((v - v.mean()).abs() > 3*v.std(ddof=0)).mean())
            if (v.min()>=0.0 and v.max()<=1.0): return MinMaxScaler()
            if abs(skew)>=1.0: return Pipeline([('power',PowerTransformer(method='yeo-johnson')),('std',StandardScaler())])
            if extreme_frac>0.01 or abs(kurt)>10: return RobustScaler()
            return StandardScaler()

        # -------------- Encoder choice --------------------
        def choose_categorical_encoder(col, series, target_series=None, train_mode=True):
            n=len(series); nunique=series.nunique(dropna=True); high_card=(nunique>max(50,0.05*n))
            if high_card:
                if target_series is not None and train_mode:
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    return 'count', None, {}
            else:
                if nunique<=10:
                    enc=OneHotEncoder(sparse_output=False, handle_unknown='ignore'); return 'onehot', enc, {}
                if target_series is not None and train_mode:
                    enc=ce.TargetEncoder(cols=[col], smoothing=0.3); return 'target', enc, {}
                else:
                    enc=OrdinalEncoder(); return 'ordinal', enc, {}

        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=0.3, random_state=42):
            X_col=df[col].astype(object).fillna('__NA__'); global_mean=y.mean()
            oof=pd.Series(index=df.index, dtype=float)
            kf=KFold(n_splits=min(n_splits,max(2,len(df)//10)), shuffle=True, random_state=random_state)
            for tr_idx, val_idx in kf.split(df):
                means=y.iloc[tr_idx].groupby(X_col.iloc[tr_idx]).mean()
                oof.iloc[val_idx]=X_col.iloc[val_idx].map(lambda v: means.get(v, global_mean))
            counts=X_col.value_counts(); smooth_map={}
            for cat,cnt in counts.items():
                cat_mean=y[X_col==cat].mean() if cnt>0 else global_mean
                alpha=cnt/(cnt+smoothing); smooth_map[cat]=alpha*cat_mean+(1-alpha)*global_mean
            return oof.fillna(global_mean), smooth_map, float(global_mean)

        # -------------- Text Featurizer -------------------
        def _should_textify(ser, min_words=3, min_len=10):
            s = ser.astype(str).fillna("")
            word_cnt_med = s.str.count(r"\\b\\w+\\b").median()
            len_med = s.str.len().median()
            numeric_frac = pd.to_numeric(s, errors="coerce").notna().mean()
            uniq_ratio = ser.nunique(dropna=True) / max(len(ser), 1)
            alpha_len = s.str.replace(r"[^A-Za-z]", "", regex=True).str.len()
            alpha_ratio_med = (alpha_len / s.str.len().replace(0, 1)).median()
            tokens = s.str.findall(r"\\b\\w+\\b")
            long_token_cnt_med = tokens.map(lambda ts: sum(len(t) >= 10 for t in ts)).median()
            cond_words_or_len = (word_cnt_med >= min_words) or (len_med >= min_len) or (long_token_cnt_med >= 2)
            cond_alpha = alpha_ratio_med >= 0.5
            cond_not_numeric = numeric_frac < 0.4
            cond_not_id = uniq_ratio < 0.95
            return bool(cond_words_or_len and cond_alpha and cond_not_numeric and cond_not_id)

        class TextFeaturizer:
            def __init__(self, text_cols=None, max_tfidf_features=20000, svd_dims=30):
                self.text_cols = text_cols
                self.max_tfidf_features = int(max_tfidf_features)
                self.svd_dims = int(svd_dims)
                self.vectorizers = {}
                self.svds = {}
                self.stats_cols = {}
                self.fitted_cols = []

            def fit(self, X):
                if self.text_cols is None or len(self.text_cols)==0:
                    self.text_cols = [c for c in X.columns if (X[c].dtype=='object' or pd.api.types.is_string_dtype(X[c])) and _should_textify(X[c])]
                for col in self.text_cols:
                    s = X[col].astype(str).fillna("")
                    vec = TfidfVectorizer(max_features=self.max_tfidf_features, ngram_range=(1,2), min_df=2)
                    try:
                        M = vec.fit_transform(s.values)
                    except Exception:
                        continue
                    k = min(self.svd_dims, max(2, min(M.shape)-1)) if min(M.shape)>2 else 2
                    svd = TruncatedSVD(n_components=k, random_state=42)
                    Z = svd.fit_transform(M)
                    self.vectorizers[col] = vec
                    self.svds[col] = svd
                    self.stats_cols[col] = [f"{col}__len", f"{col}__word_count", f"{col}__digit_count", f"{col}__alpha_ratio"]
                    self.fitted_cols.append(col)
                return self

            def transform(self, X):
                X = X.copy()
                out = []
                for col in self.fitted_cols:
                    s = X[col].astype(str).fillna("")
                    vec = self.vectorizers[col]; svd = self.svds[col]
                    try:
                        M = vec.transform(s.values)
                        Z = svd.transform(M)
                        Z_df = pd.DataFrame(Z, index=X.index, columns=[f"{col}__svd{i+1}" for i in range(Z.shape[1])])
                    except Exception:
                        Z_df = pd.DataFrame(index=X.index)
                    stats = pd.DataFrame({
                        f"{col}__len": s.str.len(),
                        f"{col}__word_count": s.str.count(r"\\b\\w+\\b"),
                        f"{col}__digit_count": s.str.count(r"\\d"),
                        f"{col}__alpha_ratio": s.str.replace(r"[^A-Za-z]", "", regex=True).str.len() / s.str.len().replace(0,1)
                    }, index=X.index)
                    out.append(pd.concat([Z_df, stats], axis=1))
                if out:
                    feat = pd.concat(out, axis=1)
                    X = pd.concat([X.drop(columns=[c for c in self.fitted_cols if c in X.columns]), feat], axis=1)
                return X

        # -------------- Preprocessor ----------------------
        class Preprocessor:
            def __init__(self):
                self.num_cols=[]; self.cat_cols=[]; self.col_config={}; self.global_metadata={}
                self.text = None

            def fit(self, df, y=None, config=None, text_params=None):
                self.global_metadata['config']=config or {}
                self.global_metadata['text_params']=text_params or {}
                nrows=len(df)
                self.num_cols=df.select_dtypes(include=[np.number]).columns.tolist()
                self.cat_cols=[c for c in df.columns if c not in self.num_cols]
                self.text = TextFeaturizer(
                    text_cols=None,
                    max_tfidf_features=int(self.global_metadata['text_params'].get('tfidf_max_features', 20000)),
                    svd_dims=int(self.global_metadata['text_params'].get('svd_dims', 30))
                ).fit(df)
                df_txt = self.text.transform(df)

                self.num_cols = [c for c in df_txt.columns if pd.api.types.is_numeric_dtype(df_txt[c])]
                self.cat_cols = [c for c in df_txt.columns if c not in self.num_cols]

                for c in self.num_cols:
                    s=df_txt[c]; cfg={}
                    missing_frac=s.isna().mean(); cfg['missing_frac']=float(missing_frac)
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['skew']=float(s.dropna().skew()) if s.dropna().shape[0]>2 else 0.0
                    cfg['kurtosis']=float(s.dropna().kurtosis()) if s.dropna().shape[0]>3 else 0.0
                    if missing_frac==0: cfg['imputer']=('none',None)
                    elif missing_frac<0.02:
                        imp=SimpleImputer(strategy='median'); imp.fit(np.array(s).reshape(-1,1)); cfg['imputer']=('simple_median',imp)
                    else:
                        if nrows<=self.global_metadata['config'].get('max_knn_rows',5000) and missing_frac<0.25:
                            cfg['imputer']=('knn',{'n_neighbors':5})
                        else:
                            cfg['imputer']=('iterative',{'max_iter':10,'random_state':0})
                    cfg['scaler_choice']='auto'
                    self.col_config[c]=cfg
                for c in self.cat_cols:
                    s=df_txt[c].astype(object); cfg={}
                    cfg['n_unique']=int(s.nunique(dropna=True))
                    cfg['missing_frac']=float(s.isna().mean())
                    cfg['high_card']=cfg['n_unique']>max(50,0.05*nrows)
                    cfg['rare_threshold_frac']=self.global_metadata['config'].get('rare_threshold',0.01)
                    enc_name,enc_obj,_=choose_categorical_encoder(c,s,(y if y is not None else None),train_mode=(y is not None))
                    cfg['encoder_type']=enc_name; cfg['encoder_obj']=enc_obj
                    self.col_config[c]=cfg
                if y is not None:
                    for c in self.cat_cols:
                        cfg=self.col_config[c]
                        if cfg['encoder_type']=='target':
                            _, mapping, global_mean = fit_target_encoder_kfold(df_txt, c, y, n_splits=5)
                            cfg['target_mapping']=mapping; cfg['target_global_mean']=float(global_mean)
                        elif cfg['encoder_type']=='count':
                            counts=df_txt[c].astype(object).value_counts().to_dict(); cfg['count_map']=counts
                self.global_metadata['num_cols']=self.num_cols; self.global_metadata['cat_cols']=self.cat_cols
                return self

            def transform(self, df, training_mode=False):
                df=df.copy()
                if self.text is not None: df = self.text.transform(df)
                temp_df=df.copy()
                for c in self.num_cols:
                    imputer_info=self.col_config.get(c,{}).get('imputer',('none',None))
                    if imputer_info[0] in ('knn','iterative'):
                        med=temp_df[c].median(); temp_df[c]=temp_df[c].fillna(med)
                scaler_objs={}
                for c in self.num_cols:
                    sample=temp_df[c]; scaler=choose_scaler_for_series(sample)
                    try: scaler.fit(sample.values.reshape(-1,1))
                    except Exception: scaler=StandardScaler(); scaler.fit(sample.values.reshape(-1,1))
                    scaler_objs[c]=scaler; self.col_config[c]['scaler_obj']=scaler
                num_matrix=temp_df[self.num_cols].copy()
                scaled_matrix=np.zeros_like(num_matrix.values, dtype=float)
                for i,c in enumerate(self.num_cols):
                    sc=scaler_objs[c]; col_vals=num_matrix[c].values.reshape(-1,1)
                    try: scaled_col=sc.transform(col_vals).reshape(-1)
                    except Exception: scaled_col=StandardScaler().fit_transform(col_vals).reshape(-1)
                    scaled_matrix[:,i]=scaled_col
                scaled_df=pd.DataFrame(scaled_matrix, columns=self.num_cols, index=num_matrix.index)
                need_knn=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='knn']
                if need_knn:
                    base_imp=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    knn_neighbors=base_imp.get('n_neighbors',5) if isinstance(base_imp,dict) else 5
                    knn=KNNImputer(n_neighbors=knn_neighbors)
                    imputed_scaled=knn.fit_transform(scaled_df)
                    imputed_scaled_df=pd.DataFrame(imputed_scaled, columns=self.num_cols, index=scaled_df.index)
                    for c in self.num_cols:
                        sc=scaler_objs[c]
                        try: inv=sc.inverse_transform(imputed_scaled_df[c].values.reshape(-1,1)).reshape(-1)
                        except Exception: inv=imputed_scaled_df[c].values.reshape(-1)
                        df[c]=inv
                need_iter=[c for c in self.num_cols if self.col_config[c]['imputer'][0]=='iterative']
                if need_iter:
                    iter_max_iter=10; base_imp2=self.col_config[self.num_cols[0]]['imputer'][1] if self.num_cols else None
                    if isinstance(base_imp2,dict): iter_max_iter=base_imp2.get('max_iter',10)
                    iter_imp=IterativeImputer(max_iter=iter_max_iter, random_state=0)
                    arr=df[self.num_cols].astype(float).replace([np.inf,-np.inf], np.nan).values
                    iter_out=iter_imp.fit_transform(arr)
                    df[self.num_cols]=pd.DataFrame(iter_out, columns=self.num_cols, index=df.index)
                    for c in need_iter: self.col_config[c]['imputer_obj']=iter_imp
                for c in self.num_cols:
                    cfg=self.col_config[c]
                    if cfg['imputer'][0]=='simple_median':
                        imp=cfg['imputer'][1]; df[c]=imp.transform(df[[c]])
                cfg_global=self.global_metadata.get('config') or {}
                if training_mode:
                    if cfg_global.get('outlier_strategy','drop_iqr')=='drop_iqr':
                        df, n_removed = drop_outliers_iqr(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                        self.global_metadata['outliers_removed']=int(n_removed)
                    elif cfg_global.get('outlier_strategy')=='winsorize':
                        df,_=winsorize_df(df, self.num_cols, fold=cfg_global.get('outlier_fold',1.5))
                for c in self.num_cols:
                    sc=self.col_config[c].get('scaler_obj')
                    if sc is None: sc=choose_scaler_for_series(df[c].fillna(df[c].median()))
                    try: transformed=sc.transform(df[[c]].values); df[c]=transformed.reshape(-1)
                    except Exception:
                        try: df[c]=sc.fit_transform(df[[c]].values).reshape(-1)
                        except Exception: df[c]=StandardScaler().fit_transform(df[[c]].fillna(df[c].median()).values).reshape(-1)
                    self.col_config[c]['scaler_obj']=sc
                for c in [cc for cc in self.cat_cols if cc in df.columns]:
                    cfg=self.col_config[c]; s=df[c].astype(object)
                    rare_thresh=cfg.get('rare_threshold_frac',0.01)
                    if rare_thresh<1.0: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=rare_thresh, threshold_count=None)
                    else: collapsed, rare_set = collapse_rare_labels(s, threshold_frac=0.01, threshold_count=int(rare_thresh))
                    df[c]=collapsed; cfg['rare_values']=list(rare_set)
                    if cfg_global.get('enable_string_similarity', False) and cfg['n_unique']>20:
                        grouped, mapping = string_similarity_group(df[c], score_threshold=90); df[c]=grouped; cfg['string_similarity_map']=mapping
                    enc_type=cfg.get('encoder_type')
                    if enc_type=='onehot':
                        ohe=OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                        resh=df[[c]].astype(str); ohe.fit(resh); arr=ohe.transform(resh)
                        cols=[f"{c}__{cat}" for cat in ohe.categories_[0]]
                        df_ohe=pd.DataFrame(arr, columns=cols, index=df.index)
                        df=pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                        cfg['encoder_obj']=ohe; cfg['ohe_columns']=cols
                    elif enc_type=='ordinal':
                        ord_enc=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                        resh=df[[c]].astype(object)
                        try: ord_enc.fit(resh); df[c]=ord_enc.transform(resh).astype(float); cfg['encoder_obj']=ord_enc
                        except Exception: df[c]=df[c].astype('category').cat.codes.astype(float)
                    elif enc_type=='count':
                        cnt_map=df[c].value_counts().to_dict(); df[c]=df[c].map(lambda x: cnt_map.get(x,0)); cfg['count_map']=cnt_map
                    elif enc_type=='target':
                        mapping=cfg.get('target_mapping',{}); global_mean=cfg.get('target_global_mean',0.0)
                        df[c]=df[c].map(lambda x: mapping.get(x, global_mean))
                    else:
                        df[c]=df[c].astype('category').cat.codes.replace({-1:np.nan}).astype(float)
                for c in list(df.columns):
                    if df[c].dtype=='object':
                        try: df[c]=pd.to_numeric(df[c], errors='coerce')
                        except Exception: pass
                df.replace([np.inf,-np.inf], np.nan, inplace=True)
                return df

            def save(self, path): joblib.dump(self, path)
            @staticmethod
            def load(path): return joblib.load(path)

        # -------------- Feature Selector ------------------
        class FeatureSelector:
            def __init__(self, task):
                self.task=task; self.mi_selector=None; self.model_selector=None; self.selected_features=[]

            def _auto_k(self, p):
                return max(5, min(int(round(math.sqrt(p)*2)), p))

            def fit(self, X, y):
                p=X.shape[1]; k=self._auto_k(p)
                if self.task=="classification":
                    self.mi_selector = SelectKBest(score_func=mutual_info_classif, k=k).fit(X, y)
                    model = ExtraTreesClassifier(n_estimators=600, max_features="sqrt", random_state=42, n_jobs=-1)
                else:
                    self.mi_selector = SelectKBest(score_func=mutual_info_regression, k=k).fit(X, y)
                    model = ExtraTreesRegressor(n_estimators=600, max_features="sqrt", random_state=42, n_jobs=-1)
                X1 = self.mi_selector.transform(X)
                self.model_selector = SelectFromModel(estimator=model, threshold="median", max_features=X1.shape[1]).fit(X1, y)
                mask1=self.mi_selector.get_support()
                mask2=self.model_selector.get_support()
                cols=np.array(X.columns)[mask1][mask2]
                self.selected_features=list(cols)
                return self

            def transform(self, X):
                X1=self.mi_selector.transform(X)
                X2=self.model_selector.transform(X1)
                return pd.DataFrame(X2, columns=self.selected_features, index=X.index)

            def save(self, path): joblib.dump(self, path)
            @staticmethod
            def load(path): return joblib.load(path)

        # -------------- Printing --------------------------
        def print_before(df, target_col):
            print("BEFORE PREPROCESSING df.head()")
            try: print(df.head().to_string())
            except Exception: print(df.head())
            print("BEFORE PREPROCESSING dtypes")
            print(df.dtypes.astype(str))
            print("BEFORE PREPROCESSING missing counts")
            print(df.isna().sum())

        def print_plan(pre):
            print("PLAN per-column strategies")
            print("Numeric columns:", pre.num_cols)
            print("Categorical columns:", pre.cat_cols)
            for c in pre.num_cols:
                cfg=pre.col_config.get(c,{}); imp=cfg.get('imputer',('none',None)); sc=cfg.get('scaler_obj')
                print(f"[NUM] {c} missing_frac={cfg.get('missing_frac')} imputer={imp[0]} scaler={sc.__class__.__name__ if sc else 'auto'}")
            for c in pre.cat_cols:
                cfg=pre.col_config.get(c,{})
                print(f"[CAT] {c} n_unique={cfg.get('n_unique')} high_card={cfg.get('high_card')} encoder={cfg.get('encoder_type')} rare_threshold={cfg.get('rare_threshold_frac')}")

        def print_after(X_processed, selected_cols=None):
            print("AFTER PREPROCESSING df.head()")
            try: print(X_processed.head().to_string())
            except Exception: print(X_processed.head())
            print("AFTER PREPROCESSING dtypes")
            print(X_processed.dtypes.astype(str))
            print("AFTER PREPROCESSING missing counts")
            print(X_processed.isna().sum())
            if selected_cols is not None:
                print("SELECTED FEATURES:", selected_cols)

        # -------------- Main ------------------------------
        parser=argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--model_type', type=str, default="classification")
        parser.add_argument('--outlier_strategy', type=str, default="drop_iqr")
        parser.add_argument('--outlier_fold', type=float, default=1.5)
        parser.add_argument('--max_knn_rows', type=int, default=5000)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--rare_threshold', type=float, default=0.01)
        parser.add_argument('--enable_string_similarity', type=str, default="false")
        parser.add_argument('--tfidf_max_features', type=int, default=20000)
        parser.add_argument('--svd_dims', type=int, default=30)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--X', type=str, required=True)
        parser.add_argument('--y', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args=parser.parse_args()

        try:
            in_path=args.in_file; target_col=args.target_column; model_type=args.model_type.strip().lower()
            outlier_strategy=args.outlier_strategy; outlier_fold=float(args.outlier_fold)
            max_knn_rows=int(args.max_knn_rows); detect_thresh=float(args.numeric_detection_threshold)
            rare_threshold=float(args.rare_threshold)
            enable_string_similarity=str(args.enable_string_similarity).lower() in ("1","true","t","yes","y")
            tfidf_max_features=int(args.tfidf_max_features); svd_dims=int(args.svd_dims)
            preview_rows=int(args.preview_rows)
            out_X=args.X; out_y=args.y; preproc_path=args.preprocessor; fs_path=args.feature_selector; meta_path=args.preprocess_metadata

            if not os.path.exists(in_path):
                print("ERROR input file does not exist:", in_path, file=sys.stderr); sys.exit(1)
            print("Loading:", in_path)
            df = read_with_pandas(in_path)
            print(f"Loaded shape: {df.shape}")

            drop_cols = ["piMetadata", "execution_timestamp", "pipelineid", "component_id", "projectid"]
            present=[c for c in drop_cols if c in df.columns]
            if present:
                df = df.drop(columns=present)
                print("Dropped columns:", present)

            before=len(df); df_hashable=make_hashable_for_dupes(df); df = df_hashable.drop_duplicates(keep='first')
            print(f"Removed duplicates: {before - len(df)} rows dropped")

            if target_col not in df.columns:
                print(f"ERROR target column '{target_col}' not found", file=sys.stderr); sys.exit(1)

            print_before(df, target_col)

            y_raw = df[target_col].copy()
            if model_type=="classification":
                y_pre = y_raw.map(lambda v: 1 if str(v).strip().lower() in ("1","true","yes","y","male","m","positive","pos") else (0 if str(v).strip().lower() in ("0","false","no","n","female","f","negative","neg") else v))
                if not pd.api.types.is_numeric_dtype(y_pre):
                    le=LabelEncoder(); y_enc = le.fit_transform(y_pre.astype(str))
                    y_df=pd.DataFrame({target_col:y_enc})
                    y_map={cl:int(lbl) for lbl,cl in enumerate(le.classes_)}
                else:
                    y_int = pd.to_numeric(y_pre, errors='coerce')
                    y_df=pd.DataFrame({target_col:y_int.astype('Int64') if pd.isna(y_int).any() else y_int.astype(int)}); y_map=None
            else:
                y_num = pd.to_numeric(y_raw, errors='coerce')
                y_df=pd.DataFrame({target_col: y_num})

            before=len(df); keep_mask = y_df[target_col].notna()
            df = df.loc[keep_mask].reset_index(drop=True); y_df = y_df.loc[keep_mask].reset_index(drop=True)
            print(f"Dropped rows with null target '{target_col}': {before - len(df)} rows dropped")

            print("Converting object columns to numeric where possible")
            df, conv_report = convert_object_columns_advanced(df, detect_threshold=detect_thresh, decimal_comma=False, advanced=True)
            print("Conversion report head:", json.dumps(conv_report, default=str)[:1000])

            print("Detecting date-like columns")
            df, date_report = detect_and_extract_dates(df, enabled=True)
            print("Date parse report head:", json.dumps(date_report, default=str)[:1000])

            config={
                'outlier_strategy': outlier_strategy,
                'outlier_fold': outlier_fold,
                'max_knn_rows': max_knn_rows,
                'numeric_detection_threshold': detect_thresh,
                'rare_threshold': rare_threshold,
                'enable_string_similarity': enable_string_similarity
            }
            text_params={'tfidf_max_features': tfidf_max_features, 'svd_dims': svd_dims}

            pre=Preprocessor()
            X_init = df.drop(columns=[target_col]) if target_col in df.columns else df.copy()
            pre.fit(X_init, y=y_df[target_col] if model_type=="classification" else y_df[target_col], config=config, text_params=text_params)

            print_plan(pre)

            print("Applying preprocessing pipeline")
            Xp = pre.transform(X_init, training_mode=True)

            for c in list(Xp.columns):
                if Xp[c].dtype=='object':
                    try: Xp[c]=pd.to_numeric(Xp[c], errors='coerce')
                    except Exception: pass
            Xp.replace([np.inf,-np.inf], np.nan, inplace=True)

            print("Optimizing numeric dtypes")
            for col in Xp.select_dtypes(include=[np.number]).columns:
                try:
                    s = Xp[col]
                    if np.allclose(s.dropna(), np.round(s.dropna())):
                        if s.min() >= np.iinfo(np.int8).min and s.max() <= np.iinfo(np.int8).max:
                            Xp[col] = s.astype('Int8')
                        elif s.min() >= np.iinfo(np.int16).min and s.max() <= np.iinfo(np.int16).max:
                            Xp[col] = s.astype('Int16')
                        elif s.min() >= np.iinfo(np.int32).min and s.max() <= np.iinfo(np.int32).max:
                            Xp[col] = s.astype('Int32')
                        else:
                            Xp[col] = s.astype('Int64')
                    else:
                        Xp[col]=pd.to_numeric(Xp[col], downcast='float')
                except Exception:
                    pass

            print("Fitting feature selector")
            fs=FeatureSelector(task=("classification" if model_type=="classification" else "regression"))
            fs.fit(Xp, y_df[target_col].values)
            Xs = fs.transform(Xp)
            print_after(Xs, fs.selected_features)

            ensure_dir_for(out_X); ensure_dir_for(out_y); ensure_dir_for(preproc_path); ensure_dir_for(fs_path); ensure_dir_for(meta_path)
            Xs.to_parquet(out_X, index=False)
            y_df.to_parquet(out_y, index=False)
            pre.save(preproc_path)
            fs.save(fs_path)

            metadata={
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'model_type': model_type,
                'target_column': target_col,
                'label_mapping': (y_map if model_type=="classification" else None),
                'config': config,
                'text_params': text_params,
                'conversion_report': conv_report,
                'date_report': date_report,
                'num_cols': pre.global_metadata.get('num_cols', []),
                'cat_cols': pre.global_metadata.get('cat_cols', []),
                'selected_features': fs.selected_features
            }
            with open(meta_path,'w',encoding='utf-8') as fh:
                json.dump(metadata, fh, indent=2, ensure_ascii=False)
            print("SUCCESS Preprocessing complete")
        except Exception as exc:
            print("ERROR during preprocessing:", exc, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --outlier_strategy
      - {inputValue: outlier_strategy}
      - --outlier_fold
      - {inputValue: outlier_fold}
      - --max_knn_rows
      - {inputValue: max_knn_rows}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --enable_string_similarity
      - {inputValue: enable_string_similarity}
      - --tfidf_max_features
      - {inputValue: tfidf_max_features}
      - --svd_dims
      - {inputValue: svd_dims}
      - --preview_rows
      - {inputValue: preview_rows}
      - --X
      - {outputPath: X}
      - --y
      - {outputPath: y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --feature_selector
      - {outputPath: feature_selector}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
