name: Data loader with cdn
description: |
  Downloads a dataset from a CDN URL, saves it in Parquet format, and splits it into train/test sets. Prints dataframe diagnostics and ensures split follows given rules. If model_type is classification, prints target variable distribution.
inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download dataset file (CSV, JSON, or Parquet)'}
  - {name: split_size, type: Integer, default: "15", description: 'Test split size as integer percent (default 15) or fraction (e.g. 0.2)'}
  - {name: target_column, type: String, description: 'Name of target column used for splitting'}
  - {name: model_type, type: String, default: "classification", description: 'Type of model (classification or regression)'}
outputs:
  - {name: train_set, type: Data}
  - {name: test_x, type: Data}
  - {name: test_y, type: Data}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, pandas as pd, numpy as np, requests
        from sklearn.model_selection import train_test_split
        parser = argparse.ArgumentParser()
        parser.add_argument('--cdn_url', type=str, required=True)
        parser.add_argument('--split_size', type=float, default=15)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--model_type', type=str, default="classification")
        parser.add_argument('--train_set', type=str, required=True)
        parser.add_argument('--test_x', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        args = parser.parse_args()
        print("===== DOWNLOADING DATA FROM CDN =====")
        resp = requests.get(args.cdn_url, timeout=30)
        resp.raise_for_status()
        tmp_path = "temp_downloaded_file"
        with open(tmp_path, "wb") as f:
            f.write(resp.content)
        if args.cdn_url.endswith(".parquet"):
            df = pd.read_parquet(tmp_path)
        elif args.cdn_url.endswith(".csv"):
            df = pd.read_csv(tmp_path)
        elif args.cdn_url.endswith(".json"):
            df = pd.read_json(tmp_path)
        else:
            raise ValueError("Unsupported file format. Must be .csv, .json, or .parquet")
        print("===== FULL DATAFRAME SUMMARY =====")
        print("Shape:", df.shape)
        print("DF_HEAD:")
        print(df.head().to_string())
        print("DF_INFO:")
        df.info()
        print("DF_DTYPES:")
        print(df.dtypes.to_string())
        print("DF_NULL_COUNTS:")
        print(df.isnull().sum().to_string())
        if args.target_column in df.columns:
            target_values = df[args.target_column].copy()
        else:
            print(f"WARNING: Target column '{args.target_column}' not found. Creating dummy column.")
            target_values = pd.Series(np.zeros(len(df)), index=df.index, name=args.target_column)
        if args.model_type.lower() == "classification":
            print("===== TARGET VARIABLE DISTRIBUTION =====")
            print(target_values.value_counts(dropna=False).to_string())
            print("Unique classes:", target_values.nunique())
        else:
            print("Model type is regression — skipping target distribution summary.")
        split_size_input = float(args.split_size)
        test_size = split_size_input / 100.0 if split_size_input >= 1.0 else split_size_input
        if not (0.0 < test_size < 1.0):
            raise ValueError("split_size must be between 0 and 1 or 1–99 (as percent)")
        stratify_param = target_values if args.model_type.lower() == "classification" and target_values.nunique() > 1 else None
        train_df, test_df, train_y, test_y = train_test_split(df, target_values, test_size=test_size, random_state=42, stratify=stratify_param)
        test_x_df = test_df.drop(columns=[args.target_column], errors="ignore")
        train_set_df = train_df.copy()
        test_y_df = pd.DataFrame({args.target_column: test_y})
        print("===== TRAIN SET =====")
        print("Shape:", train_set_df.shape)
        print(train_set_df.head(3).to_string())
        print("===== TEST X =====")
        print("Shape:", test_x_df.shape)
        print(test_x_df.head(3).to_string())
        print("===== TEST Y =====")
        print("Shape:", test_y_df.shape)
        print(test_y_df.head(10).to_string(index=False))
        os.makedirs(os.path.dirname(args.train_set) or ".", exist_ok=True)
        train_set_df.to_parquet(args.train_set, index=False)
        os.makedirs(os.path.dirname(args.test_x) or ".", exist_ok=True)
        test_x_df.to_parquet(args.test_x, index=False)
        os.makedirs(os.path.dirname(args.test_y) or ".", exist_ok=True)
        test_y_df.to_parquet(args.test_y, index=False)
    args:
      - --cdn_url
      - {inputValue: cdn_url}
      - --split_size
      - {inputValue: split_size}
      - --target_column
      - {inputValue: target_column}
      - --model_type
      - {inputValue: model_type}
      - --train_set
      - {outputPath: train_set}
      - --test_x
      - {outputPath: test_x}
      - --test_y
      - {outputPath: test_y}
