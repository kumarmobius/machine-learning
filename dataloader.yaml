name: Load Data into Pandas
description: |
  Loads a dataset file (CSV, TSV, JSON/JSONL, Excel, Parquet, Feather, ORC) produced by the
  CDN Generic Downloader into a pandas DataFrame, then writes:
    - processed_file (Parquet)
    - preview_file (JSON with preview_rows)
    - schema_file (JSON with column dtypes)
    - row_count (text file containing the number of rows)

  This version is more robust: if the provided `in_file` is a directory (common for the CDN
  downloader), the brick will automatically pick the best candidate file inside (single file
  or largest file). It also supports compressed .gz and .zip inputs and better content sniffing
  when extensions are missing.
inputs:
  - {name: in_file, type: Data, description: "Path to the downloaded file or directory (connect CDN Generic Downloader: out_file)"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview JSON", default: "20"}
outputs:
  - {name: processed_file, type: Data, description: "Processed dataset saved as Parquet"}
  - {name: preview_file, type: Data, description: "Top-N preview saved as JSON"}
  - {name: schema_file, type: Data, description: "Schema (columns -> dtypes) saved as JSON"}
  - {name: row_count, type: Data, description: "File containing the row count (plain text)"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, subprocess, io, gzip, zipfile

        # Try import pandas and engines, install if missing
        try:
            import pandas as pd
        except Exception:
            print("pandas not found — installing pandas and engines (pyarrow, openpyxl, fastparquet)...", file=sys.stderr)
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-input", "pandas", "pyarrow", "openpyxl", "fastparquet"])
            import pandas as pd

        parser = argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--preview_rows', type=str, default="20")
        parser.add_argument('--processed_file', type=str, required=True)
        parser.add_argument('--preview_file', type=str, required=True)
        parser.add_argument('--schema_file', type=str, required=True)
        parser.add_argument('--row_count', type=str, required=True)
        args = parser.parse_args()

        def makedirs_for(path):
            if not path:
                return
            d = os.path.dirname(path)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        # --- robust helpers for sniffing ---
        def _sample_bytes(path, n=8192):
            try:
                with open(path, "rb") as fh:
                    return fh.read(n)
            except Exception:
                return b""

        def _likely_json(sample_bytes):
            if not sample_bytes:
                return False
            s = sample_bytes.lstrip()
            return s.startswith(b"{") or s.startswith(b"[") or b"\n{" in s or b"\n[" in s

        # Robust read_with_pandas:
        def read_with_pandas(path):
            # if path is a directory choose candidate file
            if os.path.isdir(path):
                entries = [os.path.join(path, f) for f in os.listdir(path) if not f.startswith(".")]
                if not entries:
                    raise ValueError(f"Input directory is empty: {path}")
                # prefer regular files only
                files = [p for p in entries if os.path.isfile(p)]
                if not files:
                    raise ValueError(f"No regular files inside directory: {path}")
                # If only one file, pick it; otherwise pick the largest
                if len(files) == 1:
                    path = files[0]
                else:
                    path = max(files, key=lambda p: os.path.getsize(p))
                print(f"Info: in_file was a directory — selected candidate file: {path}")

            # ensure final path exists and is a file
            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError(f"Input path not found or not a file: {path}")

            ext = os.path.splitext(path)[1].lower()

            # handle gz compressed CSV/JSONL
            if ext in (".gz",) or path.endswith(".csv.gz") or path.endswith(".json.gz"):
                with gzip.open(path, "rt", encoding="utf-8", errors="ignore") as fh:
                    sample = fh.read(8192)
                    fh.seek(0)
                    if _likely_json(sample.encode() if isinstance(sample, str) else sample):
                        try:
                            fh.seek(0)
                            return pd.read_json(fh, lines=True)
                        except Exception:
                            fh.seek(0)
                            return pd.read_csv(fh)
                    else:
                        fh.seek(0)
                        return pd.read_csv(fh)

            # handle zip archives: pick first non-dir member (or largest member)
            if ext == ".zip":
                with zipfile.ZipFile(path, "r") as z:
                    names = [n for n in z.namelist() if not n.endswith("/")]
                    if not names:
                        raise ValueError(f"No files inside zip archive: {path}")
                    # prefer largest member
                    member = max(names, key=lambda n: z.getinfo(n).file_size if z.getinfo(n).file_size else 0)
                    with z.open(member) as fh:
                        sample = fh.read(8192)
                        if _likely_json(sample):
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2, encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2, encoding="utf-8"))

            # extension-based quick attempts
            try:
                if ext == ".csv":
                    return pd.read_csv(path)
                if ext in (".tsv", ".tab"):
                    return pd.read_csv(path, sep="\t")
                if ext in (".json", ".ndjson", ".jsonl"):
                    try:
                        return pd.read_json(path, lines=True)
                    except ValueError:
                        return pd.read_json(path)
                if ext in (".xls", ".xlsx"):
                    return pd.read_excel(path)
                if ext in (".parquet", ".pq"):
                    return pd.read_parquet(path, engine="auto")
                if ext == ".feather":
                    return pd.read_feather(path)
                if ext == ".orc":
                    return pd.read_orc(path)
            except Exception:
                # fallthrough to sniffing heuristics
                pass

            # Fallback sniffing based on sampling bytes - prefer JSONL if it looks like JSON, otherwise CSV
            sample = _sample_bytes(path, n=8192)
            if _likely_json(sample):
                try:
                    return pd.read_json(path, lines=True)
                except Exception:
                    pass
            try:
                return pd.read_csv(path)
            except Exception:
                pass

            raise ValueError(f"Unsupported or unreadable file format for automatic loading: {path}")

        # ---------- main ----------
        try:
            in_path = args.in_file
            preview_rows = int(str(args.preview_rows))
            processed_out = args.processed_file
            preview_out = args.preview_file
            schema_out = args.schema_file
            row_count_out = args.row_count

            # If input is a directory, auto-select a candidate file inside
            if os.path.isdir(in_path):
                entries = [os.path.join(in_path, f) for f in os.listdir(in_path) if not f.startswith(".")]
                if not entries:
                    print(f"ERROR: input directory is empty: {in_path}", file=sys.stderr)
                    sys.exit(1)
                files = [p for p in entries if os.path.isfile(p)]
                if not files:
                    print(f"ERROR: no regular files inside directory: {in_path}", file=sys.stderr)
                    sys.exit(1)
                if len(files) == 1:
                    selected = files[0]
                else:
                    selected = max(files, key=lambda p: os.path.getsize(p))
                print(f"Detected directory input; selected file: {selected}")
                in_path = selected

            if not os.path.exists(in_path):
                print(f"ERROR: input file does not exist: {in_path}", file=sys.stderr)
                sys.exit(1)

            print(f"Loading file: {in_path}")
            df = read_with_pandas(in_path)
            print(f"Loaded DataFrame: rows={len(df)}, cols={len(df.columns)}")

            # Ensure output dirs exist
            makedirs_for(processed_out)
            makedirs_for(preview_out)
            makedirs_for(schema_out)
            makedirs_for(row_count_out)

            # Save processed file as Parquet
            df.to_parquet(processed_out, index=False)
            print("Wrote processed parquet:", processed_out)

            # Save preview (JSON records)
            preview_df = df.head(max(1, preview_rows))
            preview_df.to_json(preview_out, orient="records", indent=2, force_ascii=False)
            print("Wrote preview json:", preview_out)

            # Save schema
            schema = { str(c): str(t) for c, t in zip(df.columns, df.dtypes) }
            with open(schema_out, "w", encoding="utf-8") as fh:
                json.dump({"columns": schema}, fh, indent=2, ensure_ascii=False)
            print("Wrote schema json:", schema_out)

            # Save row count
            with open(row_count_out, "w", encoding="utf-8") as fh:
                fh.write(str(len(df)))
            print("Wrote row count:", row_count_out)

            print("SUCCESS: All artifacts produced.")
        except Exception as exc:
            print("ERROR during Load Data into Pandas:", exc, file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --preview_rows
      - {inputValue: preview_rows}
      - --processed_file
      - {outputPath: processed_file}
      - --preview_file
      - {outputPath: preview_file}
      - --schema_file
      - {outputPath: schema_file}
      - --row_count
      - {outputPath: row_count}
