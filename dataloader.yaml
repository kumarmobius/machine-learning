name: Load Data into
description: |
  Loads a local file (downloaded from CDN) into a pandas DataFrame.
  Detects format (CSV/TSV/JSON/JSONL/XLSX/XLS/PARQUET/FEATHER/ORC) and writes:
    - processed_file (Parquet)
    - preview_file (JSON of first rows)
    - schema_file (JSON describing columns and dtypes)
    - row_count (number of rows)
inputs:
  - {name: in_file, type: Data, description: "Path to the downloaded file (local) to load"}
  - {name: preview_rows, type: Integer, description: "Number of rows to include in preview", optional: true}
outputs:
  - {name: processed_file, type: Data, description: "Canonicalized Parquet file (processed) saved locally"}
  - {name: preview_file, type: Data, description: "Preview JSON file with first N rows"}
  - {name: schema_file, type: Data, description: "JSON file describing column names and dtypes"}
  - {name: row_count, type: Number, description: "Number of rows in the loaded DataFrame"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import subprocess
        from urllib.parse import unquote
        import traceback

        CHUNK_SIZE = 16 * 1024

        # Try to import pandas and engines. If missing, install them.
        try:
            import pandas as pd
        except Exception:
            print("pandas not found — installing pandas and minimal engines (pyarrow, openpyxl)...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet", "pandas", "pyarrow", "openpyxl", "fastparquet"])
            import pandas as pd

        parser = argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--processed_file', type=str, required=True)
        parser.add_argument('--preview_file', type=str, required=True)
        parser.add_argument('--schema_file', type=str, required=True)
        parser.add_argument('--row_count', type=str, required=True)
        args = parser.parse_args()

        def makedirs(p):
            if p and not os.path.exists(p):
                os.makedirs(p, exist_ok=True)

        def resolve_output_path(output_arg, default_filename):
            base = os.path.basename(output_arg)
            if base == "data":
                makedirs(os.path.dirname(output_arg) or ".")
                return output_arg
            else:
                makedirs(output_arg)
                return os.path.join(output_arg, default_filename)

        def load_with_pandas(path):
            ext = os.path.splitext(path)[1].lower()
            try:
                if ext in ['.csv']:
                    return pd.read_csv(path)
                elif ext in ['.tsv', '.tab']:
                    return pd.read_csv(path, sep='\t')
                elif ext in ['.json']:
                    # try line-delimited first, then generic
                    try:
                        return pd.read_json(path, lines=True)
                    except ValueError:
                        return pd.read_json(path)
                elif ext in ['.ndjson', '.jsonl']:
                    return pd.read_json(path, lines=True)
                elif ext in ['.xlsx', '.xls', '.xlsm']:
                    return pd.read_excel(path)
                elif ext in ['.parquet', '.pq']:
                    return pd.read_parquet(path, engine='auto')
                elif ext in ['.feather']:
                    return pd.read_feather(path)
                elif ext in ['.orc']:
                    return pd.read_orc(path)
                else:
                    # Fallback heuristics: try JSON lines, then CSV
                    try:
                        return pd.read_json(path, lines=True)
                    except Exception:
                        pass
                    try:
                        return pd.read_csv(path)
                    except Exception:
                        pass
                    raise ValueError("Unsupported or non-tabular file format for automated loading")
            except Exception as e:
                raise

        try:
            in_path = args.in_file
            if not os.path.exists(in_path):
                print(f"Input file does not exist: {in_path}")
                sys.exit(1)

            print("Loading file:", in_path)
            try:
                df = load_with_pandas(in_path)
            except Exception as e:
                print("Failed to load file into pandas:", str(e))
                traceback.print_exc()
                sys.exit(2)

            # Resolve outputs (allow runtime 'data' sentinel for file paths)
            processed_out = resolve_output_path(args.processed_file, "data.parquet")
            preview_out = resolve_output_path(args.preview_file, "preview.json")
            schema_out = resolve_output_path(args.schema_file, "schema.json")
            row_count_out = args.row_count  # will be treated as an outputPath by runtime

            # Write processed file as parquet
            makedirs(os.path.dirname(processed_out) or ".")
            print("Writing processed parquet to:", processed_out)
            # Use pyarrow if available, else fastparquet; let pandas choose
            df.to_parquet(processed_out, index=False)

            # Write preview
            preview_rows = max(1, int(args.preview_rows or 20))
            preview_df = df.head(preview_rows)
            makedirs(os.path.dirname(preview_out) or ".")
            preview_df.to_json(preview_out, orient='records', lines=False, force_ascii=False)

            # Write schema (columns and dtypes)
            schema = { str(col): str(dtype) for col, dtype in zip(df.columns, df.dtypes) }
            makedirs(os.path.dirname(schema_out) or ".")
            with open(schema_out, 'w', encoding='utf-8') as fh:
                json.dump({"columns": schema}, fh, indent=2, ensure_ascii=False)

            # Row count — write a simple text file as the runtime output location
            row_count = int(len(df))
            makedirs(os.path.dirname(row_count_out) or ".")
            with open(row_count_out, 'w') as fh:
                fh.write(str(row_count))

            # Print artifacts (for logs)
            print("Artifacts produced:")
            print(" - Processed parquet:", processed_out)
            print(" - Preview JSON:", preview_out)
            print(" - Schema JSON:", schema_out)
            print(" - Row count file:", row_count_out, "(contains number of rows)")

        except Exception as e:
            print("Error in Load Data into Pandas:", str(e))
            traceback.print_exc()
            sys.exit(1)
  args:
    - --in_file
    - {inputPath: in_file}
    - --preview_rows
    - {inputValue: preview_rows}
    - --processed_file
    - {outputPath: processed_file}
    - --preview_file
    - {outputPath: preview_file}
    - --schema_file
    - {outputPath: schema_file}
    - --row_count
    - {outputPath: row_count}
