name: Encoding and Feature Creation v4.6
inputs:
  - name: imputed_X
    type: Dataset
    description: "Imputed features from Part 1"
  
  - name: aligned_y
    type: Dataset
    description: "Aligned targets from Part 1"
  
  - name: imputation_config
    type: Data
    description: "Config from Part 1 with column info"

  - name: create_interactions
    type: String
    description: "'true' or 'false' - Create polynomial interactions"
    optional: true
    default: "false"
  
  - name: interaction_degree
    type: Integer
    description: "Polynomial degree (2 recommended)"
    optional: true
    default: "2"
  
  - name: max_interaction_features
    type: Integer
    description: "Maximum interaction features to create"
    optional: true
    default: "15"

  - name: encoding_strategy
    type: String
    description: "Global encoding strategy: 'auto' (smart selection), 'ohe' (force OneHot), 'target_kfold' (force Target Encoding), 'count' (force Count Encoding), 'ordinal' (force Ordinal Encoding)"
    optional: true
    default: "auto"
  
  - name: ohe_max_categories
    type: Integer
    description: "Maximum unique categories for OneHot Encoding. Columns with more unique values will use alternative encoding."
    optional: true
    default: "30"
  
  - name: target_enc_smoothing
    type: Float
    description: "Smoothing parameter for target encoding (higher = more smoothing to global mean)"
    optional: true
    default: "10.0"
  
  - name: target_enc_folds
    type: Integer
    description: "Number of folds for out-of-fold target encoding"
    optional: true
    default: "5"
  
  - name: rare_threshold
    type: Float
    description: "Threshold for rare category grouping (0.01 = 1%). Categories with frequency < threshold are grouped."
    optional: true
    default: "0.01"
  
  - name: numeric_transform_strategy
    type: String
    description: "Numeric transformation strategy: 'auto' (smart selection), 'standard', 'robust', 'minmax', 'log', 'logcp', 'sqrt', 'reciprocal', 'arcsin', 'power', 'boxcox', 'yeojohnson', 'quantile', 'none'"
    optional: true
    default: "auto"
  
  - name: transform_positive_only
    type: String
    description: "Apply transformations only to positive-valued features: 'true' or 'false'"
    optional: true
    default: "true"
  
  - name: skewness_threshold
    type: Float
    description: "Skewness threshold for applying transformations (absolute value)"
    optional: true
    default: "0.5"
  
  - name: kurtosis_threshold
    type: Float
    description: "Kurtosis threshold for applying robust scaling"
    optional: true
    default: "3.0"
  
  - name: outlier_threshold
    type: Float
    description: "Outlier threshold for applying robust scaling (fraction of outliers)"
    optional: true
    default: "0.01"
  
  - name: positive_offset
    type: Float
    description: "Offset added to ensure positivity for log/boxcox transforms"
    optional: true
    default: "1e-6"

outputs:
  - {name: engineered_X, type: Dataset, description: "Final engineered features"}
  - {name: train_y, type: Dataset, description: "Final aligned targets"}
  - {name: preprocessor, type: Data, description: "Fitted preprocessor pickle"}
  - {name: engineering_metadata, type: Data, description: "Complete metadata JSON with per-column encoding details"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -c
      - |
        # Install feature-engine (other packages should already be in the image)
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \
          'feature-engine'
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, gzip, math
        from datetime import datetime
        from itertools import combinations
        import pandas as pd, numpy as np
        from sklearn.preprocessing import (StandardScaler, RobustScaler, MinMaxScaler, 
                                         PowerTransformer, OneHotEncoder, OrdinalEncoder,
                                         QuantileTransformer, KBinsDiscretizer, FunctionTransformer)
        from sklearn.pipeline import Pipeline
        from sklearn.model_selection import StratifiedKFold, KFold
        import cloudpickle
        from scipy import stats
        from scipy.special import boxcox, inv_boxcox
        
        # Import feature-engine transformers
        from feature_engine.transformation import (
            LogTransformer, LogCpTransformer, ReciprocalTransformer,
            ArcsinTransformer, PowerTransformer as FEPowerTransformer,
            BoxCoxTransformer, YeoJohnsonTransformer
        )
        
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        # ADVANCED NUMERIC TRANSFORMER SELECTION 
        # Add this after imports
        def safe_sqrt(x):
            result = np.sqrt(x)
            return result.flatten() if hasattr(result, 'flatten') and len(result.shape) > 1 else result
                def analyze_numeric_feature(series):
                    v = series.dropna()
                    if len(v) < 10:
                        return None
            
            stats_dict = {
                'n_samples': len(v),
                'mean': float(v.mean()),
                'std': float(v.std()),
                'min': float(v.min()),
                'max': float(v.max()),
                'skewness': float(v.skew()),
                'kurtosis': float(v.kurtosis()),
                'is_positive': bool(v.min() > 0),
                'has_negatives': bool(v.min() < 0),
                'has_zeros': bool(v.min() <= 0 <= v.max()),
                'range': float(v.max() - v.min()),
                'cv': float(v.std() / v.mean()) if v.mean() != 0 else float('inf'),
                'q1': float(v.quantile(0.25)),
                'median': float(v.median()),
                'q3': float(v.quantile(0.75)),
                'iqr': float(v.quantile(0.75) - v.quantile(0.25))
            }
            
            # Check for outliers using IQR method
            lower_bound = stats_dict['q1'] - 1.5 * stats_dict['iqr']
            upper_bound = stats_dict['q3'] + 1.5 * stats_dict['iqr']
            outliers = v[(v < lower_bound) | (v > upper_bound)]
            stats_dict['outlier_fraction'] = len(outliers) / len(v)
            stats_dict['outlier_count'] = len(outliers)
            
            # Check distribution shape
            stats_dict['is_normal'] = (abs(stats_dict['skewness']) < 0.5 and 
                                      abs(stats_dict['kurtosis']) < 3.0)
            stats_dict['is_highly_skewed'] = abs(stats_dict['skewness']) > 1.0
            stats_dict['is_heavy_tailed'] = stats_dict['kurtosis'] > 3.0
            
            return stats_dict
        
        def choose_numeric_transformer(series, strategy='auto', positive_only=True, 
                                      skew_thresh=0.5, kurt_thresh=3.0, outlier_thresh=0.01,
                                      positive_offset=1e-6):            
            # If user specified a specific strategy, use it
            if strategy != 'auto':
                if strategy == 'standard':
                    return 'standard', StandardScaler(), 'user_specified'
                elif strategy == 'robust':
                    return 'robust', RobustScaler(), 'user_specified'
                elif strategy == 'minmax':
                    return 'minmax', MinMaxScaler(), 'user_specified'
                elif strategy == 'log':
                    return 'log', LogTransformer(), 'user_specified'
                elif strategy == 'logcp':
                    return 'logcp', LogCpTransformer(), 'user_specified'
                elif strategy == 'sqrt':
                    return 'sqrt', FunctionTransformer(np.sqrt), 'user_specified'
                elif strategy == 'reciprocal':
                    return 'reciprocal', ReciprocalTransformer(), 'user_specified'
                elif strategy == 'arcsin':
                    return 'arcsin', ArcsinTransformer(), 'user_specified'
                elif strategy == 'power':
                    return 'power', FEPowerTransformer(), 'user_specified'
                elif strategy == 'boxcox':
                    return 'boxcox', BoxCoxTransformer(), 'user_specified'
                elif strategy == 'yeojohnson':
                    return 'yeojohnson', YeoJohnsonTransformer(), 'user_specified'
                elif strategy == 'quantile':
                    return 'quantile', QuantileTransformer(output_distribution='normal'), 'user_specified'
                elif strategy == 'none':
                    return 'none', None, 'user_specified'
                else:
                    print(f"[WARNING] Unknown strategy '{strategy}', using 'auto'")
                    # Fall through to auto strategy
            
            # AUTO STRATEGY - Intelligent selection based on data characteristics
            stats = analyze_numeric_feature(series)
            if stats is None:
                return 'standard', StandardScaler(), 'insufficient_data'
            
            # Initialize decision factors
            transform_type = 'standard'
            transform_reason = []
            
            # Decision Tree for Transform Selection:
            
            # 1. Check for heavy outliers (>5% of data)
            if stats['outlier_fraction'] > 0.05:
                transform_type = 'robust'
                transform_reason.append(f"high_outliers_{stats['outlier_fraction']:.1%}")
            
            # 2. Check for moderate outliers AND heavy tails
            elif stats['outlier_fraction'] > outlier_thresh and stats['kurtosis'] > kurt_thresh:
                transform_type = 'robust'
                transform_reason.append(f"outliers_heavy_tails")
            
            # 3. Check for highly skewed positive data
            elif abs(stats['skewness']) > 1.5 and stats['is_positive']:
                if stats['has_zeros']:
                    transform_type = 'logcp'  # Log(x + 1) for data with zeros
                    transform_reason.append(f"highly_skewed_with_zeros_skew{stats['skewness']:.2f}")
                else:
                    transform_type = 'log'  # Natural log for strictly positive
                    transform_reason.append(f"highly_skewed_positive_skew{stats['skewness']:.2f}")
            
            # 4. Check for moderately skewed data
            elif abs(stats['skewness']) > skew_thresh:
                if stats['is_positive']:
                    transform_type = 'boxcox'  # Box-Cox for positive skewed
                    transform_reason.append(f"moderately_skewed_positive_skew{stats['skewness']:.2f}")
                else:
                    transform_type = 'yeojohnson'  # Yeo-Johnson for data with negatives/zeros
                    transform_reason.append(f"moderately_skewed_with_negatives_skew{stats['skewness']:.2f}")
            
            # 5. Check for bounded data (0-1 range)
            elif stats['min'] >= 0 and stats['max'] <= 1:
                transform_type = 'arcsin'  # Arcsin for proportions
                transform_reason.append(f"bounded_0_1_range")
            
            # 6. Check for count data (non-negative integers)
            elif stats['min'] >= 0 and series.apply(lambda x: float(x).is_integer()).all():
                transform_type = 'sqrt'  # Square root for count data (variance stabilizing)
                transform_reason.append(f"count_data")
            
            # 7. Check for ratio data (values > 0, often with large range)
            elif stats['min'] > 0 and stats['max'] / stats['min'] > 100:
                transform_type = 'log'  # Log for ratios
                transform_reason.append(f"large_ratio_range_{stats['max']/stats['min']:.0f}x")
            
            # 8. Check for data with extreme kurtosis (heavy tails)
            elif abs(stats['kurtosis']) > 10:
                transform_type = 'quantile'  # Quantile transform for heavy tails
                transform_reason.append(f"extreme_kurtosis_{stats['kurtosis']:.2f}")
            
            # 9. Check for reciprocal relationship (inverse)
            elif stats['min'] > 0 and stats['skewness'] < -1.0:
                transform_type = 'reciprocal'  # Reciprocal for negatively skewed positive data
                transform_reason.append(f"negative_skew_reciprocal_skew{stats['skewness']:.2f}")
            
            # Create transformer based on selected type
            transformer = None
            if transform_type == 'standard':
                transformer = StandardScaler()
            elif transform_type == 'robust':
                transformer = RobustScaler()
            elif transform_type == 'minmax':
                transformer = MinMaxScaler()
            elif transform_type == 'log':
                transformer = LogTransformer()
            elif transform_type == 'logcp':
                transformer = LogCpTransformer()
            elif transform_type == 'sqrt':
                transformer = FunctionTransformer(safe_sqrt)
            elif transform_type == 'reciprocal':
                transformer = ReciprocalTransformer()
            elif transform_type == 'arcsin':
                transformer = ArcsinTransformer()
            elif transform_type == 'power':
                transformer = FEPowerTransformer()
            elif transform_type == 'boxcox':
                transformer = BoxCoxTransformer()
            elif transform_type == 'yeojohnson':
                transformer = YeoJohnsonTransformer()
            elif transform_type == 'quantile':
                transformer = QuantileTransformer(output_distribution='normal', random_state=42)
            elif transform_type == 'none':
                transformer = None
            
            reason_str = "_".join(transform_reason) if transform_reason else "normal_distribution"
            return transform_type, transformer, reason_str
        
        
        def choose_categorical_encoder(col, series, target_series=None, model_type='classification',
                                      encoding_strategy='auto', ohe_max_categories=30, rare_threshold=0.01):
            nunique = series.nunique(dropna=True)
            total = len(series.dropna())
            
            # Apply rare category grouping
            if rare_threshold > 0 and total > 0:
                value_counts = series.value_counts(normalize=True)
                rare_categories = value_counts[value_counts < rare_threshold].index.tolist()
                if rare_categories:
                    series = series.copy()
                    series[series.isin(rare_categories)] = '__RARE__'
                    nunique = series.nunique(dropna=True)
            
            # Force encoding strategy if specified
            if encoding_strategy == 'ohe':
                if nunique <= ohe_max_categories:
                    return 'onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)
                else:
                    print(f"[WARNING] Column '{col}' has {nunique} categories (> {ohe_max_categories}). Using count encoding instead.")
                    return 'count', None
            
            elif encoding_strategy == 'target_kfold':
                if target_series is not None:
                    return 'target_kfold', None
                else:
                    print(f"[WARNING] No target for column '{col}'. Using count encoding instead.")
                    return 'count', None
            
            elif encoding_strategy == 'count':
                return 'count', None
            
            elif encoding_strategy == 'ordinal':
                if nunique <= 50:
                    return 'ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
                else:
                    print(f"[WARNING] Column '{col}' has {nunique} categories (>50). Using count encoding instead.")
                    return 'count', None
            
            # Auto strategy (default)
            if nunique <= 2:
                return 'onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)
            
            elif nunique <= 10:
                return 'onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)
            
            elif nunique <= ohe_max_categories:
                if target_series is not None:
                    # For binary/multi-class classification and regression
                    return 'target_kfold', None
                else:
                    return 'onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)
            
            else:
                # High cardinality
                if target_series is not None:
                    return 'target_kfold', None
                else:
                    return 'count', None

        def fit_target_encoder_kfold(df, col, y, n_splits=5, smoothing=1.0, random_state=42, model_type='classification'):
            X_col = df[col].astype(object).fillna('__NA__')
            
            if model_type == 'classification' and len(y.unique()) > 2:
                # Multi-class: encode each class probability
                classes = sorted(y.unique())
                oof_maps = {}
                for cls in classes:
                    y_binary = (y == cls).astype(int)
                    global_mean = float(y_binary.mean())
                    
                    # Create OOF predictions
                    cls_oof = pd.Series(index=df.index, dtype=float)
                    try:
                        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                        splits = list(kf.split(df, y_binary))
                    except:
                        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                        splits = list(kf.split(df))
                    
                    for train_idx, val_idx in splits:
                        train_means = y_binary.iloc[train_idx].groupby(X_col.iloc[train_idx]).mean()
                        cls_oof.iloc[val_idx] = X_col.iloc[val_idx].map(lambda v: train_means.get(v, global_mean))
                    
                    # Apply smoothing
                    counts = X_col.value_counts()
                    smooth_map = {}
                    for cat, cnt in counts.items():
                        if cnt > 0:
                            cat_mean = float(y_binary[X_col == cat].mean())
                            alpha = cnt / (cnt + smoothing)
                            smooth_map[cat] = alpha * cat_mean + (1 - alpha) * global_mean
                        else:
                            smooth_map[cat] = global_mean
                    
                    oof_maps[cls] = {
                        'oof': cls_oof.fillna(global_mean),
                        'mapping': smooth_map,
                        'global_mean': global_mean
                    }
                
                return oof_maps
            
            else:
                # Binary classification or regression
                if model_type == 'classification':
                    y_encoded = y.astype(int) if len(y.unique()) == 2 else y
                else:
                    y_encoded = y
                
                global_mean = float(y_encoded.mean())
                oof = pd.Series(index=df.index, dtype=float)
                
                try:
                    if model_type == 'classification' and len(y.unique()) <= 10:
                        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                        splits = list(kf.split(df, y))
                    else:
                        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                        splits = list(kf.split(df))
                except:
                    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
                    splits = list(kf.split(df))
                
                for train_idx, val_idx in splits:
                    train_means = y_encoded.iloc[train_idx].groupby(X_col.iloc[train_idx]).mean()
                    oof.iloc[val_idx] = X_col.iloc[val_idx].map(lambda v: train_means.get(v, global_mean))
                
                # Apply smoothing
                counts = X_col.value_counts()
                smooth_map = {}
                for cat, cnt in counts.items():
                    if cnt > 0:
                        cat_mean = float(y_encoded[X_col == cat].mean())
                        alpha = cnt / (cnt + smoothing)
                        smooth_map[cat] = alpha * cat_mean + (1 - alpha) * global_mean
                    else:
                        smooth_map[cat] = global_mean
                
                return {
                    'single': {
                        'oof': oof.fillna(global_mean),
                        'mapping': smooth_map,
                        'global_mean': global_mean
                    }
                }

        def add_polynomial_features(df, numeric_cols, degree=2, max_features=20):
            if len(numeric_cols) < 2:
                return df
            print(f"[INFO] Creating polynomial features (degree={degree})")
            variances = df[numeric_cols].var().sort_values(ascending=False)
            top_cols = variances.head(min(5, len(numeric_cols))).index.tolist()
            new_features = {}
            interaction_count = 0
            for col1, col2 in combinations(top_cols, 2):
                if interaction_count >= max_features:
                    break
                new_features[f"{col1}_x_{col2}"] = df[col1] * df[col2]
                interaction_count += 1
                if (df[col2] != 0).all():
                    new_features[f"{col1}_div_{col2}"] = df[col1] / df[col2]
                    interaction_count += 1
                if interaction_count >= max_features:
                    break
            if degree == 2 and interaction_count < max_features:
                for col in top_cols[:3]:
                    if interaction_count >= max_features:
                        break
                    new_features[f"{col}_squared"] = df[col] ** 2
                    interaction_count += 1
            for name, values in new_features.items():
                df[name] = values
            print(f"[INFO] Created {len(new_features)} polynomial features")
            return df

        class Preprocessor:
            def __init__(self, encoding_strategy='auto', ohe_max_categories=30, 
                        target_enc_smoothing=10.0, target_enc_folds=5, rare_threshold=0.01,
                        numeric_transform_strategy='auto', transform_positive_only=True,
                        skewness_threshold=0.5, kurtosis_threshold=3.0, outlier_threshold=0.01,
                        positive_offset=1e-6):
                self.col_config = {}
                self.metadata = {}
                self.imputation_fitted = False
                self.encoding_strategy = encoding_strategy
                self.ohe_max_categories = ohe_max_categories
                self.target_enc_smoothing = target_enc_smoothing
                self.target_enc_folds = target_enc_folds
                self.rare_threshold = rare_threshold
                self.numeric_transform_strategy = numeric_transform_strategy
                self.transform_positive_only = transform_positive_only
                self.skewness_threshold = skewness_threshold
                self.kurtosis_threshold = kurtosis_threshold
                self.outlier_threshold = outlier_threshold
                self.positive_offset = positive_offset
            
            def fit(self, df, y=None, config=None, model_type='classification'):
                self.metadata = config or {}
                self.metadata['model_type'] = model_type
                self.metadata['encoding_config'] = {
                    'strategy': self.encoding_strategy,
                    'ohe_max_categories': self.ohe_max_categories,
                    'target_enc_smoothing': self.target_enc_smoothing,
                    'target_enc_folds': self.target_enc_folds,
                    'rare_threshold': self.rare_threshold
                }
                self.metadata['numeric_transform_config'] = {
                    'strategy': self.numeric_transform_strategy,
                    'transform_positive_only': self.transform_positive_only,
                    'skewness_threshold': self.skewness_threshold,
                    'kurtosis_threshold': self.kurtosis_threshold,
                    'outlier_threshold': self.outlier_threshold,
                    'positive_offset': self.positive_offset
                }
                
                num_cols = self.metadata['num_cols']
                cat_cols = self.metadata['cat_cols']
                target_cols = self.metadata['target_columns']
                datetime_cols = self.metadata.get('datetime_cols', [])
                
                # Store imputation info
                self.imputation_fitted = True
                self.numeric_imputation_methods = self.metadata.get('numeric_imputation_methods', {})
                
                print(f"[INFO] Encoding strategy: {self.encoding_strategy}")
                print(f"[INFO] Numeric transform strategy: {self.numeric_transform_strategy}")
                if self.numeric_transform_strategy == 'auto':
                    print(f"[INFO] Auto transform thresholds - Skew: {self.skewness_threshold}, Kurtosis: {self.kurtosis_threshold}, Outliers: {self.outlier_threshold}")
                
                # Transform and scale numeric columns
                print("[INFO] Fitting numeric transformers...")
                numeric_transform_summary = {}
                
                for c in num_cols:
                    if c in df.columns:
                        series = df[c]
                        
                        # Choose transformer based on strategy
                        transform_type, transformer, transform_reason = choose_numeric_transformer(
                            series=series,
                            strategy=self.numeric_transform_strategy,
                            positive_only=self.transform_positive_only,
                            skew_thresh=self.skewness_threshold,
                            kurt_thresh=self.kurtosis_threshold,
                            outlier_thresh=self.outlier_threshold,
                            positive_offset=self.positive_offset
                        )
                        
                        # Fit transformer
                        if transformer is not None:
                            # Handle feature-engine transformers
                            if hasattr(transformer, 'fit'):
                                transformer.fit(df[[c]])
                            else:
                                # sklearn transformers
                                transformer.fit(df[[c]].fillna(df[c].median()))
                        
                        # Store configuration
                        self.col_config[c] = {
                            'type': 'numeric',
                            'transform_type': transform_type,
                            'transformer': transformer,
                            'transform_reason': transform_reason,
                            'statistics': analyze_numeric_feature(series)
                        }
                        
                        numeric_transform_summary[c] = {
                            'transform': transform_type,
                            'reason': transform_reason
                        }
                
                # Print numeric transform summary
                print("[INFO] Numeric Transform Summary:")
                for col, info in numeric_transform_summary.items():
                    stats = self.col_config[col]['statistics']
                    if stats:
                        skew = stats['skewness']
                        kurt = stats['kurtosis']
                        outliers = stats['outlier_fraction']
                        print(f"  {col}: {info['transform']} (skew={skew:.2f}, kurt={kurt:.2f}, outliers={outliers:.1%}) - {info['reason']}")
                
                # Encode categorical columns
                print("[INFO] Fitting encoders for categorical columns...")
                encoding_summary = {}
                
                for c in cat_cols:
                    if c not in df.columns or c in target_cols:
                        continue
                    
                    # Prepare series with rare category handling
                    s = df[c].astype(object).fillna('__NA__')
                    if self.rare_threshold > 0 and len(s) > 0:
                        value_counts = s.value_counts(normalize=True)
                        rare_categories = value_counts[value_counts < self.rare_threshold].index.tolist()
                        if rare_categories:
                            s = s.copy()
                            s[s.isin(rare_categories)] = '__RARE__'
                    
                    # Choose encoder
                    enc_type, enc_obj = choose_categorical_encoder(
                        c, s, target_series=y, model_type=model_type,
                        encoding_strategy=self.encoding_strategy,
                        ohe_max_categories=self.ohe_max_categories,
                        rare_threshold=self.rare_threshold
                    )
                    
                    cfg = {
                        'type': 'categorical',
                        'encoder_type': enc_type,
                        'original_nunique': int(s.nunique(dropna=True)),
                        'rare_categories_grouped': len(rare_categories) if 'rare_categories' in locals() else 0
                    }
                    
                    if enc_type == 'onehot':
                        enc_obj.fit(s.to_frame())
                        cfg['encoder_obj'] = enc_obj
                        cfg['categories'] = [cat for cat in enc_obj.categories_[0]]
                        cfg['n_categories'] = len(cfg['categories'])
                        cfg['ohe_columns'] = [f"{c}__{cat}" for cat in cfg['categories']]
                    
                    elif enc_type == 'target_kfold' and y is not None:
                        target_maps = fit_target_encoder_kfold(
                            df, c, y, 
                            n_splits=self.target_enc_folds,
                            smoothing=self.target_enc_smoothing,
                            random_state=42,
                            model_type=model_type
                        )
                        
                        if 'single' in target_maps:
                            cfg['target_mapping'] = target_maps['single']['mapping']
                            cfg['target_global_mean'] = target_maps['single']['global_mean']
                            cfg['oof_encoding'] = target_maps['single']['oof']
                        else:
                            # Multi-class
                            cfg['multi_class_mappings'] = {}
                            for cls, cls_data in target_maps.items():
                                cfg['multi_class_mappings'][str(cls)] = {
                                    'mapping': cls_data['mapping'],
                                    'global_mean': cls_data['global_mean']
                                }
                                cfg[f'oof_encoding_class_{cls}'] = cls_data['oof']
                    
                    elif enc_type == 'ordinal':
                        enc_obj.fit(s.to_frame())
                        cfg['encoder_obj'] = enc_obj
                        cfg['categories'] = [cat for cat in enc_obj.categories_[0]]
                        cfg['n_categories'] = len(cfg['categories'])
                    
                    elif enc_type == 'count':
                        counts = s.value_counts().to_dict()
                        cfg['count_map'] = counts
                        cfg['max_count'] = max(counts.values()) if counts else 0
                        cfg['min_count'] = min(counts.values()) if counts else 0
                    
                    self.col_config[c] = cfg
                    encoding_summary[c] = enc_type
                
                # Print encoding summary
                print("[INFO] Categorical Encoding Summary:")
                for col, enc_type in encoding_summary.items():
                    nunique = self.col_config[col]['original_nunique']
                    print(f"  {col}: {enc_type} (nunique={nunique})")
                
                self.metadata['encoding_summary'] = encoding_summary
                self.metadata['numeric_transform_summary'] = numeric_transform_summary
                return self
            
            def transform(self, df, training_mode=False):
                df = df.copy()
                num_cols = self.metadata['num_cols']
                cat_cols = self.metadata['cat_cols']
                target_cols = self.metadata['target_columns']
                
                # Check if data needs imputation
                if not self.imputation_fitted:
                    raise ValueError("This preprocessor expects pre-imputed data. Run Part 1 (Imputation) first!")
                
                # Verify no missing values in numeric columns
                missing_numeric = [c for c in num_cols if c in df.columns and df[c].isna().any()]
                if missing_numeric and not training_mode:
                    print(f"[WARNING] Found missing values in: {missing_numeric}. Applying median imputation...")
                    for c in missing_numeric:
                        df[c] = df[c].fillna(df[c].median())
                
                # Transform numeric columns
                print("[INFO] Transforming numeric columns...")
                for c in num_cols:
                    if c in df.columns and c in self.col_config:
                        cfg = self.col_config[c]
                        transformer = cfg.get('transformer')
                        
                        if transformer is not None:
                            try:
                                # Transform the column
                                transformed = transformer.transform(df[[c]])
                                if hasattr(transformed, 'shape') and transformed.shape[1] == 1:
                                    df[c] = transformed.flatten()
                                else:
                                    df[c] = transformed
                            except Exception as e:
                                print(f"[WARNING] Failed to transform {c}: {e}. Using original values.")
                
                # Create interactions if training mode
                if training_mode and self.metadata.get('create_interactions', False):
                    print("[INFO] Creating interactions...")
                    current_num = [c for c in df.columns if c in num_cols]
                    if len(current_num) >= 2:
                        df = add_polynomial_features(
                            df, current_num, 
                            degree=self.metadata.get('interaction_degree', 2),
                            max_features=self.metadata.get('max_interaction_features', 15)
                        )
                
                # Encode categorical columns
                print("[INFO] Encoding categorical columns...")
                for c in cat_cols:
                    if c not in df.columns or c in target_cols or c not in self.col_config:
                        continue
                    
                    cfg = self.col_config[c]
                    enc_type = cfg.get('encoder_type')
                    
                    # Apply rare category grouping
                    s = df[c].astype(object).fillna('__NA__')
                    if self.rare_threshold > 0 and len(s) > 0:
                        if training_mode:
                            # Use training distribution to identify rare categories
                            value_counts = s.value_counts(normalize=True)
                            rare_categories = value_counts[value_counts < self.rare_threshold].index.tolist()
                            s[s.isin(rare_categories)] = '__RARE__'
                        elif 'categories' in cfg and '__RARE__' in cfg.get('categories', []):
                            # In transform mode, map to __RARE__ if category not seen in training
                            known_categories = set(cfg.get('categories', []))
                            s[~s.isin(known_categories)] = '__RARE__'
                    
                    if enc_type == 'onehot':
                        ohe = cfg['encoder_obj']
                        arr = ohe.transform(s.to_frame())
                        df_ohe = pd.DataFrame(arr, columns=cfg['ohe_columns'], index=df.index)
                        df = pd.concat([df.drop(columns=[c]), df_ohe], axis=1)
                    
                    elif enc_type == 'target_kfold':
                        if training_mode and 'oof_encoding' in cfg:
                            df[c] = cfg['oof_encoding'].reindex(df.index, fill_value=cfg['target_global_mean']).astype(float)
                        elif training_mode and 'multi_class_mappings' in cfg:
                            first_cls = list(cfg['multi_class_mappings'].keys())[0]
                            mapping = cfg['multi_class_mappings'][first_cls]['mapping']
                            gmean = cfg['multi_class_mappings'][first_cls]['global_mean']
                            df[c] = s.map(lambda x: mapping.get(x, gmean)).astype(float)
                        else:
                            if 'target_mapping' in cfg:
                                mapping = cfg['target_mapping']
                                gmean = cfg['target_global_mean']
                                df[c] = s.map(lambda x: mapping.get(x, gmean)).astype(float)
                            elif 'multi_class_mappings' in cfg:
                                first_cls = list(cfg['multi_class_mappings'].keys())[0]
                                mapping = cfg['multi_class_mappings'][first_cls]['mapping']
                                gmean = cfg['multi_class_mappings'][first_cls]['global_mean']
                                df[c] = s.map(lambda x: mapping.get(x, gmean)).astype(float)
                            else:
                                df[c] = 0.0  # Fallback
                    
                    elif enc_type == 'ordinal':
                        ordinal = cfg['encoder_obj']
                        s_encoded = ordinal.transform(s.to_frame())
                        df[c] = s_encoded.flatten()
                    
                    elif enc_type == 'count':
                        cnt_map = cfg.get('count_map', {})
                        df[c] = s.map(lambda x: cnt_map.get(x, 0)).astype(float)
                
                df.replace([np.inf, -np.inf], np.nan, inplace=True)
                return df
            
            def save(self, path):
                with gzip.open(path, "wb") as f:
                    cloudpickle.dump(self, f)
            
            @staticmethod
            def load(path):
                with gzip.open(path, "rb") as f:
                    return cloudpickle.load(f)

        # Main execution
        parser = argparse.ArgumentParser()
        parser.add_argument('--imputed_X', type=str, required=True)
        parser.add_argument('--aligned_y', type=str, required=True)
        parser.add_argument('--imputation_config', type=str, required=True)
        parser.add_argument('--create_interactions', type=str, default="false")
        parser.add_argument('--interaction_degree', type=int, default=2)
        parser.add_argument('--max_interaction_features', type=int, default=15)
        parser.add_argument('--encoding_strategy', type=str, default="auto")
        parser.add_argument('--ohe_max_categories', type=int, default=30)
        parser.add_argument('--target_enc_smoothing', type=float, default=10.0)
        parser.add_argument('--target_enc_folds', type=int, default=5)
        parser.add_argument('--rare_threshold', type=float, default=0.01)
        parser.add_argument('--numeric_transform_strategy', type=str, default="auto")
        parser.add_argument('--transform_positive_only', type=str, default="true")
        parser.add_argument('--skewness_threshold', type=float, default=0.5)
        parser.add_argument('--kurtosis_threshold', type=float, default=3.0)
        parser.add_argument('--outlier_threshold', type=float, default=0.01)
        parser.add_argument('--positive_offset', type=float, default=1e-6)
        parser.add_argument('--engineered_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--engineering_metadata', type=str, required=True)
        args = parser.parse_args()

        try:
            print("="*80)
            print("PART 2: ADVANCED ENCODING & FEATURE ENGINEERING")
            print("="*80)
            
            # Load data
            print("[STEP 1/5] Loading imputed data...")
            X = pd.read_parquet(args.imputed_X)
            y = pd.read_parquet(args.aligned_y)
            
            with open(args.imputation_config, 'r') as f:
                config = json.load(f)
            
            print(f"[INFO] X shape: {X.shape}, y shape: {y.shape}")
            
            # Add interaction config
            create_interactions = str(args.create_interactions).lower() in ("1","true","t","yes","y")
            config['create_interactions'] = create_interactions
            config['interaction_degree'] = args.interaction_degree
            config['max_interaction_features'] = args.max_interaction_features
            
            # Fit preprocessor
            print("[STEP 2/5] Fitting preprocessor...")
            pre = Preprocessor(
                encoding_strategy=args.encoding_strategy,
                ohe_max_categories=args.ohe_max_categories,
                target_enc_smoothing=args.target_enc_smoothing,
                target_enc_folds=args.target_enc_folds,
                rare_threshold=args.rare_threshold,
                numeric_transform_strategy=args.numeric_transform_strategy,
                transform_positive_only=str(args.transform_positive_only).lower() in ("true", "1", "yes", "y"),
                skewness_threshold=args.skewness_threshold,
                kurtosis_threshold=args.kurtosis_threshold,
                outlier_threshold=args.outlier_threshold,
                positive_offset=args.positive_offset
            )
            
            target_cols = config['target_columns']
            model_type = config['model_type']
            
            if isinstance(y, pd.DataFrame) and len(target_cols) > 1:
                y_for_fit = y[target_cols[0]]
            elif isinstance(y, pd.DataFrame):
                y_for_fit = y[target_cols[0]]
            else:
                y_for_fit = y
            
            pre.fit(X, y=y_for_fit, config=config, model_type=model_type)
            
            # Transform
            print("[STEP 3/5] Transforming features...")
            X_engineered = pre.transform(X, training_mode=True)
            print(f"[INFO] Engineered shape: {X_engineered.shape}")
            
            # Optimize dtypes
            print("[STEP 4/5] Optimizing dtypes...")
            for col in X_engineered.select_dtypes(include=[np.number]).columns:
                try:
                    X_engineered[col] = pd.to_numeric(X_engineered[col], downcast='float')
                except:
                    pass
            
            # Save outputs
            print("[STEP 5/5] Saving outputs...")
            ensure_dir_for(args.engineered_X)
            ensure_dir_for(args.train_y)
            ensure_dir_for(args.preprocessor)
            ensure_dir_for(args.engineering_metadata)
            
            X_engineered.to_parquet(args.engineered_X, index=False)
            y.to_parquet(args.train_y, index=False)
            pre.save(args.preprocessor)
            
            # Create comprehensive metadata
            metadata = {
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'model_type': model_type,
                'target_columns': target_cols,
                'encoding_configuration': {
                    'strategy': args.encoding_strategy,
                    'ohe_max_categories': args.ohe_max_categories,
                    'target_enc_smoothing': args.target_enc_smoothing,
                    'target_enc_folds': args.target_enc_folds,
                    'rare_threshold': args.rare_threshold
                },
                'numeric_transform_configuration': {
                    'strategy': args.numeric_transform_strategy,
                    'transform_positive_only': str(args.transform_positive_only).lower() in ("true", "1", "yes", "y"),
                    'skewness_threshold': args.skewness_threshold,
                    'kurtosis_threshold': args.kurtosis_threshold,
                    'outlier_threshold': args.outlier_threshold,
                    'positive_offset': args.positive_offset
                },
                'interactions_created': create_interactions,
                'interaction_config': {
                    'degree': args.interaction_degree,
                    'max_features': args.max_interaction_features
                },
                'final_shape': {
                    'rows': X_engineered.shape[0],
                    'cols': X_engineered.shape[1]
                },
                'column_encoding_details': {},
                'column_transform_details': {},
                'encoding_summary': pre.metadata.get('encoding_summary', {}),
                'transform_summary': pre.metadata.get('numeric_transform_summary', {}),
                'part1_config': config
            }
            
            # Add per-column encoding details
            for col, cfg in pre.col_config.items():
                col_type = cfg.get('type', 'unknown')
                
                if col_type == 'numeric':
                    metadata['column_transform_details'][col] = {
                        'transform_type': cfg.get('transform_type', 'none'),
                        'transform_reason': cfg.get('transform_reason', 'unknown'),
                        'statistics': cfg.get('statistics', {})
                    }
                
                metadata['column_encoding_details'][col] = {
                    'type': col_type,
                    'encoder_type': cfg.get('encoder_type', 'none') if col_type == 'categorical' else 'numeric',
                    'original_nunique': cfg.get('original_nunique', 0),
                    'rare_categories_grouped': cfg.get('rare_categories_grouped', 0)
                }
                
                if col_type == 'categorical' and 'n_categories' in cfg:
                    metadata['column_encoding_details'][col]['n_categories'] = cfg['n_categories']
            
            with open(args.engineering_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print("="*80)
            print("PART 2 COMPLETE - ADVANCED ENGINEERING SUMMARY")
            print("="*80)
            print(f"Final shape: {X_engineered.shape[0]} rows Ã— {X_engineered.shape[1]} columns")
            print(f"Encoding strategy: {args.encoding_strategy}")
            print(f"Numeric transform strategy: {args.numeric_transform_strategy}")
            print(f"Interactions created: {create_interactions}")
            
            # Print transform distribution
            if 'transform_summary' in metadata:
                transform_counts = {}
                for col, info in metadata['transform_summary'].items():
                    transform_type = info['transform']
                    transform_counts[transform_type] = transform_counts.get(transform_type, 0) + 1
                
                print("Numeric Transform distribution:")
                for trans_type, count in transform_counts.items():
                    print(f"  {trans_type}: {count} columns")
            
            # Print encoding distribution
            if 'encoding_summary' in metadata:
                enc_counts = {}
                for col, enc_type in metadata['encoding_summary'].items():
                    enc_counts[enc_type] = enc_counts.get(enc_type, 0) + 1
                
                print("Categorical Encoding distribution:")
                for enc_type, count in enc_counts.items():
                    print(f"  {enc_type}: {count} columns")
            
            print("="*80)
            
        except Exception as exc:
            print("ERROR: " + str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --imputed_X
      - {inputPath: imputed_X}
      - --aligned_y
      - {inputPath: aligned_y}
      - --imputation_config
      - {inputPath: imputation_config}
      - --create_interactions
      - {inputValue: create_interactions}
      - --interaction_degree
      - {inputValue: interaction_degree}
      - --max_interaction_features
      - {inputValue: max_interaction_features}
      - --encoding_strategy
      - {inputValue: encoding_strategy}
      - --ohe_max_categories
      - {inputValue: ohe_max_categories}
      - --target_enc_smoothing
      - {inputValue: target_enc_smoothing}
      - --target_enc_folds
      - {inputValue: target_enc_folds}
      - --rare_threshold
      - {inputValue: rare_threshold}
      - --numeric_transform_strategy
      - {inputValue: numeric_transform_strategy}
      - --transform_positive_only
      - {inputValue: transform_positive_only}
      - --skewness_threshold
      - {inputValue: skewness_threshold}
      - --kurtosis_threshold
      - {inputValue: kurtosis_threshold}
      - --outlier_threshold
      - {inputValue: outlier_threshold}
      - --positive_offset
      - {inputValue: positive_offset}
      - --engineered_X
      - {outputPath: engineered_X}
      - --train_y
      - {outputPath: train_y}
      - --preprocessor
      - {outputPath: preprocessor}
      - --engineering_metadata
      - {outputPath: engineering_metadata}
