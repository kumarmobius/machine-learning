name: Load JSON dataset
description: Fetches JSON from API and prepares train/test/anomaly datasets in Parquet format. No preprocessing is applied; prints dataframe diagnostics.
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: split_size, type: Integer, description: 'Test split size as integer percent (e.g. 20) or fraction (e.g. 0.2)'}
outputs:
  - {name: train_set, type: Data}
  - {name: test_x, type: Data}
  - {name: test_y, type: Data}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn pyarrow || \
        python3 -m pip install --quiet requests pandas scikit-learn pyarrow --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True, help='API URL to fetch JSON dataset')
        parser.add_argument('--access_token', type=str, required=True, help='Bearer token for API')
        parser.add_argument('--split_size', type=float, required=True, help='Test split size as integer percent or fraction')
        parser.add_argument('--train_set', type=str, required=True, help='Path to output train dataset')
        parser.add_argument('--test_x', type=str, required=True, help='Path to output test X dataset')
        parser.add_argument('--test_y', type=str, required=True, help='Path to output test Y dataset')
        args = parser.parse_args()

        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("api_retry")

        # Setup HTTP session with retry
        session = requests.Session()
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {access_token}"}
        payload = {"dbType": "TIDB", "entityId": "", "entityIds": [], "ownedOnly": False, "projections": [], "filter": {}, "startTime": 0, "endTime": 0}

        try:
            logger.info(" Sending request to API with retries enabled")
            resp = session.post(args.api_url, headers=headers, json=payload, timeout=30)
            resp.raise_for_status()
            raw_data = resp.json()
        except requests.exceptions.RequestException as e:
            logger.error(f" API request failed after retries: {e}")
            raise

        df = pd.DataFrame(raw_data)

        # Print dataframe diagnostics
        print("DF_HEAD:")
        print(df.head().to_string())
        print("DF_INFO:")
        df.info()
        print("DF_DTYPES:")
        print(df.dtypes.to_string())
        print("DF_DESCRIBE:")
        print(df.describe(include='all').to_string())
        print("DF_NULL_COUNTS:")
        print(df.isnull().sum().to_string())

        # Define anomaly labels
        if "reachable_frac" in df.columns:
            anomaly_labels = (df["reachable_frac"] == 0.0).astype(np.int32)
        else:
            logger.info(" reachable_frac column not found. Creating zero anomaly labels")
            anomaly_labels = pd.Series(np.zeros(len(df), dtype=np.int32), index=df.index)

        # Convert split size: integer percent → fraction
        split_size_input = float(args.split_size)
        if split_size_input >= 1.0:
            test_size = split_size_input / 100.0
        else:
            test_size = split_size_input

        if not (0.0 < test_size < 1.0):
            raise ValueError("split_size must be between 0 and 1 or 1–99 (as percent)")

        stratify_param = anomaly_labels if anomaly_labels.nunique() > 1 else None

        train_df, test_df, train_labels, test_labels = train_test_split(
            df, anomaly_labels, test_size=test_size, random_state=42, stratify=stratify_param
        )

        # Save outputs in Parquet format
        os.makedirs(os.path.dirname(args.train_set) or ".", exist_ok=True)
        train_df.to_parquet(args.train_set, index=False)

        os.makedirs(os.path.dirname(args.test_x) or ".", exist_ok=True)
        test_df.to_parquet(args.test_x, index=False)

        os.makedirs(os.path.dirname(args.test_y) or ".", exist_ok=True)
        pd.DataFrame(test_labels, columns=["anomaly_label"]).to_parquet(args.test_y, index=False)
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --split_size
      - {inputValue: split_size}
      - --train_set
      - {outputPath: train_set}
      - --test_x
      - {outputPath: test_x}
      - --test_y
      - {outputPath: test_y}
