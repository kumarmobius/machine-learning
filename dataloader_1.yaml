name: Unified Data Loader
description: Loads data from API, prints info, splits into train and test, and saves outputs
inputs:
  - name: api_url
    type: String
    description: 'API URL to fetch JSON data'
  - name: access_token
    type: string
    description: 'Bearer access token for API auth'
  - name: test_split
    type: Integer
    description: 'Percentage of test split (e.g., 10 means 0.1 fraction)'
    default: "10"
outputs:
  - name: train_file
    type: Data
    description: 'Combined train set with features and target'
  - name: test_x_file
    type: Data
    description: 'Test feature set (X)'
  - name: test_y_file
    type: Data
    description: 'Test target set (y)'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn pyarrow || \
        python3 -m pip install --quiet requests pandas scikit-learn pyarrow --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, requests, pandas as pd
        from sklearn.model_selection import train_test_split

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--test_split', type=int, default=10)
        parser.add_argument('--train_file', type=str, required=True)
        parser.add_argument('--test_x_file', type=str, required=True)
        parser.add_argument('--test_y_file', type=str, required=True)
        args = parser.parse_args()

        with open(args.access_token, 'r') as f:
            token = f.read().strip()

        headers = {"Authorization": f"Bearer {token}"}
        print("DEBUG: Fetching data from API:", args.api_url)
        resp = requests.get(args.api_url, headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        if isinstance(data, dict) and 'content' in data:
            df = pd.DataFrame(data['content'])
        else:
            df = pd.DataFrame(data)

        print("DEBUG: Data loaded successfully")
        print("Number of rows:", len(df))
        print("Number of columns:", len(df.columns))
        print("Column names:", list(df.columns))
        print("DataFrame info:")
        print(df.info())
        print("DataFrame head preview:")
        print(df.head().to_string())

        if 'target' not in df.columns:
            raise ValueError("Expected a 'target' column for splitting y variable")

        test_size = args.test_split / 100.0 if args.test_split > 1 else args.test_split
        X = df.drop(columns=['target'])
        y = df['target']
        train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)

        train_combined = pd.concat([train_x, train_y], axis=1)

        os.makedirs(os.path.dirname(args.train_file) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_x_file) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_y_file) or ".", exist_ok=True)

        train_combined.to_parquet(args.train_file, index=False)
        test_x.to_parquet(args.test_x_file, index=False)
        test_y.to_frame().to_parquet(args.test_y_file, index=False)

        print(f"DEBUG: Train file saved to {args.train_file} with {len(train_combined)} rows")
        print(f"DEBUG: Test X saved to {args.test_x_file} with {len(test_x)} rows")
        print(f"DEBUG: Test y saved to {args.test_y_file} with {len(test_y)} rows")

    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --test_split
      - {inputValue: test_split}
      - --train_file
      - {outputPath: train_file}
      - --test_x_file
      - {outputPath: test_x_file}
      - --test_y_file
      - {outputPath: test_y_file}
